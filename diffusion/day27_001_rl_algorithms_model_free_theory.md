# Day 27 - Part 1: Model-Free RL Algorithms Theory

## üìö Learning Objectives
By the end of this section, you will understand:
- Mathematical foundations of Monte Carlo methods and temporal difference learning
- Theoretical analysis of SARSA, Q-learning, and Expected SARSA algorithms
- Mathematical principles of on-policy vs off-policy learning and their convergence properties
- Information-theoretic perspectives on sample efficiency and bias-variance trade-offs
- Theoretical frameworks for function approximation and generalization in RL
- Mathematical modeling of exploration strategies and their impact on learning dynamics

---

## üéØ Monte Carlo Methods Mathematical Framework

### Monte Carlo Learning Theory

#### Mathematical Foundation of Monte Carlo Methods
**Return-Based Value Estimation**:
```
Monte Carlo Principle:
Estimate expected values using sample means
V^œÄ(s) = E^œÄ[G_t | S_t = s] where G_t = Œ£_{k=0}^‚àû Œ≥^k R_{t+k+1}
Estimator: VÃÇ(s) = (1/n) Œ£_{i=1}^n G_i where G_i are sample returns from state s

First-Visit Monte Carlo:
Average returns from first visit to state s in each episode
VÃÇ(s) ‚Üê VÃÇ(s) + Œ±[G_t - VÃÇ(s)] for first visit to s
Unbiased estimator: E[VÃÇ(s)] = V^œÄ(s)
Consistency: VÃÇ(s) ‚Üí V^œÄ(s) as number of episodes ‚Üí ‚àû

Every-Visit Monte Carlo:
Average returns from all visits to state s
Multiple updates per episode if state revisited
Biased but consistent estimator
Lower variance than first-visit in some cases

Mathematical Properties:
Law of Large Numbers: sample mean converges to true expectation
Central Limit Theorem: ‚àön(VÃÇ(s) - V^œÄ(s)) ‚Üí_d N(0, œÉ¬≤)
Convergence rate: O(1/‚àön) where n is number of samples
No bootstrapping: uses actual returns, not estimates
```

**Monte Carlo Control Theory**:
```
Policy Evaluation:
Generate episodes using policy œÄ
For each state visited, update value estimate using return
V_{k+1}(s) = V_k(s) + Œ±_k[G_t - V_k(s)]
Learning rate: Œ±_k satisfies Robbins-Monro conditions

Policy Improvement:
Œµ-greedy improvement: œÄ(s) = argmax_a Q(s,a) with probability 1-Œµ
Exploration necessity: all state-action pairs must be visited
GLIE (Greedy in the Limit with Infinite Exploration)
Convergence: œÄ_k ‚Üí œÄ* under GLIE conditions

On-Policy Monte Carlo:
Evaluate and improve policy being followed
Sample collection and policy improvement use same policy
Convergence to optimal policy guaranteed under GLIE

Off-Policy Monte Carlo:
Evaluate target policy œÄ using data from behavior policy Œº
Importance sampling: weight returns by likelihood ratio
œÅ_t = œÄ(A_t|S_t) / Œº(A_t|S_t)
Weighted returns: G_t^{(œÄ)} = œÅ_t G_t^{(Œº)}

Mathematical Challenges:
High variance: returns have large variance
Sample efficiency: requires complete episodes
Exploration: ensuring adequate state-action coverage
Importance sampling: variance explosion when œÅ_t large
```

#### Importance Sampling Theory
**Mathematical Framework for Off-Policy Learning**:
```
Importance Sampling Fundamentals:
Target distribution: œÄ(a|s)
Behavior distribution: Œº(a|s)
Likelihood ratio: œÅ(a|s) = œÄ(a|s) / Œº(a|s)
Importance weight: W_t = ‚àè_{k=0}^{t-1} œÅ(A_k|S_k)

Ordinary Importance Sampling:
V^œÄ(s) = E_Œº[œÅ_{t:T-1} G_t | S_t = s]
Unbiased estimator: E[œÅG] = E^œÄ[G]
High variance: Var[œÅG] can be much larger than Var[G]
Estimator: VÃÇ(s) = Œ£_i œÅ_i G_i / n

Weighted Importance Sampling:
V^œÄ(s) = Œ£_i œÅ_i G_i / Œ£_i œÅ_i
Biased but consistent estimator
Lower variance than ordinary importance sampling
Ratio estimator: reduces impact of extreme weights

Mathematical Analysis:
Bias-variance trade-off: ordinary IS unbiased but high variance
Weighted IS biased but lower variance
Convergence: both converge to true value asymptotically
Efficiency: weighted IS often preferred in practice

Per-Decision Importance Sampling:
Separate importance weights for each decision
Reduces variance by avoiding product of ratios
More complex but potentially better performance
Theoretical analysis more involved
```

### Temporal Difference Learning Theory

#### Mathematical Foundation of TD Learning
**TD(0) Algorithm Theory**:
```
Temporal Difference Principle:
Update estimates using other estimates (bootstrapping)
TD error: Œ¥_t = R_{t+1} + Œ≥V(S_{t+1}) - V(S_t)
Update rule: V(S_t) ‚Üê V(S_t) + Œ± Œ¥_t
Combines Monte Carlo (sampling) with Dynamic Programming (bootstrapping)

Mathematical Properties:
Bootstrap estimate: uses V(S_{t+1}) instead of actual return
Bias-variance trade-off: biased but lower variance than MC
Online learning: updates after each step, not episode
Sample efficiency: can learn from incomplete episodes

Convergence Analysis:
Robbins-Monro conditions for learning rate Œ±_t:
Œ£_t Œ±_t = ‚àû and Œ£_t Œ±_t¬≤ < ‚àû
Convergence: V_t ‚Üí V^œÄ with probability 1
Rate: O(1/t) convergence under appropriate conditions
Contraction mapping: TD operator is contraction under certain conditions

TD vs Monte Carlo:
Bias: TD biased due to bootstrapping, MC unbiased
Variance: TD lower variance, MC higher variance
Convergence: TD faster in practice, MC guaranteed convergence
Markov property: TD exploits Markov property, MC doesn't require it
```

**SARSA Algorithm Theory**:
```
On-Policy TD Control:
State-Action-Reward-State-Action sequence
Q-learning for on-policy case
Update: Q(S_t, A_t) ‚Üê Q(S_t, A_t) + Œ±[R_{t+1} + Œ≥Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)]

Policy Following:
Action selection: A_{t+1} ~ œÄ(¬∑|S_{t+1})
Policy improvement: Œµ-greedy based on current Q-values
Convergence: Q ‚Üí Q^œÄ for policy being followed

SARSA(Œª) Extension:
Eligibility traces: e_t(s,a) = Œ≥Œªe_{t-1}(s,a) + 1_{S_t=s, A_t=a}
Update all state-action pairs: Q(s,a) ‚Üê Q(s,a) + Œ± Œ¥_t e_t(s,a)
Parameter Œª ‚àà [0,1]: Œª=0 gives SARSA(0), Œª=1 gives Monte Carlo
Backward view: credit assignment to recent state-action pairs

Mathematical Properties:
On-policy convergence: converges to Q^œÄ where œÄ is policy being followed
Exploration requirement: adequate exploration of state-action space
GLIE property: greedy in limit with infinite exploration
Stochastic approximation: satisfies conditions for convergence
```

**Q-Learning Algorithm Theory**:
```
Off-Policy TD Control:
Update rule: Q(S_t, A_t) ‚Üê Q(S_t, A_t) + Œ±[R_{t+1} + Œ≥ max_a Q(S_{t+1}, a) - Q(S_t, A_t)]
Target: R_{t+1} + Œ≥ max_a Q(S_{t+1}, a)
Behavior policy: any policy ensuring adequate exploration
Target policy: greedy policy with respect to Q

Off-Policy Nature:
Learns optimal Q* regardless of behavior policy
Can use Œµ-greedy for exploration while learning optimal policy
No importance sampling required (unlike Monte Carlo)
Decouples exploration from exploitation

Convergence Theory:
Watkins & Dayan (1992): Q_t ‚Üí Q* with probability 1
Conditions: all state-action pairs visited infinitely often
Learning rate satisfies Robbins-Monro conditions
Optimal convergence: regardless of exploration policy

Mathematical Analysis:
Contraction mapping: Q-learning operator is contraction
Fixed point: Q* is unique fixed point
Convergence rate: depends on problem structure and exploration
Sample complexity: polynomial in state-action space size
```

#### Advanced TD Methods Theory
**Expected SARSA Theory**:
```
Expected Update Rule:
Q(S_t, A_t) ‚Üê Q(S_t, A_t) + Œ±[R_{t+1} + Œ≥ Œ£_a œÄ(a|S_{t+1})Q(S_{t+1}, a) - Q(S_t, A_t)]
Expectation over next action: removes variance from action selection
Generalization: SARSA when œÄ deterministic, Q-learning when œÄ greedy

Mathematical Properties:
Lower variance: expectation reduces variance compared to SARSA
Computational cost: requires summing over all actions
Convergence: same conditions as SARSA but potentially faster
Bias-variance: different trade-off than standard SARSA

Double Q-Learning:
Problem: Q-learning can overestimate values due to maximization bias
Solution: maintain two Q-functions Q‚ÇÅ and Q‚ÇÇ
Update: Q‚ÇÅ(S_t, A_t) ‚Üê Q‚ÇÅ(S_t, A_t) + Œ±[R_{t+1} + Œ≥ Q‚ÇÇ(S_{t+1}, argmax_a Q‚ÇÅ(S_{t+1}, a)) - Q‚ÇÅ(S_t, A_t)]
Unbiased estimation: reduces maximization bias

N-Step TD Methods:
n-step return: G_t^{(n)} = Œ£_{k=0}^{n-1} Œ≥^k R_{t+k+1} + Œ≥^n V(S_{t+n})
Update: V(S_t) ‚Üê V(S_t) + Œ±[G_t^{(n)} - V(S_t)]
Parameter n: n=1 gives TD(0), n=‚àû gives Monte Carlo
Bias-variance spectrum: larger n reduces bias, increases variance
```

### Function Approximation Theory

#### Mathematical Framework for Generalization
**Linear Function Approximation**:
```
Value Function Approximation:
V_Œ∏(s) = Œ∏^T œÜ(s) where œÜ(s) ‚àà ‚Ñù^d is feature vector
Parameter vector: Œ∏ ‚àà ‚Ñù^d
Feature engineering: œÜ(s) captures relevant state information
Linear in parameters: enables theoretical analysis

Gradient Descent Updates:
Objective: minimize squared TD error
‚àá_Œ∏ [Œ¥_t]¬≤ = 2Œ¥_t ‚àá_Œ∏ V_Œ∏(S_t) = 2Œ¥_t œÜ(S_t)
Update: Œ∏_{t+1} = Œ∏_t - Œ± Œ¥_t œÜ(S_t)
Semi-gradient: ignores gradient through V_Œ∏(S_{t+1})

Convergence Analysis:
Linear TD: converges to unique fixed point Œ∏*
Projection: Œ∏* minimizes ||V^œÄ - V_Œ∏||¬≤_Œº in feature space
Error bound: ||V^œÄ - V_Œ∏*||¬≤_Œº ‚â§ (1/(1-Œ≥))¬≤ ||V^œÄ - Œ† V^œÄ||¬≤_Œº
Convergence rate: linear convergence under appropriate conditions

Mathematical Properties:
Positive definiteness: Œ¶^T D Œ¶ must be positive definite
Feature matrix: Œ¶ ‚àà ‚Ñù^{|S| √ó d} with rows œÜ(s)^T
Steady-state distribution: Œº(s) weighting states
Projection operator: Œ† = Œ¶(Œ¶^T D Œ¶)^{-1} Œ¶^T D
```

**Nonlinear Function Approximation**:
```
Neural Network Approximation:
V_Œ∏(s) = f_Œ∏(s) where f_Œ∏ is neural network
Parameter space: high-dimensional and non-convex
Universal approximation: can represent complex functions
Gradient computation: backpropagation

Convergence Challenges:
Non-convex optimization: multiple local minima
Divergence risk: TD with nonlinear approximation can diverge
Deadly triad: function approximation + bootstrapping + off-policy
Gradient bias: semi-gradient methods may not converge

Stability Analysis:
Lyapunov functions: analyze stability of learning dynamics
Fixed points: characterize equilibria of learning algorithm
Basin of attraction: region of convergence
Bifurcation analysis: parameter regimes for stability

Advanced Techniques:
Experience replay: break correlation in training data
Target networks: stabilize learning targets
Batch normalization: normalize activations for stability
Regularization: prevent overfitting and improve generalization
```

#### Theoretical Analysis of Approximation Error
**Bias-Variance Decomposition**:
```
Total Error Decomposition:
E[(V^œÄ(s) - VÃÇ(s))¬≤] = Bias¬≤ + Variance + Noise
Bias: E[VÃÇ(s)] - V^œÄ(s) (systematic error)
Variance: E[(VÃÇ(s) - E[VÃÇ(s)])¬≤] (estimation uncertainty)
Noise: irreducible error from environment stochasticity

Approximation vs Estimation Error:
Approximation error: ||V^œÄ - V_Œ∏*||¬≤ where Œ∏* is best parameters
Estimation error: ||V_Œ∏* - VÃÇ_Œ∏||¬≤ where VÃÇ_Œ∏ is learned approximation
Total error: sum of approximation and estimation errors
Sample complexity: estimation error decreases as O(1/n)

Function Class Complexity:
Rademacher complexity: measure of function class richness
Generalization bounds: relate empirical and true error
VC dimension: combinatorial measure of complexity
Sample complexity: depends on function class complexity

Mathematical Trade-offs:
Rich function classes: low approximation error, high estimation error
Simple function classes: high approximation error, low estimation error
Optimal complexity: minimize total error over function classes
Regularization: bias-variance trade-off through parameter penalties
```

**Generalization Theory**:
```
PAC Learning Framework:
Probably Approximately Correct learning
Sample complexity: number of samples for (Œµ,Œ¥)-approximation
Confidence Œ¥: probability of success
Accuracy Œµ: approximation quality

Uniform Convergence:
sup_{f‚ààF} |empirical_error(f) - true_error(f)| ‚â§ Œµ
Holds uniformly over function class F
Rademacher complexity bounds: problem-dependent rates
Concentration inequalities: high-probability bounds

Stability Analysis:
Algorithmic stability: sensitivity to single sample changes
Uniform stability: bounded change in output for any input change
Generalization bounds: stable algorithms generalize well
Connection to bias-variance: stability relates to variance

Online Learning Theory:
Regret bounds: cumulative loss vs best fixed strategy
Online-to-batch conversion: convert online to batch bounds
Adaptive algorithms: adjust to problem difficulty
Non-stationary environments: handle changing distributions
```

---

## üéØ Advanced Understanding Questions

### Monte Carlo Methods Theory:
1. **Q**: Analyze the mathematical trade-offs between first-visit and every-visit Monte Carlo methods in terms of bias, variance, and convergence properties.
   **A**: Mathematical comparison: first-visit MC uses only first occurrence of state in episode, giving unbiased estimator E[VÃÇ_first(s)] = V^œÄ(s). Every-visit MC uses all occurrences, creating dependence between samples from same episode. Bias analysis: first-visit unbiased by construction, every-visit biased due to within-episode correlation but asymptotically unbiased. Variance analysis: every-visit typically lower variance due to more samples per episode, first-visit higher variance but independent samples. Convergence properties: both satisfy law of large numbers, first-visit has independent increments enabling CLT, every-visit has dependent increments complicating analysis. Sample efficiency: every-visit uses more data per episode, first-visit requires more episodes for same accuracy. Mathematical conditions: both converge under standard regularity conditions, every-visit may converge faster in practice due to variance reduction. Key insight: choice depends on episode structure and correlation within episodes.

2. **Q**: Develop a theoretical framework for analyzing the variance explosion problem in importance sampling for off-policy Monte Carlo methods.
   **A**: Framework components: (1) importance ratio œÅ_t = ‚àè_{k=0}^{t-1} œÄ(A_k|S_k)/Œº(A_k|S_k), (2) variance analysis Var[œÅ_t G_t], (3) explosion conditions. Variance explosion: occurs when Var[œÅ_t] grows exponentially with episode length T. Mathematical analysis: under mild conditions, Var[œÅ_t] ‚â• (Œ±^T - 1)/(Œ± - 1) where Œ± = E[œÅ‚ÇÅ¬≤]. Explosion threshold: Œ± > 1 leads to exponential variance growth. Mitigation strategies: weighted importance sampling reduces variance at cost of bias, per-decision importance sampling avoids product accumulation, clip importance ratios to bound variance. Theoretical bounds: finite-sample concentration inequalities for bounded importance ratios. Practical implications: off-policy MC requires careful policy similarity, behavior policy should have adequate coverage. Alternative approaches: doubly robust methods, control variates for variance reduction. Key insight: variance explosion fundamental limitation requiring principled mitigation strategies.

3. **Q**: Compare the sample complexity and convergence rates of Monte Carlo versus temporal difference methods under different assumptions about function approximation and exploration.
   **A**: Sample complexity comparison: MC requires O(1/Œµ¬≤) samples for Œµ-accuracy due to variance, TD requires O(1/Œµ) under linear function approximation. Convergence rates: MC converges at O(1/‚àön) rate, TD achieves O(1/n) under contraction conditions. Function approximation impact: linear FA preserves convergence for both methods with different constants, nonlinear FA can cause TD divergence while MC remains stable. Exploration assumptions: MC requires episode-based exploration, TD can learn from partial episodes. Bias-variance analysis: MC unbiased but high variance, TD biased but lower variance, optimal choice depends on bias-variance trade-off. Markov property: TD exploits Markov structure for efficiency, MC doesn't require Markovian assumptions. Theoretical guarantees: MC has stronger convergence guarantees, TD faster but more assumptions. Bootstrap error: TD compounds estimation errors through bootstrapping, MC avoids this issue. Key insight: choice depends on problem structure, approximation quality, and computational constraints.

### Temporal Difference Learning Theory:
4. **Q**: Analyze the mathematical conditions under which SARSA and Q-learning converge, and explain why Q-learning can learn optimal policies with suboptimal behavior policies.
   **A**: Mathematical conditions: both algorithms require (1) all state-action pairs visited infinitely often, (2) learning rates satisfy Robbins-Monro conditions Œ£Œ±_t = ‚àû, Œ£Œ±_t¬≤ < ‚àû, (3) bounded rewards. SARSA convergence: converges to Q^œÄ where œÄ is the policy being followed (on-policy), requires GLIE (greedy in limit with infinite exploration). Q-learning convergence: converges to Q* regardless of behavior policy (off-policy), only requires adequate exploration. Key difference: SARSA updates using next action from current policy, Q-learning uses max over all actions. Mathematical proof: Q-learning operator T is contraction mapping in supremum norm, ||TQ - TU||_‚àû ‚â§ Œ≥||Q - U||_‚àû. Optimal learning: Q-learning target is optimal Bellman backup, SARSA target depends on current policy. Exploration requirement: Q-learning separates exploration (behavior policy) from exploitation (target policy), SARSA conflates them. Convergence rate: both achieve polynomial sample complexity under tabular setting. Key insight: off-policy nature allows Q-learning to learn optimal behavior while exploring suboptimally.

5. **Q**: Develop a mathematical theory for the bias-variance trade-off in n-step temporal difference methods, including the relationship between n and convergence properties.
   **A**: Theory components: (1) n-step return G_t^{(n)} = Œ£_{k=0}^{n-1} Œ≥^k R_{t+k+1} + Œ≥^n V(S_{t+n}), (2) bias analysis from bootstrapping, (3) variance from sampling. Bias analysis: bias decreases with n as more actual rewards used, approaches zero as n ‚Üí ‚àû (Monte Carlo limit). Variance analysis: variance increases with n due to more random rewards, influenced by environment stochasticity. Mathematical formulation: MSE = bias¬≤ + variance, optimal n minimizes total error. Convergence properties: larger n requires longer episodes but potentially faster learning per update. Theoretical bounds: bias ‚â§ Œ≥^n V_max/(1-Œ≥), variance proportional to n √ó reward_variance. Eligibility traces: TD(Œª) provides exponentially weighted average over all n-step returns, Œª controls bias-variance trade-off. Optimal n selection: depends on environment properties (reward variance, episode length, discount factor). Practical considerations: computational cost increases with n, truncation at episode boundaries. Key insight: optimal n balances immediate reward information with bootstrap accuracy.

6. **Q**: Compare the theoretical properties of on-policy versus off-policy learning in terms of sample efficiency, convergence guarantees, and exploration requirements.
   **A**: Sample efficiency: off-policy methods can reuse experience from any policy, potentially higher sample efficiency. On-policy methods must generate fresh experience for each policy update. Convergence guarantees: on-policy methods have stronger theoretical guarantees (SARSA converges to Q^œÄ), off-policy methods may suffer from distribution mismatch. Mathematical analysis: on-policy satisfies standard stochastic approximation conditions, off-policy requires additional assumptions about behavior policy coverage. Exploration requirements: on-policy must balance exploration-exploitation in single policy, off-policy can use separate exploration policy. Theoretical frameworks: on-policy fits standard Markov chain analysis, off-policy requires importance sampling or other correction techniques. Variance considerations: off-policy methods may have higher variance due to importance weighting or distribution shift. Practical implications: off-policy better for data efficiency, on-policy better for stability. Advanced techniques: off-policy correction methods (importance sampling, Q-trace), on-policy exploration strategies (UCB, Thompson sampling). Key insight: fundamental trade-off between sample efficiency and theoretical guarantees.

### Function Approximation Theory:
7. **Q**: Analyze the mathematical conditions under which temporal difference learning with linear function approximation converges, and characterize the quality of the limiting solution.
   **A**: Convergence conditions: (1) feature matrix Œ¶ has full rank, (2) steady-state distribution Œº exists and is positive, (3) learning rates satisfy Robbins-Monro conditions. Mathematical analysis: TD with linear FA converges to unique fixed point Œ∏* solving projected Bellman equation Œ¶Œ∏* = Œ†T^œÄ(Œ¶Œ∏*). Projection operator: Œ† = Œ¶(Œ¶^T D Œ¶)^{-1}Œ¶^T D where D = diag(Œº). Solution quality: ||V^œÄ - V_Œ∏*||¬≤_Œº ‚â§ (1/(1-Œ≥))¬≤||V^œÄ - Œ†V^œÄ||¬≤_Œº. Approximation error: depends on how well feature space can represent true value function. Convergence rate: linear convergence with rate depending on smallest eigenvalue of A = Œ¶^T D(I - Œ≥P^œÄ)Œ¶. Feature design impact: better features (smaller ||V^œÄ - Œ†V^œÄ||_Œº) lead to better approximation. Computational complexity: O(d¬≤) per update where d is number of features. Stability analysis: eigenvalues of A - Œ±Œ¶^T D Œ¶ determine stability for learning rate Œ±. Key insight: linear FA provides theoretical guarantees at cost of representational limitations.

8. **Q**: Develop a unified mathematical framework connecting sample complexity, approximation error, and generalization in reinforcement learning with function approximation.
   **A**: Unified framework: total error = approximation_error + estimation_error + generalization_error. Mathematical decomposition: E[L(œÄ_learned)] ‚â§ L(œÄ*) + approximation_error + O(‚àö(complexity/n)) + generalization_gap. Sample complexity: n = O(complexity √ó problem_parameters / Œµ¬≤) for Œµ-optimal policy. Approximation error: ||V* - V_Œ∏*||_‚àû depends on function class expressiveness, irreducible for given architecture. Estimation error: ||V_Œ∏* - VÃÇ_Œ∏||_‚àû decreases as O(1/‚àön) under appropriate conditions. Generalization: difference between empirical and population risk, bounded by Rademacher complexity. Function class complexity: VC dimension, Rademacher complexity, or covering numbers characterize function class richness. Problem parameters: state space size, action space size, horizon, discount factor affect difficulty. Integration: optimal function class balances approximation capability with statistical complexity. Practical implications: architecture choice involves approximation-estimation trade-off, regularization manages complexity. Theoretical guarantees: PAC bounds relate sample size to solution quality with high probability. Key insight: successful RL requires principled balance of representational power and statistical efficiency.

---

## üîë Key Model-Free RL Principles

1. **Monte Carlo Foundation**: Monte Carlo methods provide unbiased value estimates through sample returns but suffer from high variance and episode-based learning constraints.

2. **Temporal Difference Efficiency**: TD learning combines sampling with bootstrapping to achieve lower variance and online learning capability at the cost of introducing bias.

3. **On-Policy vs Off-Policy**: Fundamental trade-off between learning about the policy being executed (stable, guaranteed convergence) versus learning optimal policy from any experience (sample efficient, complex analysis).

4. **Function Approximation Necessity**: Real-world RL requires function approximation for generalization, introducing approximation error and potential convergence issues that must be carefully managed.

5. **Exploration-Learning Balance**: Model-free methods must explicitly balance exploration for learning with exploitation for performance, requiring principled approaches to ensure adequate state-action coverage.

---

**Next**: Continue with Day 28 - Deep Q Networks (DQN) Theory