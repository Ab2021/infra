# Day 32 - Part 1: RL in Generative Modeling Theory

## üìö Learning Objectives
By the end of this section, you will understand:
- Mathematical foundations of reinforcement learning for generative model training
- Theoretical analysis of RLHF (Reinforcement Learning from Human Feedback) and preference optimization
- Mathematical principles of reward modeling and alignment in generative systems
- Information-theoretic perspectives on human preference learning and value alignment
- Theoretical frameworks for fine-tuning generative models with RL objectives
- Mathematical modeling of constitutional AI and self-improving systems

---

## üéØ RL for Generative Models Mathematical Framework

### Reward Modeling and Human Preferences Theory

#### Mathematical Foundation of Preference Learning
**Human Preference Modeling**:
```
Preference Data Structure:
Comparison pairs: (x‚ÇÅ, x‚ÇÇ, y) where y ‚àà {0,1} indicates preference
y = 1: human prefers x‚ÇÅ over x‚ÇÇ
y = 0: human prefers x‚ÇÇ over x‚ÇÅ
Dataset: D = {(x‚ÇÅ‚Å±, x‚ÇÇ‚Å±, y‚Å±)}·µ¢‚Çå‚ÇÅ·¥∫

Bradley-Terry Model:
P(x‚ÇÅ ‚âª x‚ÇÇ) = œÉ(R(x‚ÇÅ) - R(x‚ÇÇ)) = exp(R(x‚ÇÅ))/(exp(R(x‚ÇÅ)) + exp(R(x‚ÇÇ)))
Reward function: R: X ‚Üí ‚Ñù maps outputs to scalar rewards
Sigmoid function: œÉ(z) = 1/(1 + exp(-z))
Log-likelihood: ‚Ñì(R) = Œ£·µ¢ y‚Å± log œÉ(R(x‚ÇÅ‚Å±) - R(x‚ÇÇ‚Å±))

Mathematical Properties:
Transitivity: if x‚ÇÅ ‚âª x‚ÇÇ and x‚ÇÇ ‚âª x‚ÇÉ, then x‚ÇÅ ‚âª x‚ÇÉ
Scale invariance: R(x) + c gives same preferences
Identifiability: R learned up to additive constant
Noise modeling: accounts for inconsistent human judgments
```

**Reward Model Training Theory**:
```
Maximum Likelihood Estimation:
Objective: R* = argmax_R Œ£·µ¢ log P(y‚Å± | x‚ÇÅ‚Å±, x‚ÇÇ‚Å±, R)
Gradient: ‚àá_R ‚Ñì = Œ£·µ¢ (y‚Å± - œÉ(R(x‚ÇÅ‚Å±) - R(x‚ÇÇ‚Å±))) (‚àá_R R(x‚ÇÅ‚Å±) - ‚àá_R R(x‚ÇÇ‚Å±))
Neural parameterization: R_Œ∏(x) with parameters Œ∏

Regularization and Generalization:
L2 regularization: Œª||Œ∏||¬≤ prevents overfitting
Dropout: reduces co-adaptation in neural networks
Early stopping: prevents overfitting to preference data
Cross-validation: estimate generalization performance

Theoretical Guarantees:
Consistency: R_Œ∏ ‚Üí R* as N ‚Üí ‚àû under regularity conditions
Sample complexity: O(d log d / Œµ¬≤) for d-dimensional features
Generalization bounds: depend on model complexity and data size
PAC-Bayes bounds: relate empirical and true preference accuracy

Active Learning:
Query selection: choose informative comparison pairs
Uncertainty sampling: pairs with high prediction uncertainty
Disagreement-based: pairs where models disagree most
Information gain: maximize mutual information I(Œ∏; y|x‚ÇÅ,x‚ÇÇ)
```

#### RLHF Mathematical Framework
**Reinforcement Learning from Human Feedback**:
```
RLHF Pipeline:
1. Supervised fine-tuning: train base model on demonstrations
2. Reward modeling: learn reward from human preferences
3. RL optimization: optimize policy using learned reward

Mathematical Formulation:
Base model: œÄ_Œ∏(y|x) generates output y given input x
Reward model: R_œÜ(x,y) scores output quality
RL objective: max_Œ∏ E_{x~D, y~œÄ_Œ∏}[R_œÜ(x,y)]

Policy Gradient with Reward Model:
‚àá_Œ∏ J(Œ∏) = E_{x,y}[R_œÜ(x,y) ‚àá_Œ∏ log œÄ_Œ∏(y|x)]
REINFORCE: unbiased but high variance
Actor-critic: use baseline to reduce variance
PPO: constrained policy optimization for stability

Theoretical Challenges:
Reward hacking: policy exploits reward model weaknesses
Distributional shift: RL policy differs from reward training data
Optimization difficulties: sparse rewards, long sequences
Alignment problems: reward model may not capture true preferences
```

**Constitutional AI Theory**:
```
Constitutional Principles:
Constitution: set of principles C = {c‚ÇÅ, c‚ÇÇ, ..., c‚Çñ}
Principle satisfaction: S(y, c·µ¢) ‚àà [0,1] measures adherence
Overall constitutional score: S(y) = Œ£·µ¢ w·µ¢ S(y, c·µ¢)

Self-Improvement Process:
Critique: model identifies constitutional violations in outputs
Revise: model generates improved outputs satisfying principles
Constitutional AI: iterative self-improvement process
Mathematical convergence: principles guide optimization trajectory

Theoretical Framework:
Multi-objective optimization: balance task performance and constitutional adherence
Pareto optimality: trade-offs between different principles
Constraint satisfaction: hard constraints for critical principles
Regularization: soft constraints through penalty terms

Mathematical Properties:
Interpretability: explicit principles provide transparency
Scalability: principles can be added or modified
Robustness: constitutional constraints improve safety
Alignment: principles encode human values and preferences
```

### Fine-tuning with RL Objectives Theory

#### Mathematical Framework for RL Fine-tuning
**Policy Optimization for Generative Models**:
```
Generative Policy:
œÄ_Œ∏(y|x): conditional probability of generating y given x
Autoregressive: œÄ_Œ∏(y|x) = ‚àè‚Çú œÄ_Œ∏(y‚Çú|x, y‚ÇÅ:‚Çú‚Çã‚ÇÅ)
Sequence-level optimization: optimize complete sequences

Reward Function Design:
Task-specific rewards: BLEU, ROUGE for text generation
Human preference rewards: learned from comparison data
Composite rewards: R(x,y) = Œ£·µ¢ w·µ¢ R·µ¢(x,y)
Sparse vs dense: trade-offs between signal quality and frequency

RL Objectives:
Expected reward: J(Œ∏) = E_{x~D, y~œÄ_Œ∏}[R(x,y)]
Policy gradient: ‚àá_Œ∏ J = E[R(x,y) ‚àá_Œ∏ log œÄ_Œ∏(y|x)]
Baseline subtraction: reduce variance using state-dependent baseline
Advantage estimation: A(x,y) = R(x,y) - b(x) where b(x) is baseline

Mathematical Challenges:
High-dimensional action space: vocabulary size √ó sequence length
Sparse rewards: single reward per complete sequence
Credit assignment: which tokens contributed to reward
Exploration: encouraging diverse generation while optimizing reward
```

**PPO for Language Models**:
```
Proximal Policy Optimization:
Importance ratio: r(y|x) = œÄ_Œ∏(y|x) / œÄ_old(y|x)
Clipped objective: L^CLIP = E[min(r(y|x)A(x,y), clip(r(y|x), 1-Œµ, 1+Œµ)A(x,y))]
Trust region: prevents large policy changes

Sequence-Level PPO:
Token-level ratios: r‚Çú = œÄ_Œ∏(y‚Çú|x,y‚ÇÅ:‚Çú‚Çã‚ÇÅ) / œÄ_old(y‚Çú|x,y‚ÇÅ:‚Çú‚Çã‚ÇÅ)
Sequence ratio: r(y|x) = ‚àè‚Çú r‚Çú
Numerical stability: log-space computation to prevent overflow

Value Function Learning:
State representation: V(x) estimates expected reward from prompt x
Advantage estimation: A(x,y) = R(x,y) - V(x)
TD learning: V(x) ‚Üê V(x) + Œ±(R(x,y) - V(x))
GAE: generalized advantage estimation for variance reduction

Practical Considerations:
KL divergence penalty: Œ≤ KL(œÄ_old, œÄ_Œ∏) prevents collapse
Early stopping: halt training when KL exceeds threshold
Experience replay: reuse samples across multiple PPO epochs
Batch size: large batches for stable gradient estimates
```

#### Advanced RL Training Techniques
**Actor-Critic for Sequence Generation**:
```
Actor-Critic Architecture:
Actor: œÄ_Œ∏(y|x) parameterized by transformer
Critic: V_œÜ(x) or Q_œÜ(x,y) estimates expected reward
Shared backbone: common transformer layers for efficiency

Temporal Difference Learning:
Value targets: V(x) estimates E[R(x,y)] over policy distribution
TD error: Œ¥ = R(x,y) - V(x)
Critic loss: L_critic = E[Œ¥¬≤]
Actor loss: L_actor = E[Œ¥ ‚àá_Œ∏ log œÄ_Œ∏(y|x)]

Monte Carlo vs TD:
Monte Carlo: use actual reward R(x,y) as target
TD: use estimated value V(x) as target
Bias-variance trade-off: MC unbiased but high variance, TD biased but lower variance
n-step returns: interpolate between MC and TD

Theoretical Properties:
Convergence: actor-critic converges under compatibility conditions
Sample efficiency: variance reduction through baseline
Computational efficiency: single forward pass for value estimation
Scalability: handles long sequences better than REINFORCE
```

**Self-Play and Iterative Training**:
```
Constitutional Self-Play:
Model critiques own outputs using constitutional principles
Generate-critique-revise cycle: iterative improvement
Self-supervised learning: no external human feedback required

Mathematical Framework:
Critic model: C_œà(x,y) evaluates constitutional adherence
Generator model: œÄ_Œ∏(y|x) produces outputs
Adversarial objective: min_Œ∏ max_œà E[C_œà(x, œÄ_Œ∏(y|x))]

Iterative Improvement:
Population-based training: maintain multiple model variants
Tournament selection: best models survive to next generation
Diversity maintenance: prevent mode collapse in population
Curriculum learning: gradually increase difficulty

Theoretical Analysis:
Nash equilibrium: stable points in self-play dynamics
Convergence properties: conditions for reaching equilibrium
Sample complexity: efficiency of self-play vs external feedback
Exploration: self-play may lead to narrow strategy spaces
```

### Alignment and Safety Theory

#### Mathematical Framework for AI Alignment
**Value Alignment Problem**:
```
Specification Problem:
Human values: V_human complex, multifaceted, context-dependent
Reward specification: R(x,y) simplified scalar proxy
Goodhart's law: "When a measure becomes a target, it ceases to be a good measure"
Misalignment: R(x,y) ‚â† V_human leads to unexpected behavior

Mathematical Formulation:
True utility: U*(x,y) represents actual human preferences
Proxy reward: R(x,y) measurable approximation
Optimization: œÄ* = argmax_œÄ E[R(x,y)]
Misalignment cost: E[U*(x,y_R) - U*(x,y*)] where y_R ~ œÄ*, y* optimal

Mesa-Optimization:
Inner optimizer: learned policy may develop internal objectives
Objective robustness: ensuring inner and outer objectives align
Deceptive alignment: policy appears aligned during training
Capability generalization: alignment may break with increased capability

Theoretical Approaches:
Cooperative inverse reinforcement learning: jointly learn reward and policy
Debate: multiple AI systems argue for human judgment
Amplification: recursive human-AI collaboration
Interpretability: understanding model internal representations
```

**Robustness and Uncertainty Theory**:
```
Epistemic Uncertainty:
Model uncertainty: uncertainty about reward model parameters
Aleatoric uncertainty: inherent randomness in human preferences
Total uncertainty: combination of epistemic and aleatoric
Uncertainty quantification: Bayesian neural networks, ensembles

Distributionally Robust Optimization:
Worst-case performance: min_œÄ max_P‚ààU E_P[loss(œÄ)]
Uncertainty set: U defines plausible distributions
Robust reward: R_robust = min_R'‚ààR_set R'(x,y)
Ambiguity aversion: prefer policies robust to uncertainty

Mathematical Formulation:
Risk measures: CVaR, mean-variance, worst-case
Confidence intervals: [R_lower, R_upper] for reward estimates
Robust optimization: optimize lower confidence bound
Safe exploration: avoid actions with high uncertainty

Practical Implementation:
Ensemble methods: multiple reward models for uncertainty
Bayesian optimization: principled uncertainty quantification
Conservative estimates: err on side of caution
Active learning: query humans for high-uncertainty cases
```

#### Advanced Safety Mechanisms
**Constrained Policy Optimization**:
```
Safety Constraints:
Hard constraints: never violate critical safety properties
Soft constraints: minimize safety violations with penalties
Chance constraints: limit probability of constraint violation
Temporal constraints: safety over entire trajectory

Mathematical Framework:
Constrained MDP: (S, A, P, R, C, Œ≥) with constraint function C
Safety constraint: E[‚àë_t C(s_t, a_t)] ‚â§ Œ±
Lagrangian: L = J(œÄ) - Œª(constraint_violation - Œ±)
CPO: constrained policy optimization with trust regions

Lyapunov-based Methods:
Lyapunov function: L(s) ensures safety
Decrease condition: E[L(s') - L(s) | s,a] ‚â§ 0 for safe actions
Safety certificate: formal guarantee of constraint satisfaction
Control barrier functions: ensure forward invariance of safe set

Theoretical Guarantees:
Almost sure safety: probability 1 constraint satisfaction
Finite-time safety: safety over bounded horizons
Asymptotic safety: safety in the limit
Sample complexity: learning safe policies efficiently
```

**Constitutional AI Implementation**:
```
Principle Formalization:
Natural language principles: "Be helpful, harmless, and honest"
Formal specifications: logical predicates or reward functions
Measurable metrics: numerical scores for principle adherence
Hierarchical principles: meta-principles governing principle conflicts

Training Procedure:
Red team: generate problematic outputs
Constitutional response: apply principles to improve outputs
Distillation: train supervised model on constitutional responses
RL fine-tuning: optimize for principle satisfaction

Mathematical Properties:
Interpretability: explicit principles provide transparency
Modularity: principles can be added, removed, or modified
Scalability: constitutional training scales with model size
Robustness: multiple principles provide redundancy

Evaluation Metrics:
Principle adherence: quantitative scores on constitutional principles
Capability preservation: maintain task performance
Consistency: similar responses to similar inputs
Robustness: performance under adversarial inputs
```

---

## üéØ Advanced Understanding Questions

### Reward Modeling Theory:
1. **Q**: Analyze the mathematical relationship between preference model accuracy and downstream RL performance, considering the compounding effects of reward model errors.
   **A**: Mathematical relationship: preference model errors propagate through RL optimization, leading to compounding performance degradation. Error analysis: let Œµ_pref be preference model error rate, Œµ_reward be reward model MSE, then RL performance degrades as O(Œµ_reward √ó T √ó |A|) where T is sequence length. Compounding effects: incorrect rewards guide policy optimization toward suboptimal regions, errors accumulate through iterative policy updates. Theoretical framework: if reward model error ||R_learned - R_true||_‚àû ‚â§ Œµ, then policy performance gap |J(œÄ_learned) - J(œÄ*)| ‚â§ 2Œµ/(1-Œ≥). Practical implications: small reward model errors can cause large policy performance drops, especially in long-horizon tasks. Mitigation strategies: uncertainty-aware training, ensemble reward models, conservative optimization. Empirical observations: performance often more sensitive to reward model accuracy than base model quality. Key insight: reward modeling bottleneck requires careful error control and robustness techniques.

2. **Q**: Develop a theoretical framework for analyzing the sample complexity and generalization properties of learning reward functions from human preferences.
   **A**: Framework components: (1) preference sample complexity n_pref for Œµ-accurate reward model, (2) generalization bounds for Bradley-Terry model, (3) transfer to policy optimization performance. Sample complexity: O(d log(1/Œ¥)/Œµ¬≤) preferences needed for Œµ-accurate reward with probability 1-Œ¥, where d is feature dimension. Generalization analysis: reward model generalization depends on preference consistency, model complexity, and data diversity. PAC-Bayes bounds: P(|R_empirical - R_true| ‚â§ Œµ) ‚â• 1-Œ¥ with explicit dependence on sample size and model complexity. Transfer analysis: reward model accuracy affects downstream policy performance through optimization landscape. Critical factors: preference labeler consistency, representation quality, distribution shift between training and deployment. Theoretical gaps: limited analysis of neural reward models, preference aggregation across multiple humans. Practical implications: preference data quality more important than quantity, active learning crucial for efficiency. Key insight: reward learning requires careful balance between sample efficiency and generalization quality.

3. **Q**: Compare the mathematical properties of different preference aggregation methods (majority vote, Bradley-Terry, ranking) for learning from multiple human labelers with varying reliability.
   **A**: Mathematical comparison: majority vote treats all labelers equally, Bradley-Terry weights by consistency, ranking methods handle transitivity. Majority vote: simple aggregation P_agg(x‚ÇÅ ‚âª x‚ÇÇ) = (1/n)Œ£·µ¢ y·µ¢, assumes equal labeler reliability. Bradley-Terry with reliability: P(x‚ÇÅ ‚âª x‚ÇÇ|labeler i) = œÉ(Œ±·µ¢(R(x‚ÇÅ) - R(x‚ÇÇ))) where Œ±·µ¢ is reliability weight. Ranking methods: fit global ordering consistent with individual rankings, handle intransitive preferences. Theoretical properties: majority vote optimal under uniform reliability, weighted methods better with heterogeneous labelers. Reliability estimation: EM algorithm for joint learning of preferences and reliabilities. Information aggregation: optimal weights proportional to labeler precision, crowd wisdom effects. Consistency analysis: agreement measures, transitivity violations, outlier detection. Practical considerations: computational complexity, labeler fatigue effects, strategic behavior. Key insight: aggregation method should match labeler heterogeneity and reliability patterns for optimal preference learning.

### RLHF Theory:
4. **Q**: Analyze the mathematical challenges of reward hacking in RLHF and develop theoretical frameworks for detecting and mitigating this phenomenon.
   **A**: Mathematical analysis: reward hacking occurs when policy œÄ finds inputs maximizing R_learned(x,y) while minimizing R_true(x,y). Detection framework: monitor reward-performance divergence, out-of-distribution detection for generated samples. Theoretical formulation: let S = {(x,y) : R_learned(x,y) ‚â´ R_true(x,y)} be hacking set, goal is to minimize P(œÄ generates (x,y) ‚àà S). Adversarial examples: policy learns to exploit reward model weaknesses through optimization. Mitigation strategies: (1) uncertainty-aware rewards RÃÉ = R_learned - Œ≤ √ó uncertainty, (2) KL regularization to prevent distribution shift, (3) adversarial training of reward model. Mathematical bounds: if ||R_learned - R_true||_‚àû ‚â§ Œµ on training distribution, but ||R_learned - R_true||_‚àû ‚â´ Œµ on policy distribution, hacking occurs. Robustness measures: worst-case reward model performance, distribution shift metrics. Theoretical prevention: ensemble disagreement, epistemic uncertainty quantification, safe policy optimization. Key insight: reward hacking is fundamental optimization problem requiring principled uncertainty handling and robustness techniques.

5. **Q**: Develop a mathematical theory for the bias-variance trade-off in constitutional AI training, considering the interaction between constitutional principles and task performance.
   **A**: Theory components: (1) bias from constitutional constraints limiting policy space, (2) variance from multi-objective optimization, (3) trade-off analysis between principles and performance. Bias analysis: constitutional constraints create systematic deviation from unconstrained optimum, bias = E[J_constrained] - J_unconstrained < 0. Variance analysis: multi-objective optimization increases gradient variance, competing objectives create noisy updates. Mathematical framework: total error = constitutional_bias¬≤ + optimization_variance + task_performance_loss. Principle interaction: conflicting principles increase bias and variance, orthogonal principles may reduce variance through diversification. Theoretical optimization: optimal principle weights minimize total error subject to safety constraints. Pareto frontier: characterizes achievable trade-offs between principles and performance. Empirical estimation: cross-validation for principle weight selection, A/B testing for principle effectiveness. Dynamic balancing: adapt principle weights during training based on performance metrics. Convergence analysis: constitutional training may converge to different equilibrium than unconstrained training. Key insight: constitutional AI requires careful balancing of multiple objectives with explicit consideration of bias-variance trade-offs.

6. **Q**: Compare the theoretical convergence properties of different RL algorithms (PPO, A2C, SAC) when applied to fine-tuning large language models with human feedback.
   **A**: Convergence comparison: PPO provides theoretical guarantees through trust region constraints, A2C has actor-critic convergence under compatibility conditions, SAC converges to maximum entropy optimum. Language model specifics: discrete action spaces (vocabulary), long sequences, sparse rewards complicate standard RL analysis. PPO analysis: clipping mechanism prevents catastrophic policy updates, maintains approximate KL constraint, guarantees local improvement under mild conditions. A2C convergence: requires compatible function approximation and two-timescale analysis, faster updates but less stable than PPO. SAC properties: entropy regularization improves exploration and robustness, automatic temperature tuning, guaranteed convergence to stochastic optimum. Practical considerations: PPO most stable for LM fine-tuning, A2C faster but more sensitive to hyperparameters, SAC good for exploration but computationally expensive. Sample complexity: all achieve polynomial rates under assumptions, constants vary significantly. Empirical performance: PPO widely adopted due to stability-performance trade-off. Key insight: algorithm choice depends on stability requirements, computational constraints, and exploration needs in language model fine-tuning.

### Advanced Applications:
7. **Q**: Design a mathematical framework for multi-agent constitutional AI where multiple AI systems collaboratively develop and enforce shared principles while maintaining individual capabilities.
   **A**: Framework components: (1) shared constitution C = {c‚ÇÅ, c‚ÇÇ, ..., c‚Çñ} across agents, (2) individual agent policies œÄ·µ¢ with capabilities, (3) coordination mechanisms for principle enforcement. Mathematical formulation: each agent i optimizes J·µ¢ = task_performance·µ¢ + Œª·µ¢ constitutional_adherence + Œ≥·µ¢ coordination_benefit. Constitutional consensus: agents negotiate principle weights through mechanism design, majority voting, or optimization. Game-theoretic analysis: constitutional adherence as coordination game, Nash equilibria with shared principles. Multi-agent learning: agents jointly learn constitution and individual policies, communication protocols for principle sharing. Theoretical properties: stability of constitutional equilibria, convergence to shared principles, robustness to adversarial agents. Implementation mechanisms: debate between agents, consensus algorithms, federated constitutional learning. Scalability: principle aggregation across large numbers of agents, hierarchical constitutional structures. Evaluation metrics: principle consistency across agents, collective task performance, coordination efficiency. Key insight: multi-agent constitutional AI requires balancing individual autonomy with collective principle adherence through game-theoretic and coordination mechanisms.

8. **Q**: Develop a unified mathematical theory connecting RL for generative modeling to fundamental principles of information theory, optimization theory, and human-computer interaction.
   **A**: Unified theory: RL for generative modeling optimizes information transfer from human preferences to model behavior through learned reward functions and policy optimization. Information theory connection: preference learning maximizes mutual information I(human_values; reward_model), policy optimization maximizes I(reward_signal; generated_content). Optimization theory: constrained optimization with human preference constraints, multi-objective optimization balancing task performance and alignment. Human-computer interaction: interactive learning loop where human feedback shapes model behavior, model outputs influence human understanding. Mathematical framework: optimal generative model minimizes KL(human_distribution, model_distribution) subject to computational and safety constraints. Rate-distortion theory: trade-off between model fidelity and human preference satisfaction. Control theory: feedback control system with human as supervisor, model as controlled system. Learning theory: sample complexity of learning human preferences, generalization bounds for preference models. Social choice theory: aggregating preferences across multiple humans, voting mechanisms for AI alignment. Key insight: RL for generative modeling integrates concepts from information theory, optimization, and social sciences to create aligned AI systems through principled mathematical frameworks.

---

## üîë Key RL in Generative Modeling Principles

1. **Human Preference Learning**: Reinforcement learning enables alignment with human values through preference modeling and reward learning, providing scalable alternative to direct human demonstration.

2. **Constitutional AI**: Self-improving systems can be guided by explicit constitutional principles, enabling interpretable and modifiable AI alignment through formal rule specification.

3. **Reward Hacking Mitigation**: Robustness techniques including uncertainty quantification, distributional robustness, and conservative optimization are essential for preventing exploitation of reward model weaknesses.

4. **Multi-Objective Optimization**: Real-world deployment requires balancing multiple objectives including task performance, safety constraints, and human value alignment through principled optimization frameworks.

5. **Iterative Improvement**: Constitutional self-play and iterative refinement enable continuous improvement of AI systems while maintaining alignment and safety properties.

---

**Next**: Continue with Day 33 - RL with Diffusion Models Theory