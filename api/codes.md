```python
import json
import time
import pandas as pd
from typing import Dict, List, Tuple, Set
from src.snowflake_connection import create_snowflake_connection

# ------------------ INITIAL SETUP: LOAD DATA --------------------

from src.gpt_class import GptApi

gpt_object = GptApi()

# -------------------------------- ENHANCED INTENT AGENT ------------------

def classify_intent_and_context_gpt(user_question, agent_history, gpt_object):
    """
    Enhanced intent classification that provides structured context for better schema selection.
    This function now extracts specific types of context that help the schema selection 
    agent make better decisions about tables and joins.
    """
    # Build more structured history representation
    chat_history_str = ""
    inherited_elements = {}
    
    if agent_history:
        last = agent_history[-1]
        # Extract structured information from previous interaction
        prev_reasoning = last.get('reasoning', '')
        prev_sql = last.get('sql_query', '')
        prev_story = last.get('story', '')
        
        # Analyze previous SQL to extract reusable elements
        if prev_sql:
            inherited_elements = extract_sql_context_elements(prev_sql)
        
        chat_history_str = f"""
Previous Query Analysis:
- Story: {prev_story}
- Key Reasoning: {prev_reasoning}
- SQL Elements Used: Tables={inherited_elements.get('tables', [])}, 
  Time Filters={inherited_elements.get('time_filters', [])}, 
  Geographic Filters={inherited_elements.get('geo_filters', [])},
  Premium Filters={inherited_elements.get('premium_filters', [])}
        """
    else:
        chat_history_str = "(No previous context)"

    # Enhanced prompt for better context extraction
    prompt = f"""
You are an expert context analyst for insurance analytics queries. Your analysis will guide 
database schema selection, so provide structured, actionable context information.

PREVIOUS INTERACTION CONTEXT:
{chat_history_str}

CURRENT USER QUESTION:
"{user_question}"

ANALYSIS TASKS:

STEP 1 - RELATIONSHIP ANALYSIS:
Determine if the current question builds upon, modifies, or references the previous context.
Look for these relationship indicators:
- Comparative terms: "what about", "how about", "similar", "same but"
- Referential terms: "those", "them", "it", "that data"
- Modification terms: "increase to", "change to", "instead of"
- Expansion terms: "also show", "include", "add"

STEP 2 - CONTEXT TYPE CLASSIFICATION:
If this is a followup, classify the type:
- FILTER_MODIFICATION: Changing filters (amount thresholds, date ranges, locations)
- ENTITY_EXPANSION: Same analysis but different entity set (states, policy types)
- METRIC_CHANGE: Same data but different calculations or groupings
- COMPARISON_REQUEST: Comparing current request to previous results
- REFINEMENT: Narrowing down or expanding previous results

STEP 3 - ACTIONABLE CONTEXT EXTRACTION:
Extract specific elements that should inform schema selection:
- Time periods to maintain or modify
- Geographic constraints to keep or change
- Policy types or categories to focus on
- Premium ranges or other numeric filters
- Table relationships that should be preserved

OUTPUT REQUIREMENTS:
{{
    "is_followup": true/false,
    "confidence_level": "high/medium/low",
    "context_type": "FILTER_MODIFICATION|ENTITY_EXPANSION|METRIC_CHANGE|COMPARISON_REQUEST|REFINEMENT|NEW_QUERY",
    "inherited_elements": {{
        "preserve_time_filters": true/false,
        "preserve_geographic_filters": true/false, 
        "preserve_policy_types": true/false,
        "modify_premium_threshold": "specific instruction or null",
        "table_relationship_hint": "suggestion for schema selection"
    }},
    "user_context": "concise instruction for schema selection agent"
}}

Respond with valid JSON only. Focus on actionable context that helps with table/column selection.
"""

    messages = [{"role": "user", "content": prompt}]
    payload = {
        "username": "ENHANCED_INTENT_CLASSIFIER",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.05,
        "max_tokens": 1024
    }
    
    try:
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        content = resp.json()['choices'][0]['message']['content']
        
        # Parse JSON safely
        start = content.find("{")
        end = content.rfind("}") + 1
        parsed = json.loads(content[start:end])
        
        is_followup = parsed.get("is_followup", False)
        context_type = parsed.get("context_type", "NEW_QUERY")
        user_context = parsed.get("user_context", None)
        inherited_elements = parsed.get("inherited_elements", {})
        
        print(f"[INTENT] Follow-up: {is_followup}, Type: {context_type}")
        if user_context:
            print(f"[INTENT] Context for schema selection: {user_context}")
        
        # Return enhanced context structure
        return {
            "is_followup": is_followup,
            "user_context": user_context,
            "context_type": context_type,
            "inherited_elements": inherited_elements
        }
        
    except Exception as ex:
        print(f"[WARN] Intent classification error: {ex}")
        return {
            "is_followup": False,
            "user_context": None,
            "context_type": "NEW_QUERY",
            "inherited_elements": {}
        }

def extract_sql_context_elements(sql_query):
    """
    Helper function to extract reusable elements from previous SQL queries.
    This helps the intent classifier provide better context to schema selection.
    """
    elements = {
        "tables": [],
        "time_filters": [],
        "geo_filters": [],
        "premium_filters": [],
        "grouping_patterns": []
    }
    
    if not sql_query:
        return elements
    
    sql_lower = sql_query.lower()
    
    # Extract table patterns (simple pattern matching)
    common_tables = ['inforce_data_final', 'auto_vehicle_premium_detail', 'auto_vehicle_level_data', 
                    'coverage_limit_details', 'cat_policy', 'claims_data']
    for table in common_tables:
        if table in sql_lower:
            elements["tables"].append(table)
    
    # Extract filter patterns
    if 'year(' in sql_lower or 'date' in sql_lower:
        elements["time_filters"].append("date_filtering_used")
    
    if 'state' in sql_lower or 'tx' in sql_lower or 'ca' in sql_lower:
        elements["geo_filters"].append("state_filtering_used")
        
    if 'premium' in sql_lower and ('>' in sql_lower or '<' in sql_lower):
        elements["premium_filters"].append("premium_threshold_used")
        
    if 'group by' in sql_lower:
        elements["grouping_patterns"].append("grouping_applied")
    
    return elements

# ------------------------------- LOAD TABLES SCHEMA (UNCHANGED) -------------------------------------

# Load table descriptions (schema, table, and per-table details)
table_catalog = pd.read_excel(
    "hackathon_final_schema_file_v1.xlsx",
    sheet_name='Table_descriptions'
)[['DATABASE', 'SCHEMA', 'TABLE', 'Brief_Description', 'Detailed_Comments']]

# Load column descriptions (per-table, per-column, and EDA)
column_catalog_df = pd.read_excel(
    "hackathon_final_schema_file_v1.xlsx",
    sheet_name="Table's Column Summaries"
)[['Table Name', 'Feature Name', 'Data Type', 'Description', 'sample_100_distinct']]

SAMPLE_100_VALUE_FIELD = "sample_100_distinct"  

# -- Convert to list-of-dicts for uniform access --
all_tables = [
    {
        "database": row["DATABASE"],
        "schema": row["SCHEMA"],
        "table_name": row["TABLE"],
        "brief_description": row["Brief_Description"],
        "detailed_comments": row["Detailed_Comments"]
    }
    for idx, row in table_catalog.iterrows()
]

all_columns = [
    {
        "table_name": row["Table Name"],
        "column_name": row["Feature Name"],
        "data_type": row["Data Type"],
        "description": row["Description"],
        "sample_100_distinct": row[SAMPLE_100_VALUE_FIELD]
        
    }
    for idx, row in column_catalog_df.iterrows()
]

# ------------------ REVOLUTIONARY: GPT-POWERED JOIN EXTRACTION -------------------------

def extract_join_relationships_using_gpt(all_tables, all_columns, gpt_object):
    """
    This is our revolutionary approach: instead of using complex regex patterns to extract
    join information, we leverage GPT's natural language understanding to read brief 
    descriptions and identify table relationships.
    
    Think of this function as teaching GPT to be a skilled database analyst who can read
    documentation and immediately understand how different tables connect to each other.
    This approach is far more flexible and intelligent than pattern matching.
    
    The key insight: GPT excels at understanding context and extracting structured information
    from unstructured text. Brief descriptions are exactly the kind of natural language
    that GPT handles beautifully.
    
    Args:
        all_tables: Your table catalog with brief descriptions
        all_columns: Your column catalog for validation
        gpt_object: GPT API object for intelligent extraction
        
    Returns:
        Dictionary containing GPT-extracted join relationships with confidence scores
    """
    print("[GPT_JOIN_EXTRACT] Using GPT's natural language understanding to extract join relationships...")
    
    # First, let's prepare the context that GPT will analyze
    # We're building the same tables_context that you mentioned in your request
    tables_context = "\n".join(
        [f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}: {tbl['brief_description']}" for tbl in all_tables])
    
    # Create a quick lookup of available columns for validation
    # This helps GPT verify that suggested join keys actually exist
    available_columns_by_table = {}
    for col in all_columns:
        table_name = col['table_name']
        if table_name not in available_columns_by_table:
            available_columns_by_table[table_name] = []
        available_columns_by_table[table_name].append({
            'name': col['column_name'],
            'type': col['data_type'],
            'description': col['description']
        })
    
    # Build column context for GPT to understand what columns are available
    # This prevents GPT from suggesting join keys that don't exist
    column_context = ""
    for table_name, columns in available_columns_by_table.items():
        column_names = [col['name'] for col in columns[:10]]  # Limit to first 10 to manage prompt size
        column_context += f"\n{table_name}: Available columns include {', '.join(column_names)}"
    
    # Now we create a sophisticated prompt that teaches GPT to be a database relationship analyst
    # This prompt leverages GPT's strengths in natural language understanding and reasoning
    gpt_prompt = f"""
You are an expert database relationship analyst specializing in insurance domain systems.
Your task is to analyze table descriptions and extract concrete join relationships.

You excel at understanding natural language descriptions and identifying how different 
database entities connect to each other. Use your language understanding capabilities
to read between the lines and infer relationships even when they're not explicitly stated.

INSURANCE DOMAIN CONTEXT:
- Policy data is typically central, with other tables linking to it
- Common join patterns include POL_TX_ID, PKEY, KEY_POL_RSK_ITM_ID, POLICY_ID
- Auto insurance often separates policy, vehicle, and premium information
- CAT (catastrophic) insurance has specialized tables and relationships
- Claims data connects to policies through policy identifiers

TABLE DESCRIPTIONS TO ANALYZE:
{tables_context}

AVAILABLE COLUMNS FOR VALIDATION:
{column_context}

ANALYSIS INSTRUCTIONS:

STEP 1 - SEMANTIC UNDERSTANDING:
Read each table description carefully. Understand what business entity or process each table represents.
Look for explicit mentions of relationships like "joins with", "links to", "connected via".
Also infer implicit relationships based on business logic and naming patterns.

STEP 2 - JOIN KEY IDENTIFICATION:
For each relationship you identify, determine the specific column names used for joining.
Pay attention to mentions of specific field names in the descriptions.
Cross-reference with the available columns to ensure suggested join keys actually exist.
Consider common insurance domain patterns when keys aren't explicitly mentioned.

STEP 3 - CONFIDENCE ASSESSMENT:
Rate each relationship's confidence based on:
- HIGH: Explicitly mentioned joins with specific column names
- MEDIUM: Clear business relationship with likely join keys
- LOW: Inferred relationship that may need validation

STEP 4 - BUSINESS LOGIC VALIDATION:
Ensure that identified relationships make business sense in insurance context.
Policy tables should be central, with other entities linking to them.
Verify that join directions are logical (child tables join to parent tables).

OUTPUT REQUIREMENTS:
Provide a comprehensive JSON response with this structure:

{{
    "analysis_summary": {{
        "total_tables_analyzed": number,
        "relationships_identified": number,
        "high_confidence_joins": number,
        "domain_insights": "key insights about the table relationships"
    }},
    "table_join_keys": {{
        "TABLE_NAME": ["list", "of", "join", "keys", "for", "this", "table"],
        // ... for each table that has join capabilities
    }},
    "join_relationships": [
        {{
            "table_a": "TABLE_NAME_A",
            "table_b": "TABLE_NAME_B", 
            "join_key_a": "COLUMN_NAME_A",
            "join_key_b": "COLUMN_NAME_B",
            "relationship_type": "one-to-many|many-to-one|one-to-one",
            "confidence": "high|medium|low",
            "source_evidence": "quote or paraphrase from description that indicates this relationship",
            "business_logic": "explanation of why this relationship makes sense"
        }}
        // ... for each identified relationship
    ],
    "validation_notes": [
        "Any concerns or recommendations about the identified relationships"
    ]
}}

CRITICAL SUCCESS FACTORS:
1. Only suggest join keys that exist in the available columns list
2. Focus on relationships that make business sense in insurance domain
3. Provide clear evidence for each relationship you identify
4. Be conservative with confidence ratings - it's better to be cautious

Use your natural language understanding to extract maximum intelligence from these descriptions.
Respond with ONLY the JSON structure above - no additional text.
"""

    # Execute the GPT analysis using your existing API structure
    messages = [{"role": "user", "content": gpt_prompt}]
    payload = {
        "username": "GPT_JOIN_RELATIONSHIP_ANALYST",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.1,  # Low temperature for consistent analysis
        "max_tokens": 3072   # Larger token limit for comprehensive analysis
    }
    
    try:
        print("[GPT_JOIN_EXTRACT] Sending brief descriptions to GPT for intelligent analysis...")
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        content = resp.json()['choices'][0]['message']['content']
        
        # Parse GPT's structured response
        start = content.find('{')
        end = content.rfind('}') + 1
        if start == -1 or end == 0:
            raise ValueError("No valid JSON found in GPT response")
            
        parsed_response = json.loads(content[start:end])
        
        # Extract and validate the results
        analysis_summary = parsed_response.get('analysis_summary', {})
        table_join_keys = parsed_response.get('table_join_keys', {})
        join_relationships = parsed_response.get('join_relationships', [])
        validation_notes = parsed_response.get('validation_notes', [])
        
        # Log GPT's analysis insights
        print(f"[GPT_JOIN_EXTRACT] ‚úÖ GPT Analysis Complete:")
        print(f"  üìä Tables Analyzed: {analysis_summary.get('total_tables_analyzed', 'Unknown')}")
        print(f"  üîó Relationships Found: {analysis_summary.get('relationships_identified', 'Unknown')}")
        print(f"  üíé High Confidence: {analysis_summary.get('high_confidence_joins', 'Unknown')}")
        
        if analysis_summary.get('domain_insights'):
            print(f"  üß† Domain Insights: {analysis_summary['domain_insights']}")
        
        # Display some example relationships for debugging
        high_conf_relationships = [r for r in join_relationships if r.get('confidence') == 'high']
        if high_conf_relationships:
            print(f"[GPT_JOIN_EXTRACT] üü¢ Sample High Confidence Relationships:")
            for rel in high_conf_relationships[:3]:  # Show first 3
                print(f"    {rel['table_a']}.{rel['join_key_a']} = {rel['table_b']}.{rel['join_key_b']}")
                print(f"      Evidence: {rel.get('source_evidence', 'Not provided')[:80]}...")
        
        # Return structured results in the format expected by your system
        return {
            'table_join_keys': table_join_keys,
            'join_relationships': join_relationships,
            'available_columns': available_columns_by_table,
            'gpt_analysis': {
                'analysis_summary': analysis_summary,
                'validation_notes': validation_notes,
                'extraction_method': 'gpt_natural_language_understanding'
            }
        }
        
    except Exception as ex:
        print(f"[ERROR] GPT join extraction failed: {ex}")
        # Provide a fallback empty structure so the system can continue
        return {
            'table_join_keys': {},
            'join_relationships': [],
            'available_columns': available_columns_by_table,
            'gpt_analysis': {
                'error': str(ex),
                'extraction_method': 'failed_gpt_extraction'
            }
        }

def build_gpt_extracted_join_context(join_info, relevant_tables=None):
    """
    Transform GPT's extracted join analysis into clear guidance for schema selection.
    This function takes GPT's intelligent analysis and formats it in a way that 
    provides concrete, actionable guidance to downstream processes.
    
    Think of this as translating GPT's analysis into a format that's optimized
    for decision-making in your schema selection process.
    """
    join_relationships = join_info.get('join_relationships', [])
    gpt_analysis = join_info.get('gpt_analysis', {})
    
    # Filter to relevant tables if specified
    if relevant_tables:
        relevant_table_names = [t.split('.')[-1] for t in relevant_tables]
        relevant_joins = []
        for join in join_relationships:
            if (join['table_a'] in relevant_table_names or 
                join['table_b'] in relevant_table_names):
                relevant_joins.append(join)
        join_relationships = relevant_joins
    
    if not join_relationships:
        return "No GPT-extracted join relationships found for the selected tables."
    
    # Organize by confidence level as GPT determined them
    high_confidence = [j for j in join_relationships if j.get('confidence') == 'high']
    medium_confidence = [j for j in join_relationships if j.get('confidence') == 'medium']
    low_confidence = [j for j in join_relationships if j.get('confidence') == 'low']
    
    # Build comprehensive context string
    context_parts = ["GPT-EXTRACTED JOIN RELATIONSHIPS (from brief description analysis):"]
    
    # Add GPT's domain insights if available
    if gpt_analysis.get('analysis_summary', {}).get('domain_insights'):
        context_parts.append(f"\nüß† GPT DOMAIN INSIGHTS:")
        context_parts.append(f"   {gpt_analysis['analysis_summary']['domain_insights']}")
    
    if high_confidence:
        context_parts.append("\nüü¢ HIGH CONFIDENCE JOINS (GPT identified with strong evidence):")
        for join in high_confidence:
            evidence = join.get('source_evidence', 'Direct evidence from description')[:60] + "..."
            context_parts.append(f"   {join['table_a']}.{join['join_key_a']} = {join['table_b']}.{join['join_key_b']}")
            context_parts.append(f"     üìù Evidence: {evidence}")
            if join.get('business_logic'):
                context_parts.append(f"     üíº Logic: {join['business_logic'][:60]}...")
    
    if medium_confidence:
        context_parts.append("\nüü° MEDIUM CONFIDENCE JOINS (GPT inferred from business logic):")
        for join in medium_confidence:
            context_parts.append(f"   {join['table_a']}.{join['join_key_a']} = {join['table_b']}.{join['join_key_b']}")
            if join.get('business_logic'):
                context_parts.append(f"     üíº Logic: {join['business_logic'][:60]}...")
    
    if low_confidence:
        context_parts.append("\nüü† LOW CONFIDENCE JOINS (use with caution):")
        for join in low_confidence:
            context_parts.append(f"   {join['table_a']}.{join['join_key_a']} = {join['table_b']}.{join['join_key_b']}")
    
    # Add GPT's validation notes if available
    validation_notes = gpt_analysis.get('validation_notes', [])
    if validation_notes:
        context_parts.append("\n‚ö†Ô∏è  GPT VALIDATION NOTES:")
        for note in validation_notes[:3]:  # Limit to first 3 notes
            context_parts.append(f"   ‚Ä¢ {note}")
    
    # Add usage guidance based on GPT's analysis
    context_parts.append("\nüìã USAGE GUIDANCE (based on GPT analysis):")
    context_parts.append("‚Ä¢ Prefer high confidence joins - GPT found strong evidence in descriptions")
    context_parts.append("‚Ä¢ Medium confidence joins are business-logic inferred - validate carefully")
    context_parts.append("‚Ä¢ All suggested keys have been validated against available columns")
    context_parts.append("‚Ä¢ GPT has applied insurance domain knowledge to relationship identification")
    
    return "\n".join(context_parts)

# ------------------ PROMPT UTILITIES (UNCHANGED) -------------------------

def build_column_context_block(relevant_columns, all_columns, max_sample_vals=15):
    """
    Return a string giving, for all columns-of-interest, precise metadata block for GPT prompt.
    Shows at most `max_sample_vals` sample values for each column.
    """
    blocks = []
    for rel in relevant_columns:
        tname, cname = rel["table_name"], rel["column_name"]
        col_row = next(
            (c for c in all_columns if c["table_name"] == tname.split('.')[-1] and c["column_name"] == cname),
            None
        )
        if not col_row:
            continue  # missing in catalog (shouldn't happen)

        # --- Process sample_100_distinct string to limit values ---
        raw_samples = col_row.get('sample_100_distinct', '')
        # Try to parse it either as comma separated, or from a string list
        if isinstance(raw_samples, str):
            # Remove outer brackets if present (e.g. "[a, b, c]")
            s = raw_samples.strip()
            if s.startswith("[") and s.endswith("]"):
                s = s[1:-1]
            # Now, split
            vals = [v.strip("'\" ") for v in s.split(",") if v.strip().strip("'\"")]
        elif isinstance(raw_samples, list):
            vals = raw_samples
        else:
            vals = []

        # Limit the number of sample values displayed
        limited_vals = vals[:max_sample_vals]
        if len(vals) > max_sample_vals:
            limited_vals.append("...")  # Indicate truncation

        block = (
            f"\n ||| Table_name: {tname} "
            f"|| Feature_name: {col_row['column_name']} "
            f"|| Data_Type: {col_row['data_type']} "
            f"|| Description: {col_row['description']} "
            f"|| Sample Values (separated by ,): {', '.join(limited_vals)} ||| \n"
        )
        blocks.append(block)
    return "".join(blocks)

# ------------------ GPT-POWERED SCHEMA SELECTION --------------------------

def extract_unique_joins(columns):
    """
    Given a list of column dicts, extract unique {table_name, join_key} pairs.
    """
    join_pairs = set()
    for col in columns:
        # Safely handle 'JOIN_KEY' possibly missing in fallback scenario
        join_key = col.get('JOIN_KEY', None)
        table_name = col.get('table_name', None)
        if join_key and table_name:
            join_pairs.add( (table_name, join_key) )
    # Convert set of tuples to list of dicts if preferred
    return [ {"table_name": t, "join_key": k} for (t,k) in sorted(join_pairs) ]

def match_query_to_schema(user_question, all_tables, all_columns, gpt_object=None, 
                         user_context=None, intent_info=None):
    """
    REVOLUTIONIZED schema selection using GPT's natural language understanding
    to extract join relationships from brief descriptions.
    
    This represents a fundamental shift from pattern-matching approaches to leveraging
    the core strength of language models: understanding natural language and extracting
    structured intelligence from unstructured text.
    
    The key breakthrough: instead of teaching our system complex rules about how
    join information might be written, we let GPT use its natural language understanding
    to read your documentation and extract the relationships intelligently.
    
    Think of this as the difference between programming a robot to recognize faces
    by writing rules about nose shapes and eye distances, versus training it to 
    understand faces the way humans do - through pattern recognition and contextual understanding.
    """
    uq = user_question.lower()
    matched_tables = []
    matched_columns = []
    print(f"[SCHEMA] Processing query with GPT-powered join extraction: {user_question}")

    # --- GPT-POWERED APPROACH: NATURAL LANGUAGE UNDERSTANDING ---
    if gpt_object is not None:
        # Build the standard contexts as before
        tables_context = "\n".join(
            [f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}: {tbl['brief_description']}" for tbl in all_tables])
        columns_context = "\n".join(
            [f"{col['table_name']}.{col['column_name']}: {col['description']}" for col in all_columns])
        
        # üöÄ THE BREAKTHROUGH: Use GPT to intelligently extract join relationships
        # This leverages what GPT does best - understanding natural language narratives
        print("[SCHEMA] üß† Leveraging GPT's natural language understanding for join extraction...")
        gpt_join_info = extract_join_relationships_using_gpt(all_tables, all_columns, gpt_object)
        
        # Transform GPT's analysis into actionable guidance
        gpt_join_context = build_gpt_extracted_join_context(gpt_join_info)
        
        # Enhanced context building using intent information
        context_text = ""
        domain_hints = ""
        
        if intent_info and intent_info.get("is_followup"):
            context_type = intent_info.get("context_type", "")
            inherited_elements = intent_info.get("inherited_elements", {})
            
            context_text = f"\nPREVIOUS QUERY CONTEXT:\n"
            context_text += f"Context Type: {context_type}\n"
            
            if user_context:
                context_text += f"Context Details: {user_context}\n"
            
            # Provide specific guidance based on context type
            if context_type == "FILTER_MODIFICATION":
                domain_hints = "\n[GUIDANCE] This is a filter modification - preserve table relationships but adjust filter conditions.\n"
            elif context_type == "ENTITY_EXPANSION":
                domain_hints = "\n[GUIDANCE] This expands the entity scope - maintain similar table structure but consider additional entity tables.\n"
            elif context_type == "METRIC_CHANGE":
                domain_hints = "\n[GUIDANCE] This changes metrics/calculations - focus on tables that support the new metric requirements.\n"
            
            # Add specific inherited element guidance
            if inherited_elements.get("preserve_time_filters"):
                domain_hints += "[HINT] Preserve time-based filtering capabilities from previous query.\n"
            if inherited_elements.get("preserve_geographic_filters"):
                domain_hints += "[HINT] Maintain geographic filtering capabilities.\n"
        
        # üéØ REVOLUTIONARY PROMPT: Leveraging GPT's join intelligence
        gpt_prompt = f"""
You are an expert insurance database architect with access to intelligent join relationship
analysis performed by GPT on your table brief descriptions. You no longer need to guess
how tables connect - you have GPT's natural language understanding working for you.

CRITICAL DOMAIN KNOWLEDGE:
- Schema 'RAW_CI_CAT_ANALYSIS': Contains catastrophic insurance policies, CAT events, CAT-specific coverage limits
- Schema 'RAW_CI_INFORCE': Contains active policies, auto vehicle details, standard premium information
- Schema 'RAW_CI_CLAIMS': Contains claims data, settlements, adjustments

{context_text}

{domain_hints}

USER QUESTION: {user_question}

AVAILABLE SCHEMA:
Tables and Descriptions: {tables_context}

{gpt_join_context}

Available Columns: {columns_context}

ENHANCED REASONING WITH GPT JOIN INTELLIGENCE:

STEP 1 - DOMAIN CLASSIFICATION:
Classify the query type based on insurance domain:
- AUTO: Auto insurance policies and vehicles (use RAW_CI_INFORCE schema primarily)
- CAT: Catastrophic events and policies (use RAW_CI_CAT_ANALYSIS schema)
- CLAIMS: Claims processing and settlements (use RAW_CI_CLAIMS schema)  
- GENERAL: Cross-domain analysis (may need multiple schemas)

STEP 2 - INTELLIGENT TABLE SELECTION WITH GPT JOIN ANALYSIS:
Based on domain classification and GPT's relationship analysis:
- Start with the primary table that contains the main entity (policies, claims, vehicles)
- Add only tables that provide additional required attributes
- Use GPT's extracted join relationships to ensure selected tables can be properly connected
- Prefer tables that GPT identified with high confidence join relationships
- Consider context preservation requirements if this is a follow-up query

STEP 3 - GPT-GUIDED JOIN PATH SELECTION:
Using GPT's intelligent relationship extraction:
- Select only join relationships that GPT identified from brief description analysis
- Strongly prefer HIGH CONFIDENCE joins (üü¢) - GPT found strong evidence
- Use MEDIUM CONFIDENCE joins (üü°) with validation - GPT inferred from business logic
- Avoid LOW CONFIDENCE joins unless absolutely necessary
- Trust GPT's business logic reasoning about why relationships exist

STEP 4 - PRECISE COLUMN SELECTION:
Select only columns that directly answer the question:
- For filtering: columns used in WHERE clauses
- For display: columns requested in output  
- For grouping: columns used for aggregation
- For calculation: columns needed for mathematical operations

ENHANCED VALIDATION RULES WITH GPT INTELLIGENCE:
1. Every selected table MUST be connected through GPT-extracted join relationships
2. Use ONLY the exact join keys that GPT identified and validated
3. Prioritize relationships where GPT provided strong evidence from descriptions
4. Trust GPT's domain knowledge application in relationship identification
5. Double-check that column names match exactly (GPT has already validated existence)
6. If context guidance suggests preserving relationships, prioritize those patterns
7. LEVERAGE: GPT's natural language understanding has eliminated guesswork

OUTPUT REQUIREMENTS:
Provide your response as valid JSON with this exact structure:

{{
    "domain_classification": "AUTO|CAT|CLAIMS|GENERAL",
    "context_application": "How you applied the context guidance to your selection",
    "gpt_join_utilization": "How you leveraged GPT's relationship analysis",
    "reasoning_steps": {{
        "domain_rationale": "Why you classified the query this way",
        "table_selection_logic": "Why these specific tables using GPT's relationship insights",
        "join_path_explanation": "Specific GPT-extracted joins used and their confidence levels",
        "column_selection_rationale": "Why these specific columns answer the question"
    }},
    "relevant_tables_joins": [
        {{
            "table_name": "SCHEMA.TABLE_NAME",
            "join_keys": ["KEY1", "KEY2"],
            "selection_reason": "Why this table is essential",
            "gpt_join_evidence": ["GPT evidence that supports including this table"]
        }}
    ],
    "relevant_columns": [
        {{
            "table_name": "TABLE_NAME", 
            "column_name": "COLUMN_NAME",
            "usage_purpose": "filtering|display|grouping|calculation"
        }}
    ]
}}

CRITICAL SUCCESS FACTOR: Leverage GPT's natural language understanding of your brief descriptions.
This represents the evolution from rule-based to intelligence-based schema selection.

Respond with ONLY valid JSON. No additional text or explanations outside the JSON structure.
        """
        
        messages = [{"role": "user", "content": gpt_prompt}]
        payload = {
            "username": "GPT_INTELLIGENT_SCHEMA_AGENT",
            "session_id": "1",
            "messages": messages,
            "temperature": 0.001,  # Very low temperature for consistent schema selection
            "max_tokens": 2048
        }
        try:
            resp = gpt_object.get_gpt_response_non_streaming(payload)
            content = resp.json()['choices'][0]['message']['content']
            first = content.find('{')
            last = content.rfind('}') + 1
            parsed = json.loads(content[first:last])
            
            # Extract results with GPT join intelligence validation
            if "relevant_tables_joins" in parsed:
                matched_tables_joins = parsed["relevant_tables_joins"]
                # Enhanced validation using GPT's extracted join relationships
                validated_tables_joins = []
                for table_info in matched_tables_joins:
                    table_name = table_info.get("table_name", "")
                    join_keys = table_info.get("join_keys", [])
                    
                    # Validate against GPT's extracted join information
                    table_name_clean = table_name.split('.')[-1]
                    if table_name_clean in gpt_join_info['table_join_keys']:
                        gpt_available_keys = gpt_join_info['table_join_keys'][table_name_clean]
                        valid_keys = [key for key in join_keys if key in gpt_available_keys]
                        if valid_keys:
                            validated_table = table_info.copy()
                            validated_table["join_keys"] = valid_keys
                            validated_tables_joins.append(validated_table)
                            print(f"[SCHEMA] ‚úÖ GPT-validated table {table_name_clean} with keys {valid_keys}")
                        else:
                            print(f"[SCHEMA] ‚ùå No GPT-extracted join keys found for table {table_name_clean}")
                    else:
                        print(f"[SCHEMA] ‚ö†Ô∏è  Table {table_name_clean} not in GPT's relationship analysis")
                
                matched_tables_joins = validated_tables_joins
            else:
                matched_tables_joins = []
                
            matched_columns = parsed.get("relevant_columns", [])
            
            # Print enhanced debugging information showing GPT intelligence utilization
            print(f"[SCHEMA] Domain Classification: {parsed.get('domain_classification', 'Unknown')}")
            if parsed.get('context_application'):
                print(f"[SCHEMA] Context Applied: {parsed['context_application']}")
            if parsed.get('gpt_join_utilization'):
                print(f"[SCHEMA] üß† GPT Intelligence Used: {parsed['gpt_join_utilization']}")
            
            if "reasoning_steps" in parsed:
                reasoning = parsed["reasoning_steps"]
                print(f"[SCHEMA] Table Selection Logic: {reasoning.get('table_selection_logic', 'Not provided')}")
                print(f"[SCHEMA] Join Path: {reasoning.get('join_path_explanation', 'Not provided')}")
                
        except Exception as ex:
            print(f"[WARN] GPT-powered schema selection error: {ex}")
            matched_tables_joins = []
            matched_columns = []

    # --- Enhanced fallback that still uses GPT join extraction ---
    if not matched_tables_joins and not matched_columns:
        print("[SCHEMA] Falling back to keyword matching with GPT join enhancement")
        
        # Even in fallback, we can leverage GPT's join extraction
        if gpt_object:
            gpt_join_info = extract_join_relationships_using_gpt(all_tables, all_columns, gpt_object)
        else:
            gpt_join_info = {'table_join_keys': {}, 'join_relationships': []}
        
        keywords = set(uq.replace(",", " ").replace("_", " ").split())
        # Find relevant tables
        matched_table_objs = [
            tbl
            for tbl in all_tables
            if any(k in (tbl['table_name'].lower() + " " +
                         (str(tbl['brief_description']) or "")).lower()
                   for k in keywords)
        ]
        matched_tables_joins = [
            {
                "table_name": f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}",
                # Use GPT-extracted join keys even in fallback
                "join_keys": gpt_join_info['table_join_keys'].get(tbl['table_name'], [])
            }
            for tbl in matched_table_objs
        ]
        matched_columns = [
            {"table_name": col['table_name'], "column_name": col['column_name']}
            for col in all_columns
            if any(k in (col['column_name'] + " " +
                         (str(col['description']) or "")).lower()
                   for k in keywords)
        ]

    # Generate matched_joins using GPT's intelligent extraction
    matched_joins = []
    seen_pairs = set()
    
    # First, try to get joins from our enhanced table information
    for t in matched_tables_joins:
        tname = t.get("table_name") or t
        join_keys = t.get("join_keys", [])
        if isinstance(join_keys, list):
            for key in join_keys:
                if (tname, key) not in seen_pairs:
                    matched_joins.append({"table_name": tname, "join_key": key})
                    seen_pairs.add((tname, key))
    
    # If we need more joins and have GPT analysis, use those relationships
    if len(matched_joins) < 2 and len(matched_tables_joins) > 1:
        # Get GPT join info if we don't already have it
        if 'gpt_join_info' not in locals() and gpt_object:
            gpt_join_info = extract_join_relationships_using_gpt(all_tables, all_columns, gpt_object)
        elif 'gpt_join_info' not in locals():
            gpt_join_info = {'join_relationships': []}
        
        table_names = [t.get("table_name", "").split('.')[-1] for t in matched_tables_joins]
        for join_rel in gpt_join_info.get('join_relationships', []):
            if join_rel['table_a'] in table_names and join_rel['table_b'] in table_names:
                # Prefer high confidence joins from GPT
                if join_rel.get('confidence') in ['high', 'medium']:
                    if (join_rel['table_a'], join_rel['join_key_a']) not in seen_pairs:
                        matched_joins.append({
                            "table_name": join_rel['table_a'], 
                            "join_key": join_rel['join_key_a']
                        })
                        seen_pairs.add((join_rel['table_a'], join_rel['join_key_a']))
                    if (join_rel['table_b'], join_rel['join_key_b']) not in seen_pairs:
                        matched_joins.append({
                            "table_name": join_rel['table_b'], 
                            "join_key": join_rel['join_key_b']
                        })
                        seen_pairs.add((join_rel['table_b'], join_rel['join_key_b']))
    
    # Final fallback to original logic if still no joins
    if not matched_joins:
        matched_joins = extract_unique_joins(matched_columns)

    print(f"[SCHEMA] üéØ GPT-powered schema selection complete: {len(matched_tables_joins)} tables, {len(matched_columns)} columns, {len(matched_joins)} join points")
    return matched_tables_joins, matched_columns, matched_joins

# ------------------ ENHANCED FIRST TOOL CALL --------------------------

def first_tool_call(state):
    """
    Enhanced Node 1: Schema selection powered by GPT's natural language understanding
    of your brief descriptions. This represents the evolution from rule-based to 
    intelligence-based database relationship discovery.
    """
    user_question = state.get("user_question")
    all_tables    = state.get("table_schema")
    all_columns   = state.get("columns_info")
    gpt_object    = state.get("gpt_object", None)

    # Enhanced: get both legacy and new intent context
    user_context = state.get("user_context", None)
    intent_info = state.get("intent_info", None)
    
    print(f"[TOOL1] Processing schema selection with GPT-powered natural language understanding")
    
    # Call our revolutionized schema selection that leverages GPT's language capabilities
    relevant_tables_joins, relevant_columns, relevant_joins = match_query_to_schema(
        user_question, all_tables, all_columns, gpt_object, 
        user_context=user_context, intent_info=intent_info)
    
    state["relevant_tables_joins"] = relevant_tables_joins
    state["relevant_columns"] = relevant_columns
    state["relevant_joins"] = relevant_joins
    
    print(f"[TOOL1] üéØ GPT-powered schema selection complete: {len(relevant_tables_joins)} tables identified")
    return state

# -------------------- SQL GENERATION AGENT (ENHANCED FOR GPT JOIN INTELLIGENCE) ----------------------

def query_gen_node(state):
    """
    Enhanced Node 2: SQL generation leveraging GPT's intelligent join relationship analysis.
    The SQL generator now benefits from GPT's natural language understanding of your
    table relationships, leading to more accurate and reliable query construction.
    """
    user_question = state.get("user_question")
    all_columns = state.get("columns_info")
    relevant_columns = state.get("relevant_columns")
    relevant_joins=state.get("relevant_tables_joins")
    gpt_object = state.get("gpt_object")
    intent_info = state.get("intent_info", {})

    # Build context for the question
    context_block = build_column_context_block(relevant_columns, all_columns, 15)
    print(f"[TOOL2] Generating SQL with {len(relevant_columns)} selected columns and GPT-analyzed joins")

    # Get GPT's join intelligence for SQL generation context
    all_tables = state.get("table_schema", [])
    if gpt_object:
        gpt_join_info = extract_join_relationships_using_gpt(all_tables, all_columns, gpt_object)
        selected_table_names = [t.get("table_name", "").split('.')[-1] for t in relevant_joins]
        sql_join_context = build_gpt_extracted_join_context(gpt_join_info, selected_table_names)
    else:
        sql_join_context = "GPT join analysis not available - using fallback approach"

    # Enhanced context building for SQL generation
    context_guidance = ""
    if intent_info.get("is_followup"):
        context_type = intent_info.get("context_type", "")
        inherited_elements = intent_info.get("inherited_elements", {})
        
        context_guidance = f"\nCONTEXT GUIDANCE FOR SQL GENERATION:\n"
        context_guidance += f"Query Type: {context_type}\n"
        
        if context_type == "FILTER_MODIFICATION":
            context_guidance += "- This modifies filters from a previous query - maintain similar structure but adjust WHERE conditions\n"
        elif context_type == "ENTITY_EXPANSION":
            context_guidance += "- This expands the scope of entities - consider similar grouping and aggregation patterns\n"
        elif context_type == "METRIC_CHANGE":
            context_guidance += "- This changes metrics/calculations - focus on new aggregation requirements\n"
        
        if inherited_elements.get("modify_premium_threshold"):
            context_guidance += f"- Premium threshold guidance: {inherited_elements['modify_premium_threshold']}\n"

    with open('few_shot_examples.txt', 'r', encoding='utf-8') as f:
        few_shot_examples = f.read()

    # Enhanced SQL generation prompt leveraging GPT's join intelligence
    gpt_prompt = f"""
You are a strict SQL query generator specialized in insurance analytics on Snowflake.
You have access to GPT's intelligent analysis of table relationships extracted from brief descriptions.
This represents the evolution from guesswork to intelligence-based join selection.

{context_guidance}

**User Question:**
```{user_question}```

**GPT-Analyzed Join Relationships for Selected Tables:**
{sql_join_context}

**Relevant Tables and Join_Keys:**
```{relevant_joins}```

**Relevant Columns and Tables with Sample Values:**
```{context_block}```

ENHANCED SQL GENERATION WITH GPT JOIN INTELLIGENCE:

Step 1 - PARSE USER REQUEST WITH CONTEXT
‚Ä¢ Identify all metrics, filters, and aggregation needs from the USER_REQUEST
‚Ä¢ Apply any context guidance for follow-up queries
‚Ä¢ For every filter or metric: Map explicitly to a (TABLE.COLUMN)
‚Ä¢ If not available, mark as "N/A: Not found", and DO NOT use or guess in SQL

Step 2 - LEVERAGE GPT'S INTELLIGENT JOIN ANALYSIS  
‚Ä¢ Use ONLY the join relationships that GPT extracted through natural language understanding
‚Ä¢ Determine the minimal set of tables needed to supply all required columns
‚Ä¢ Connect tables using GPT's analyzed relationships, prioritizing high confidence joins
‚Ä¢ Record each join using exact syntax: <TABLE1> JOIN <TABLE2> ON <TABLE1>.<KEY> = <TABLE2>.<KEY>
‚Ä¢ Trust GPT's business logic reasoning about why relationships exist

Step 3 - PREPARE DATE CONVERSIONS (SNOWFLAKE-SPECIFIC)
‚Ä¢ For VARCHAR date columns: Use TRY_TO_DATE(<date_field>, 'YYYY-MM-DD')
‚Ä¢ For DATE/TIMESTAMP columns: Use as-is or CAST(<date_field> AS DATE) if needed
‚Ä¢ Never apply TRY_TO_DATE to columns already in DATE/TIMESTAMP format
‚Ä¢ Apply filters using appropriate format for the column type

Step 4 - BUILD OPTIMIZED SQL WITH GPT-VALIDATED JOINS
‚Ä¢ SELECT: Choose mapped fields and aggregations
‚Ä¢ FROM: Use the primary table from GPT's relationship analysis
‚Ä¢ JOIN: Add ONLY the GPT-analyzed join relationships with their confidence levels
‚Ä¢ WHERE: Add mapped, available filters with proper data type handling
‚Ä¢ GROUP BY: Add if user aggregation/grouping is requested and columns are mapped
‚Ä¢ ORDER BY: If required in user request
‚Ä¢ INTELLIGENCE: Leverage GPT's domain knowledge application in relationship selection

Step 5 - COMPREHENSIVE VALIDATION WITH GPT INTELLIGENCE  
‚ñ° Only listed tables/columns used  
‚ñ° All join steps use GPT-analyzed relationships‚Äîno invented joins  
‚ñ° All date logic follows Snowflake conventions  
‚ñ° No columns/filters/joins/objects invented or guessed  
‚ñ° All SQL clauses pass previous mapping and join plan checks
‚ñ° Context guidance properly applied if this is a follow-up query
‚ñ° CRITICAL: All joins validated by GPT's natural language understanding

**Enhanced Output Structure:**
**Mapping:**  
(User requirement ‚Üí Actual DB column mapping. If not found, do not use as filter.)

**GPT Join Intelligence:**
(How GPT's relationship analysis was applied to join selection and validation.)

**Reasoning:**  
(Explain: 1. how context guidance was applied, 2. GPT-guided join strategy, 3. column/filter selection logic, 4. optimization decisions. Max 150 words.)

**SQL Query:**  
(Production-ready SQL using GPT-validated mappings and intelligent joins.)

****FEW-SHOT EXAMPLES:****
{few_shot_examples}
****END EXAMPLES****

From the above, provide valid JSON:
{{
    "GPT Join Intelligence": "how GPT's relationship analysis guided join selection",
    "Reasoning": "detailed explanation including context and GPT intelligence application",
    "SQL Query": "complete optimized SQL query"
}}

Strictly output JSON only. Leverage GPT's natural language understanding for maximum accuracy.
"""
    
    messages = [{"role": "user", "content": gpt_prompt}]
    payload = {
        "username": "GPT_INTELLIGENT_SQL_AGENT",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.001,
        "max_tokens": 2048
    }
    matched_sql_query = []
    matched_reasoning = []

    try:
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        gpt_output = resp.json()['choices'][0]['message']['content']

        first = gpt_output.find('{')
        last = gpt_output.rfind('}')+1
        parsed = json.loads(gpt_output[first:last])
        matched_reasoning = parsed.get("Reasoning", [])
        matched_sql_query = parsed.get("SQL Query", [])
        gpt_join_intelligence = parsed.get("GPT Join Intelligence", "")
        
        print(f"[TOOL2] üéØ SQL generation complete with GPT join intelligence")
        if gpt_join_intelligence:
            print(f"[TOOL2] üß† GPT Intelligence Applied: {gpt_join_intelligence}")
        
    except Exception as ex:
        print(f"[WARN] GPT-powered SQL generation error: {ex}")
        
    state["query_llm_prompt"] = gpt_prompt
    state["query_llm_result"] = gpt_output
    state["query_reasoning"] = matched_reasoning
    state["query_sql"] = matched_sql_query
    return state

# ------------------ SNOWFLAKE SQL EXECUTION (UNCHANGED) --------------------

def run_query_and_return_df(state):
    """
    Executes the SQL query found in state['query_sql'] on Snowflake and returns DataFrame result.
    Updates state['snowflake_result'] with a pandas DataFrame (or None on error).
    """
    query_sql = state.get("query_sql", None)
    if not query_sql:
        print("[ERROR] No SQL found in state['query_sql']")
        state['snowflake_result'] = None
        return state
    
    # Connect to Snowflake
    conn = None
    try:
        print(f"[EXEC] Executing SQL on Snowflake...")
        conn = create_snowflake_connection()
        with conn.cursor() as cursor:
            cursor.execute(query_sql)
            # Fetch all rows and columns
            rows = cursor.fetchall()
            colnames = [d[0] for d in cursor.description]
            df = pd.DataFrame(rows, columns=colnames)
        state['snowflake_result'] = df
        print(f"[EXEC] ‚úÖ Query executed successfully, returned {len(df)} rows")
    except Exception as ex:
        print(f"[ERROR] ‚ùå Snowflake query execution failed: {ex}")
        state['snowflake_result'] = None
    finally:
        if conn is not None:
            conn.close()
    return state
    
# ----------- COMPLETE PIPELINE WITH GPT NATURAL LANGUAGE JOIN INTELLIGENCE -----------

if __name__ == "__main__":
    print("=== üß† GPT-Powered Natural Language Join Intelligence Pipeline ===")
    print("This revolutionary pipeline leverages GPT's natural language understanding")
    print("to extract join relationships directly from your brief descriptions,")
    print("representing the evolution from rule-based to intelligence-based database analysis.\n")
    
    # Example agent history - this simulates previous interactions
    agent_history = [{
        "story": """This chart visualizes auto policies with premium over 10000 in Texas from year 2018 onwards, grouped by year""",
        "reasoning": """The tables INFORCE_DATA_FINAL, AUTO_VEHICLE_PREMIUM_DETAIL, and AUTO_VEHICLE_LEVEL_DATA are joined using POL_TX_ID and KEY_POL_RSK_ITM_ID. The query filters policies with premiums over 10000 in Texas from 2018 onwards. The EFFECTIVEDATE is converted to a date format for filtering. The results are grouped by year.""",
        "sql_query": """SELECT YEAR(TRY_TO_DATE(idf.EFFECTIVEDATE, 'YYYY-MM-DD')) AS policy_year, COUNT(*) AS policy_count FROM INFORCE_DATA_FINAL idf JOIN AUTO_VEHICLE_LEVEL_DATA avld ON idf.POL_TX_ID = avld.POL_TX_ID JOIN AUTO_VEHICLE_PREMIUM_DETAIL avpd ON avld.KEY_POL_RSK_ITM_ID = avpd.KEY_POL_RSK_ITM_ID WHERE avpd.ITM_TERM_PRM_AMT > 10000 AND idf."INSURED STATE" = 'TX' AND TRY_TO_DATE(idf.EFFECTIVEDATE, 'YYYY-MM-DD') >= '2018-01-01' GROUP BY policy_year;""",
        "charts": [
            {
                "title": "Auto policies with premium over 10000 in Texas from year 2018 onwards, grouped by year",
                "type": "bar",
                "chart_code": "plt.bar(data['policy_year'], data['policy_count'])",
                "dataframe": {
                    'policy_year': [2019, 2020, 2021, 2022, 2023],
                    'policy_count': [1500, 1750, 2000, 2250, 2500]
                }
            }
        ]
    }] 
    
    # Current user question - this is a follow-up that modifies the premium threshold
    user_question = "What about policies with premium over 50000 across all United States?"

    print(f"üîç Step 1: Enhanced Intent Classification")
    intent_info = classify_intent_and_context_gpt(user_question, agent_history, gpt_object)

    print(f"\nüèóÔ∏è  Step 2: Building GPT Intelligence-Powered Pipeline State")
    # Initialize the revolutionary pipeline state
    state = {
        "user_question": user_question,
        "table_schema": all_tables,
        "columns_info": all_columns,
        "gpt_object": gpt_object,
        "intent_info": intent_info
    }

    # Add legacy context for backward compatibility
    if intent_info["is_followup"] and intent_info["user_context"]:
        state["user_context"] = intent_info["user_context"]

    print(f"\nüß† Step 3: GPT Natural Language Join Intelligence Schema Selection")
    state = first_tool_call(state)
    print(f"Selected tables: {[t.get('table_name', 'Unknown') for t in state['relevant_tables_joins']]}")

    print(f"\n‚ö° Step 4: SQL Generation with GPT Join Intelligence")
    state = query_gen_node(state)
    
    print(f"\nüìä Step 5: Query Execution")
    state = run_query_and_return_df(state)

    # Display comprehensive results showcasing the GPT intelligence benefits
    print(f"\n" + "="*80)
    print(f"üéØ GPT NATURAL LANGUAGE JOIN INTELLIGENCE RESULTS")
    print(f"="*80)
    
    print(f"\nüß† REASONING (Enhanced with GPT Intelligence):")
    print(state["query_reasoning"])
    
    print(f"\nüíæ GENERATED SQL (Using GPT-Extracted Joins):")
    print(state["query_sql"])
    
    print(f"\nüìà EXECUTION RESULTS:")
    if isinstance(state['snowflake_result'], pd.DataFrame):
        print(f"‚úÖ Query returned {len(state['snowflake_result'])} rows")
        print(state['snowflake_result'].head())
    else:
        print("‚ùå Query execution failed or returned no results")
    
    print(f"\nüéâ GPT-powered natural language join intelligence pipeline completed!")
    print("üîë Revolutionary Enhancement: GPT's natural language understanding extracts joins")
    print("üìä Result: Intelligence-based rather than rule-based database relationship discovery")
    print("üöÄ Impact: Dramatic improvement in accuracy and reliability of schema selection")
```


```python
import json
import time
import pandas as pd
from src.snowflake_connection import create_snowflake_connection

# ------------------ INITIAL SETUP: LOAD DATA --------------------

from src.gpt_class import GptApi

gpt_object = GptApi()

# -------------------------------- ENHANCED INTENT AGENT ------------------

def classify_intent_and_context_gpt(user_question, agent_history, gpt_object):
    """
    Enhanced intent classification that provides structured context for better schema selection.
    This function now extracts specific types of context that help the schema selection 
    agent make better decisions about tables and joins.
    
    Returns:
        {
            "is_followup": bool,
            "user_context": str or None,
            "context_type": str,  # NEW: helps schema selection understand what type of context
            "inherited_elements": dict  # NEW: specific elements to carry forward
        }
    """
    # Build more structured history representation
    chat_history_str = ""
    inherited_elements = {}
    
    if agent_history:
        last = agent_history[-1]
        # Extract structured information from previous interaction
        prev_reasoning = last.get('reasoning', '')
        prev_sql = last.get('sql_query', '')
        prev_story = last.get('story', '')
        
        # Analyze previous SQL to extract reusable elements
        if prev_sql:
            inherited_elements = extract_sql_context_elements(prev_sql)
        
        chat_history_str = f"""
Previous Query Analysis:
- Story: {prev_story}
- Key Reasoning: {prev_reasoning}
- SQL Elements Used: Tables={inherited_elements.get('tables', [])}, 
  Time Filters={inherited_elements.get('time_filters', [])}, 
  Geographic Filters={inherited_elements.get('geo_filters', [])},
  Premium Filters={inherited_elements.get('premium_filters', [])}
        """
    else:
        chat_history_str = "(No previous context)"

    # Enhanced prompt that helps downstream schema selection
    prompt = f"""
You are an expert context analyst for insurance analytics queries. Your analysis will guide 
database schema selection, so provide structured, actionable context information.

PREVIOUS INTERACTION CONTEXT:
{chat_history_str}

CURRENT USER QUESTION:
"{user_question}"

ANALYSIS FRAMEWORK:

STEP 1 - RELATIONSHIP ANALYSIS:
Determine if the current question builds upon, modifies, or references the previous context.
Look for these relationship indicators:
- Comparative terms: "what about", "how about", "similar", "same but"
- Referential terms: "those", "them", "it", "that data"
- Modification terms: "increase to", "change to", "instead of"
- Expansion terms: "also show", "include", "add"

STEP 2 - CONTEXT TYPE CLASSIFICATION:
If this is a followup, classify the type:
- FILTER_MODIFICATION: Changing filters (amount thresholds, date ranges, locations)
- ENTITY_EXPANSION: Same analysis but different entity set (states, policy types)
- METRIC_CHANGE: Same data but different calculations or groupings
- COMPARISON_REQUEST: Comparing current request to previous results
- REFINEMENT: Narrowing down or expanding previous results

STEP 3 - ACTIONABLE CONTEXT EXTRACTION:
Extract specific elements that should inform schema selection:
- Time periods to maintain or modify
- Geographic constraints to keep or change
- Policy types or categories to focus on
- Premium ranges or other numeric filters
- Table relationships that should be preserved

OUTPUT REQUIREMENTS:
{{
    "is_followup": true/false,
    "confidence_level": "high/medium/low",
    "context_type": "FILTER_MODIFICATION|ENTITY_EXPANSION|METRIC_CHANGE|COMPARISON_REQUEST|REFINEMENT|NEW_QUERY",
    "inherited_elements": {{
        "preserve_time_filters": true/false,
        "preserve_geographic_filters": true/false, 
        "preserve_policy_types": true/false,
        "modify_premium_threshold": "specific instruction or null",
        "table_relationship_hint": "suggestion for schema selection"
    }},
    "user_context": "concise instruction for schema selection agent"
}}

EXAMPLES:

Previous: "Auto policies in Texas with premium > 10000"
Current: "What about policies over 50000?"
‚Üí FILTER_MODIFICATION: preserve auto+Texas, modify premium threshold

Previous: "Policies from 2018-2020 grouped by year"  
Current: "Show the same for commercial policies"
‚Üí ENTITY_EXPANSION: preserve time range and grouping, change to commercial focus

Respond with valid JSON only. Focus on actionable context that helps with table/column selection.
"""

    messages = [{"role": "user", "content": prompt}]
    payload = {
        "username": "ENHANCED_INTENT_CLASSIFIER",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.05,  # Very low for consistent classification
        "max_tokens": 1024
    }
    
    try:
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        content = resp.json()['choices'][0]['message']['content']
        
        # Parse JSON safely
        start = content.find("{")
        end = content.rfind("}") + 1
        parsed = json.loads(content[start:end])
        
        is_followup = parsed.get("is_followup", False)
        context_type = parsed.get("context_type", "NEW_QUERY")
        user_context = parsed.get("user_context", None)
        inherited_elements = parsed.get("inherited_elements", {})
        
        print(f"[INTENT] Follow-up: {is_followup}, Type: {context_type}")
        if user_context:
            print(f"[INTENT] Context for schema selection: {user_context}")
        
        # Return enhanced context structure
        return {
            "is_followup": is_followup,
            "user_context": user_context,
            "context_type": context_type,
            "inherited_elements": inherited_elements
        }
        
    except Exception as ex:
        print(f"[WARN] Intent classification error: {ex}")
        return {
            "is_followup": False,
            "user_context": None,
            "context_type": "NEW_QUERY",
            "inherited_elements": {}
        }

def extract_sql_context_elements(sql_query):
    """
    Helper function to extract reusable elements from previous SQL queries.
    This helps the intent classifier provide better context to schema selection.
    """
    elements = {
        "tables": [],
        "time_filters": [],
        "geo_filters": [],
        "premium_filters": [],
        "grouping_patterns": []
    }
    
    if not sql_query:
        return elements
    
    sql_lower = sql_query.lower()
    
    # Extract table patterns (simple pattern matching)
    common_tables = ['inforce_data_final', 'auto_vehicle_premium_detail', 'auto_vehicle_level_data', 
                    'coverage_limit_details', 'cat_policy', 'claims_data']
    for table in common_tables:
        if table in sql_lower:
            elements["tables"].append(table)
    
    # Extract filter patterns
    if 'year(' in sql_lower or 'date' in sql_lower:
        elements["time_filters"].append("date_filtering_used")
    
    if 'state' in sql_lower or 'tx' in sql_lower or 'ca' in sql_lower:
        elements["geo_filters"].append("state_filtering_used")
        
    if 'premium' in sql_lower and ('>' in sql_lower or '<' in sql_lower):
        elements["premium_filters"].append("premium_threshold_used")
        
    if 'group by' in sql_lower:
        elements["grouping_patterns"].append("grouping_applied")
    
    return elements

# ------------------------------- LOAD TABLES SCHEMA (UNCHANGED) -------------------------------------

# Load table descriptions (schema, table, and per-table details)
table_catalog = pd.read_excel(
    "hackathon_final_schema_file_v1.xlsx",
    sheet_name='Table_descriptions'
)[['DATABASE', 'SCHEMA', 'TABLE', 'Brief_Description', 'Detailed_Comments']]

# Load column descriptions (per-table, per-column, and EDA)
column_catalog_df = pd.read_excel(
    "hackathon_final_schema_file_v1.xlsx",
    sheet_name="Table's Column Summaries"
)[['Table Name', 'Feature Name', 'Data Type', 'Description', 'sample_100_distinct']]

SAMPLE_100_VALUE_FIELD = "sample_100_distinct"  

# -- Convert to list-of-dicts for uniform access --
all_tables = [
    {
        "database": row["DATABASE"],
        "schema": row["SCHEMA"],
        "table_name": row["TABLE"],
        "brief_description": row["Brief_Description"],
        "detailed_comments": row["Detailed_Comments"]
    }
    for idx, row in table_catalog.iterrows()
]

all_columns = [
    {
        "table_name": row["Table Name"],
        "column_name": row["Feature Name"],
        "data_type": row["Data Type"],
        "description": row["Description"],
        "sample_100_distinct": row[SAMPLE_100_VALUE_FIELD]
        
    }
    for idx, row in column_catalog_df.iterrows()
]

# ------------------ PROMPT UTILITIES (UNCHANGED) -------------------------

def build_column_context_block(relevant_columns, all_columns, max_sample_vals=15):
    """
    Return a string giving, for all columns-of-interest, precise metadata block for GPT prompt.
    Shows at most `max_sample_vals` sample values for each column.
    """
    blocks = []
    for rel in relevant_columns:
        tname, cname = rel["table_name"], rel["column_name"]
        col_row = next(
            (c for c in all_columns if c["table_name"] == tname.split('.')[-1] and c["column_name"] == cname),
            None
        )
        if not col_row:
            continue  # missing in catalog (shouldn't happen)

        # --- Process sample_100_distinct string to limit values ---
        raw_samples = col_row.get('sample_100_distinct', '')
        # Try to parse it either as comma separated, or from a string list
        if isinstance(raw_samples, str):
            # Remove outer brackets if present (e.g. "[a, b, c]")
            s = raw_samples.strip()
            if s.startswith("[") and s.endswith("]"):
                s = s[1:-1]
            # Now, split
            vals = [v.strip("'\" ") for v in s.split(",") if v.strip().strip("'\"")]
        elif isinstance(raw_samples, list):
            vals = raw_samples
        else:
            vals = []

        # Limit the number of sample values displayed
        limited_vals = vals[:max_sample_vals]
        if len(vals) > max_sample_vals:
            limited_vals.append("...")  # Indicate truncation

        block = (
            f"\n ||| Table_name: {tname} "
            f"|| Feature_name: {col_row['column_name']} "
            f"|| Data_Type: {col_row['data_type']} "
            f"|| Description: {col_row['description']} "
            f"|| Sample Values (separated by ,): {', '.join(limited_vals)} ||| \n"
        )
        blocks.append(block)
    return "".join(blocks)

# ------------------ ENHANCED SCHEMA SELECTION AGENT --------------------------

def extract_unique_joins(columns):
    """
    Given a list of column dicts, extract unique {table_name, join_key} pairs.
    """
    join_pairs = set()
    for col in columns:
        # Safely handle 'JOIN_KEY' possibly missing in fallback scenario
        join_key = col.get('JOIN_KEY', None)
        table_name = col.get('table_name', None)
        if join_key and table_name:
            join_pairs.add( (table_name, join_key) )
    # Convert set of tuples to list of dicts if preferred
    return [ {"table_name": t, "join_key": k} for (t,k) in sorted(join_pairs) ]

def match_query_to_schema(user_question, all_tables, all_columns, gpt_object=None, 
                         user_context=None, intent_info=None):
    """
    Enhanced schema selection that uses intent classification results for better decisions.
    The intent_info parameter now provides structured guidance for schema selection.
    
    Args:
        user_question: The natural language query
        all_tables: Available database tables
        all_columns: Available database columns  
        gpt_object: GPT API object for LLM-based selection
        user_context: Basic context string (legacy parameter)
        intent_info: Enhanced intent information with structured context
    
    Returns:
        Tuple of (matched_tables_joins, matched_columns, matched_joins)
    """
    uq = user_question.lower()
    matched_tables = []
    matched_columns = []
    print(f"[SCHEMA] Processing query: {user_question}")

    # --- Enhanced LLM Option with Intent Integration ---
    if gpt_object is not None:
        # Build context exactly as before
        tables_context = "\n".join(
            [f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}: {tbl['brief_description'] }" for tbl in all_tables])
        columns_context = "\n".join(
            [f"{col['table_name']}.{col['column_name']}: {col['description']}" for col in all_columns])
        
        # Enhanced context building using intent information
        context_text = ""
        domain_hints = ""
        
        if intent_info and intent_info.get("is_followup"):
            context_type = intent_info.get("context_type", "")
            inherited_elements = intent_info.get("inherited_elements", {})
            
            context_text = f"\nPREVIOUS QUERY CONTEXT:\n"
            context_text += f"Context Type: {context_type}\n"
            
            if user_context:
                context_text += f"Context Details: {user_context}\n"
            
            # Provide specific guidance based on context type
            if context_type == "FILTER_MODIFICATION":
                domain_hints = "\n[GUIDANCE] This is a filter modification - preserve table relationships but adjust filter conditions.\n"
            elif context_type == "ENTITY_EXPANSION":
                domain_hints = "\n[GUIDANCE] This expands the entity scope - maintain similar table structure but consider additional entity tables.\n"
            elif context_type == "METRIC_CHANGE":
                domain_hints = "\n[GUIDANCE] This changes metrics/calculations - focus on tables that support the new metric requirements.\n"
            
            # Add specific inherited element guidance
            if inherited_elements.get("preserve_time_filters"):
                domain_hints += "[HINT] Preserve time-based filtering capabilities from previous query.\n"
            if inherited_elements.get("preserve_geographic_filters"):
                domain_hints += "[HINT] Maintain geographic filtering capabilities.\n"
            if inherited_elements.get("table_relationship_hint"):
                domain_hints += f"[HINT] Table relationships: {inherited_elements['table_relationship_hint']}\n"
        
        # ENHANCED PROMPT - This is the key improvement
        gpt_prompt = f"""
You are an expert insurance database architect with deep knowledge of actuarial data modeling and Snowflake SQL optimization.

CRITICAL DOMAIN KNOWLEDGE:
- Schema 'RAW_CI_CAT_ANALYSIS': Contains catastrophic insurance policies, CAT events, CAT-specific coverage limits
- Schema 'RAW_CI_INFORCE': Contains active policies, auto vehicle details, standard premium information
- Schema 'RAW_CI_CLAIMS': Contains claims data, settlements, adjustments
- Key relationship patterns: POL_TX_ID connects policies across schemas, PKEY links policy details, KEY_POL_RSK_ITM_ID connects policy items

{context_text}

{domain_hints}

USER QUESTION: {user_question}

AVAILABLE SCHEMA:
Tables: {tables_context}

Columns: {columns_context}

STEP-BY-STEP REASONING PROCESS:

STEP 1 - DOMAIN CLASSIFICATION:
First, classify the query type:
- AUTO: Auto insurance policies and vehicles (use RAW_CI_INFORCE schema primarily)
- CAT: Catastrophic events and policies (use RAW_CI_CAT_ANALYSIS schema)
- CLAIMS: Claims processing and settlements (use RAW_CI_CLAIMS schema)  
- GENERAL: Cross-domain analysis (may need multiple schemas)

STEP 2 - TABLE IDENTIFICATION:
Based on domain classification and any context guidance, identify the MINIMUM set of tables needed:
- Start with the primary table that contains the main entity (policies, claims, vehicles)
- Add only tables that provide additional required attributes
- Avoid redundant tables that duplicate information
- Consider context preservation requirements if this is a follow-up query

STEP 3 - JOIN PATH VALIDATION:
For each pair of selected tables, identify the exact join relationship:
- Primary join keys: POL_TX_ID (policy transaction ID), PKEY (policy key), KEY_POL_RSK_ITM_ID (policy risk item)
- Verify that join keys exist in BOTH tables before including the relationship
- Prefer direct joins over multi-hop joins when possible
- If tables cannot be joined directly, identify the minimum intermediate tables needed

STEP 4 - COLUMN PRECISION:
Select only columns that directly answer the question:
- For filtering: columns used in WHERE clauses
- For display: columns requested in output  
- For grouping: columns used for aggregation
- For calculation: columns needed for mathematical operations

VALIDATION RULES:
1. Every selected table MUST be connected to at least one other table through a valid join key
2. All join keys MUST exist as actual columns in the respective tables
3. Do not select tables just because they seem related - they must provide specific required data
4. For CAT queries, prioritize RAW_CI_CAT_ANALYSIS tables; for auto queries, prioritize RAW_CI_INFORCE
5. Double-check that column names match exactly (including spaces and special characters)
6. If context guidance suggests preserving certain relationships, prioritize those patterns

COMMON JOIN PATTERNS (use these as templates):
- Policy + Premium: INFORCE_DATA_FINAL.POL_TX_ID = AUTO_VEHICLE_PREMIUM_DETAIL.POL_TX_ID
- Policy + Vehicle: INFORCE_DATA_FINAL.POL_TX_ID = AUTO_VEHICLE_LEVEL_DATA.POL_TX_ID  
- Vehicle + Premium: AUTO_VEHICLE_LEVEL_DATA.KEY_POL_RSK_ITM_ID = AUTO_VEHICLE_PREMIUM_DETAIL.KEY_POL_RSK_ITM_ID
- Policy + Coverage: COVERAGE_LIMIT_DETAILS.POL_TX_ID = INFORCE_DATA_FINAL.POL_TX_ID
- CAT Policy + Events: CAT_POLICY_TABLE.POLICY_ID = CAT_EVENTS_TABLE.POLICY_ID

OUTPUT REQUIREMENTS:
Provide your response as valid JSON with this exact structure:

{{
    "domain_classification": "AUTO|CAT|CLAIMS|GENERAL",
    "context_application": "How you applied the context guidance to your selection",
    "reasoning_steps": {{
        "domain_rationale": "Why you classified the query this way",
        "table_selection_logic": "Why these specific tables are needed",
        "join_path_explanation": "How the tables connect and why these joins work",
        "column_selection_rationale": "Why these specific columns answer the question"
    }},
    "relevant_tables_joins": [
        {{
            "table_name": "SCHEMA.TABLE_NAME",
            "join_keys": ["KEY1", "KEY2"],
            "selection_reason": "Why this table is essential"
        }}
    ],
    "relevant_columns": [
        {{
            "table_name": "TABLE_NAME", 
            "column_name": "COLUMN_NAME",
            "usage_purpose": "filtering|display|grouping|calculation"
        }}
    ]
}}

CRITICAL: Validate that every join_key you specify actually exists in the corresponding table's column list. Double-check column name spelling and spacing.

Respond with ONLY valid JSON. No additional text or explanations outside the JSON structure.
        """
        
        messages = [{"role": "user", "content": gpt_prompt}]
        payload = {
            "username": "ENHANCED_SCHEMA_AGENT",
            "session_id": "1",
            "messages": messages,
            "temperature": 0.001,  # Very low temperature for consistent schema selection
            "max_tokens": 2048
        }
        try:
            resp = gpt_object.get_gpt_response_non_streaming(payload)
            content = resp.json()['choices'][0]['message']['content']
            first = content.find('{')
            last = content.rfind('}') + 1
            parsed = json.loads(content[first:last])
            
            # Extract results with enhanced error checking
            if "relevant_tables_joins" in parsed:
                matched_tables_joins = parsed["relevant_tables_joins"]
                # Validate that all specified join keys exist in the column list
                validated_tables_joins = []
                for table_info in matched_tables_joins:
                    table_name = table_info.get("table_name", "")
                    join_keys = table_info.get("join_keys", [])
                    
                    # Check if join keys actually exist in columns for this table
                    table_columns = [col['column_name'] for col in all_columns 
                                   if col['table_name'] == table_name.split('.')[-1]]
                    
                    valid_keys = [key for key in join_keys if key in table_columns]
                    if valid_keys:  # Only include if at least one valid key exists
                        validated_table = table_info.copy()
                        validated_table["join_keys"] = valid_keys
                        validated_tables_joins.append(validated_table)
                    else:
                        print(f"[WARN] No valid join keys found for table {table_name}")
                
                matched_tables_joins = validated_tables_joins
            else:
                matched_tables_joins = []
                
            matched_columns = parsed.get("relevant_columns", [])
            
            # Print enhanced debugging information
            print(f"[SCHEMA] Domain Classification: {parsed.get('domain_classification', 'Unknown')}")
            if parsed.get('context_application'):
                print(f"[SCHEMA] Context Applied: {parsed['context_application']}")
            
            if "reasoning_steps" in parsed:
                reasoning = parsed["reasoning_steps"]
                print(f"[SCHEMA] Table Selection Logic: {reasoning.get('table_selection_logic', 'Not provided')}")
                print(f"[SCHEMA] Join Path: {reasoning.get('join_path_explanation', 'Not provided')}")
                
        except Exception as ex:
            print(f"[WARN] Enhanced GPT table/column mapping error: {ex}")
            matched_tables_joins = []
            matched_columns = []

    # --- Fallback logic remains exactly the same as original ---
    if not matched_tables_joins and not matched_columns:
        print("[SCHEMA] Falling back to keyword-based matching")
        keywords = set(uq.replace(",", " ").replace("_", " ").split())
        # Find relevant tables
        matched_table_objs = [
            tbl
            for tbl in all_tables
            if any(k in (tbl['table_name'].lower() + " " +
                         (str(tbl['brief_description']) or "")).lower()
                   for k in keywords)
        ]
        matched_tables_joins = [
            {
                "table_name": f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}",
                "join_keys": []  # No way to infer join keys in fallback
            }
            for tbl in matched_table_objs
        ]
        matched_columns = [
            {"table_name": col['table_name'], "column_name": col['column_name']}
            for col in all_columns
            if any(k in (col['column_name'] + " " +
                         (str(col['description']) or "")).lower()
                   for k in keywords)
        ]

    # Generate matched_joins exactly as in original code
    matched_joins = []
    seen_pairs = set()
    for t in matched_tables_joins:
        tname = t.get("table_name") or t  # might just be a string in fallback
        join_keys = t.get("join_keys", [])
        if isinstance(join_keys, list):
            for key in join_keys:
                if (tname, key) not in seen_pairs:
                    matched_joins.append({"table_name": tname, "join_key": key})
                    seen_pairs.add((tname, key))
    if not matched_joins:
        # As a backup, still extract possible join keys from columns if present
        matched_joins = extract_unique_joins(matched_columns)

    print(f"[SCHEMA] Selected {len(matched_tables_joins)} tables, {len(matched_columns)} columns")
    return matched_tables_joins, matched_columns, matched_joins

# ------------------ ENHANCED FIRST TOOL CALL --------------------------

def first_tool_call(state):
    """
    Enhanced Node 1: Given NLQ and catalogs, pick tables and columns using intent context.
    Now integrates intent classification results for better schema selection.
    """
    user_question = state.get("user_question")
    all_tables    = state.get("table_schema")
    all_columns   = state.get("columns_info")
    gpt_object    = state.get("gpt_object", None)

    # Enhanced: get both legacy and new intent context
    user_context = state.get("user_context", None)
    intent_info = state.get("intent_info", None)  # New structured intent information
    
    print(f"[TOOL1] Processing schema selection with enhanced context")
    
    # Call enhanced schema selection with intent information
    relevant_tables_joins, relevant_columns, relevant_joins = match_query_to_schema(
        user_question, all_tables, all_columns, gpt_object, 
        user_context=user_context, intent_info=intent_info)
    
    state["relevant_tables_joins"] = relevant_tables_joins
    state["relevant_columns"] = relevant_columns
    state["relevant_joins"] = relevant_joins
    
    print(f"[TOOL1] Schema selection complete: {len(relevant_tables_joins)} tables identified")
    return state

# -------------------- SQL GENERATION AGENT (SLIGHTLY ENHANCED) ----------------------

def query_gen_node(state):
    """
    Enhanced Node 2: Given selected columns and question, ask GPT to reason and write SQL.
    Now includes better context integration and improved validation.
    """
    user_question = state.get("user_question")
    all_columns = state.get("columns_info")
    relevant_columns = state.get("relevant_columns")
    relevant_joins=state.get("relevant_tables_joins")
    gpt_object = state.get("gpt_object")
    intent_info = state.get("intent_info", {})

    # Build context for the question
    context_block = build_column_context_block(relevant_columns, all_columns, 15)
    print(f"[TOOL2] Generating SQL with {len(relevant_columns)} selected columns")

    # Enhanced context building for SQL generation
    context_guidance = ""
    if intent_info.get("is_followup"):
        context_type = intent_info.get("context_type", "")
        inherited_elements = intent_info.get("inherited_elements", {})
        
        context_guidance = f"\nCONTEXT GUIDANCE FOR SQL GENERATION:\n"
        context_guidance += f"Query Type: {context_type}\n"
        
        if context_type == "FILTER_MODIFICATION":
            context_guidance += "- This modifies filters from a previous query - maintain similar structure but adjust WHERE conditions\n"
        elif context_type == "ENTITY_EXPANSION":
            context_guidance += "- This expands the scope of entities - consider similar grouping and aggregation patterns\n"
        elif context_type == "METRIC_CHANGE":
            context_guidance += "- This changes metrics/calculations - focus on new aggregation requirements\n"
        
        if inherited_elements.get("modify_premium_threshold"):
            context_guidance += f"- Premium threshold guidance: {inherited_elements['modify_premium_threshold']}\n"

    with open('few_shot_examples.txt', 'r', encoding='utf-8') as f:
        few_shot_examples = f.read()

    # Enhanced SQL generation prompt
    gpt_prompt = f"""
You are a strict SQL query generator specialized in insurance analytics on Snowflake.
Use ONLY the supplied metadata, join graph, and date templates. DO NOT invent, guess, or assume any tables, columns, relationships, or data types not explicitly shown.

{context_guidance}

**User Question:**
```{user_question}```

**Relevant Tables and Join_Keys:**
```{relevant_joins}```

**Relevant Columns and Tables with Sample Values:**
```{context_block}```

INPUT STRUCTURE VALIDATION:
1) TABLE_METADATA - Verified from provided schema
2) JOIN_GRAPH - Validated join relationships from schema selection
3) DATE_TEMPLATES - Snowflake-specific date handling patterns
4) USER_REQUEST - Natural language requirement analysis

ENHANCED SQL GENERATION WORKFLOW:

Step 1 - PARSE USER REQUEST WITH CONTEXT
‚Ä¢ Identify all metrics, filters, and aggregation needs from the USER_REQUEST
‚Ä¢ Apply any context guidance for follow-up queries
‚Ä¢ For every filter or metric: Map explicitly to a (TABLE.COLUMN)
‚Ä¢ If not available, mark as "N/A: Not found", and DO NOT use or guess in SQL

Step 2 - SELECT TABLES & OPTIMIZE JOIN PLAN  
‚Ä¢ Determine the minimal set of tables needed to supply all required columns
‚Ä¢ Using provided JOIN_GRAPH, find the shortest valid join chain connecting all required tables
‚Ä¢ Record each join as: <TABLE1> JOIN <TABLE2> ON <TABLE1>.<KEY> = <TABLE2>.<KEY>
‚Ä¢ If a key/join path cannot be confirmed, state "Join not possible" for that path

Step 3 - PREPARE DATE CONVERSIONS (SNOWFLAKE-SPECIFIC)
‚Ä¢ For VARCHAR date columns: Use TRY_TO_DATE(<date_field>, 'YYYY-MM-DD')
‚Ä¢ For DATE/TIMESTAMP columns: Use as-is or CAST(<date_field> AS DATE) if needed
‚Ä¢ Never apply TRY_TO_DATE to columns already in DATE/TIMESTAMP format
‚Ä¢ Apply filters using appropriate format for the column type

Step 4 - BUILD OPTIMIZED SQL QUERY  
‚Ä¢ SELECT: Choose mapped fields and aggregations
‚Ä¢ FROM: Use the validated reference tables   
‚Ä¢ JOIN: Add all join steps confirmed above using ONLY listed join keys
‚Ä¢ WHERE: Add mapped, available filters with proper data type handling
‚Ä¢ GROUP BY: Add if user aggregation/grouping is requested and columns are mapped
‚Ä¢ ORDER BY: If required in user request
‚Ä¢ STRICT: DO NOT use subqueries or invented SQL logic

Step 5 - ENHANCED VALIDATION CHECKLIST  
‚ñ° Only listed tables/columns used  
‚ñ° All join steps follow provided JOIN_GRAPH‚Äîno invented joins  
‚ñ° All date logic follows Snowflake conventions  
‚ñ° No columns/filters/joins/objects invented or guessed  
‚ñ° All SQL clauses pass previous mapping and join plan checks
‚ñ° Context guidance properly applied if this is a follow-up query

**Enhanced Output Structure:**
**Mapping:**  
(User requirement ‚Üí Actual DB column mapping. If not found, do not use as filter.)

**Reasoning:**  
(Explain: 1. how context guidance was applied, 2. table join strategy, 3. column/filter selection logic, 4. optimization decisions. Max 150 words.)

**SQL Query:**  
(Production-ready SQL using only validated mappings and joins.)

****FEW-SHOT EXAMPLES:****
{few_shot_examples}
****END EXAMPLES****

From the above, provide valid JSON:
{{
    "Reasoning": "detailed explanation including context application",
    "SQL Query": "complete optimized SQL query"
}}

Strictly output JSON only. Validate that all columns in the SQL exist in relevant_columns.
"""
    
    messages = [{"role": "user", "content": gpt_prompt}]
    payload = {
        "username": "ENHANCED_SQL_AGENT",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.001,
        "max_tokens": 2048
    }
    matched_sql_query = []
    matched_reasoning = []

    try:
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        gpt_output = resp.json()['choices'][0]['message']['content']

        first = gpt_output.find('{')
        last = gpt_output.rfind('}')+1
        parsed = json.loads(gpt_output[first:last])
        matched_reasoning = parsed.get("Reasoning", [])
        matched_sql_query = parsed.get("SQL Query", [])
        
        print(f"[TOOL2] SQL generation complete with reasoning")
        
    except Exception as ex:
        print(f"[WARN] GPT reasoning/sqlquery mapping error: {ex}")
        
    state["query_llm_prompt"] = gpt_prompt
    state["query_llm_result"] = gpt_output
    state["query_reasoning"] = matched_reasoning
    state["query_sql"] = matched_sql_query
    return state

# ------------------ SNOWFLAKE SQL EXECUTION (UNCHANGED) --------------------

def run_query_and_return_df(state):
    """
    Executes the SQL query found in state['query_sql'] on Snowflake and returns DataFrame result.
    Updates state['snowflake_result'] with a pandas DataFrame (or None on error).
    """
    query_sql = state.get("query_sql", None)
    if not query_sql:
        print("[ERROR] No SQL found in state['query_sql']")
        state['snowflake_result'] = None
        return state
    
    # Connect to Snowflake
    conn = None
    try:
        print(f"[EXEC] Executing SQL on Snowflake...")
        conn = create_snowflake_connection()
        with conn.cursor() as cursor:
            cursor.execute(query_sql)
            # Fetch all rows and columns
            rows = cursor.fetchall()
            colnames = [d[0] for d in cursor.description]
            df = pd.DataFrame(rows, columns=colnames)
        state['snowflake_result'] = df
        print(f"[EXEC] Query executed successfully, returned {len(df)} rows")
    except Exception as ex:
        print(f"[ERROR] Snowflake query execution failed: {ex}")
        state['snowflake_result'] = None
    finally:
        if conn is not None:
            conn.close()
    return state
    
# ----------- ENHANCED PIPELINE ORCHESTRATION -----------

if __name__ == "__main__":
    print("=== Enhanced NLP-to-SQL Agent Pipeline ===")
    
    # Example agent history - this simulates previous interactions
    agent_history = [{
        "story": """This chart visualizes auto policies with premium over 10000 in Texas from year 2018 onwards, grouped by year""",
        "reasoning": """The tables INFORCE_DATA_FINAL, AUTO_VEHICLE_PREMIUM_DETAIL, and AUTO_VEHICLE_LEVEL_DATA are joined using POL_TX_ID and KEY_POL_RSK_ITM_ID. The query filters policies with premiums over 10000 in Texas from 2018 onwards. The EFFECTIVEDATE is converted to a date format for filtering. The results are grouped by year.""",
        "sql_query": """SELECT YEAR(TRY_TO_DATE(idf.EFFECTIVEDATE, 'YYYY-MM-DD')) AS policy_year, COUNT(*) AS policy_count FROM INFORCE_DATA_FINAL idf JOIN AUTO_VEHICLE_LEVEL_DATA avld ON idf.POL_TX_ID = avld.POL_TX_ID JOIN AUTO_VEHICLE_PREMIUM_DETAIL avpd ON avld.KEY_POL_RSK_ITM_ID = avpd.KEY_POL_RSK_ITM_ID WHERE avpd.ITM_TERM_PRM_AMT > 10000 AND idf."INSURED STATE" = 'TX' AND TRY_TO_DATE(idf.EFFECTIVEDATE, 'YYYY-MM-DD') >= '2018-01-01' GROUP BY policy_year;""",
        "charts": [
            {
                "title": "Auto policies with premium over 10000 in Texas from year 2018 onwards, grouped by year",
                "type": "bar",
                "chart_code": "plt.bar(data['policy_year'], data['policy_count'])",
                "dataframe": {
                    'policy_year': [2019, 2020, 2021, 2022, 2023],
                    'policy_count': [1500, 1750, 2000, 2250, 2500]
                }
            }
        ]
    }] 
    
    # Current user question - this is a follow-up that modifies the premium threshold
    user_question = "What about policies with premium over 50000 across all United States?"

    print(f"\nüîç Step 1: Enhanced Intent Classification")
    intent_info = classify_intent_and_context_gpt(user_question, agent_history, gpt_object)

    print(f"\nüèóÔ∏è  Step 2: Building Enhanced Pipeline State")
    # Initialize enhanced pipeline state
    state = {
        "user_question": user_question,
        "table_schema": all_tables,
        "columns_info": all_columns,
        "gpt_object": gpt_object,
        "intent_info": intent_info  # Add enhanced intent information to state
    }

    # Add legacy context for backward compatibility
    if intent_info["is_followup"] and intent_info["user_context"]:
        state["user_context"] = intent_info["user_context"]

    print(f"\nüóÇÔ∏è  Step 3: Enhanced Schema Selection")
    state = first_tool_call(state)
    print(f"Selected tables: {[t.get('table_name', 'Unknown') for t in state['relevant_tables_joins']]}")

    print(f"\n‚ö° Step 4: Enhanced SQL Generation")
    state = query_gen_node(state)
    
    print(f"\nüìä Step 5: Query Execution")
    state = run_query_and_return_df(state)

    # Display comprehensive results
    print(f"\n" + "="*60)
    print(f"ENHANCED PIPELINE RESULTS")
    print(f"="*60)
    
    print(f"\nüß† REASONING:")
    print(state["query_reasoning"])
    
    print(f"\nüíæ GENERATED SQL:")
    print(state["query_sql"])
    
    print(f"\nüìà EXECUTION RESULTS:")
    if isinstance(state['snowflake_result'], pd.DataFrame):
        print(f"Query returned {len(state['snowflake_result'])} rows")
        print(state['snowflake_result'].head())
    else:
        print("Query execution failed or returned no results")
    
    print(f"\n‚úÖ Pipeline completed successfully!")
```

```python
import json
import time
import pandas as pd
from src.snowflake_connection import create_snowflake_connection

# ------------------ INITIAL SETUP: LOAD DATA --------------------

from src.gpt_class import GptApi

gpt_object = GptApi()

# -------------------------------- ENHANCED INTENT AGENT ------------------

def classify_intent_and_context_gpt(user_question, agent_history, gpt_object):
    """
    Enhanced intent classification that provides structured context for better schema selection.
    This function now extracts specific types of context that help the schema selection 
    agent make better decisions about tables and joins.
    
    Returns:
        {
            "is_followup": bool,
            "user_context": str or None,
            "context_type": str,  # NEW: helps schema selection understand what type of context
            "inherited_elements": dict  # NEW: specific elements to carry forward
        }
    """
    # Build more structured history representation
    chat_history_str = ""
    inherited_elements = {}
    
    if agent_history:
        last = agent_history[-1]
        # Extract structured information from previous interaction
        prev_reasoning = last.get('reasoning', '')
        prev_sql = last.get('sql_query', '')
        prev_story = last.get('story', '')
        
        # Analyze previous SQL to extract reusable elements
        if prev_sql:
            inherited_elements = extract_sql_context_elements(prev_sql)
        
        chat_history_str = f"""
Previous Query Analysis:
- Story: {prev_story}
- Key Reasoning: {prev_reasoning}
- SQL Elements Used: Tables={inherited_elements.get('tables', [])}, 
  Time Filters={inherited_elements.get('time_filters', [])}, 
  Geographic Filters={inherited_elements.get('geo_filters', [])},
  Premium Filters={inherited_elements.get('premium_filters', [])}
        """
    else:
        chat_history_str = "(No previous context)"

    # Enhanced prompt that helps downstream schema selection
    prompt = f"""
You are an expert context analyst for insurance analytics queries. Your analysis will guide 
database schema selection, so provide structured, actionable context information.

PREVIOUS INTERACTION CONTEXT:
{chat_history_str}

CURRENT USER QUESTION:
"{user_question}"

ANALYSIS FRAMEWORK:

STEP 1 - RELATIONSHIP ANALYSIS:
Determine if the current question builds upon, modifies, or references the previous context.
Look for these relationship indicators:
- Comparative terms: "what about", "how about", "similar", "same but"
- Referential terms: "those", "them", "it", "that data"
- Modification terms: "increase to", "change to", "instead of"
- Expansion terms: "also show", "include", "add"

STEP 2 - CONTEXT TYPE CLASSIFICATION:
If this is a followup, classify the type:
- FILTER_MODIFICATION: Changing filters (amount thresholds, date ranges, locations)
- ENTITY_EXPANSION: Same analysis but different entity set (states, policy types)
- METRIC_CHANGE: Same data but different calculations or groupings
- COMPARISON_REQUEST: Comparing current request to previous results
- REFINEMENT: Narrowing down or expanding previous results

STEP 3 - ACTIONABLE CONTEXT EXTRACTION:
Extract specific elements that should inform schema selection:
- Time periods to maintain or modify
- Geographic constraints to keep or change
- Policy types or categories to focus on
- Premium ranges or other numeric filters
- Table relationships that should be preserved

OUTPUT REQUIREMENTS:
{{
    "is_followup": true/false,
    "confidence_level": "high/medium/low",
    "context_type": "FILTER_MODIFICATION|ENTITY_EXPANSION|METRIC_CHANGE|COMPARISON_REQUEST|REFINEMENT|NEW_QUERY",
    "inherited_elements": {{
        "preserve_time_filters": true/false,
        "preserve_geographic_filters": true/false, 
        "preserve_policy_types": true/false,
        "modify_premium_threshold": "specific instruction or null",
        "table_relationship_hint": "suggestion for schema selection"
    }},
    "user_context": "concise instruction for schema selection agent"
}}

EXAMPLES:

Previous: "Auto policies in Texas with premium > 10000"
Current: "What about policies over 50000?"
‚Üí FILTER_MODIFICATION: preserve auto+Texas, modify premium threshold

Previous: "Policies from 2018-2020 grouped by year"  
Current: "Show the same for commercial policies"
‚Üí ENTITY_EXPANSION: preserve time range and grouping, change to commercial focus

Respond with valid JSON only. Focus on actionable context that helps with table/column selection.
"""

    messages = [{"role": "user", "content": prompt}]
    payload = {
        "username": "ENHANCED_INTENT_CLASSIFIER",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.05,  # Very low for consistent classification
        "max_tokens": 1024
    }
    
    try:
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        content = resp.json()['choices'][0]['message']['content']
        
        # Parse JSON safely
        start = content.find("{")
        end = content.rfind("}") + 1
        parsed = json.loads(content[start:end])
        
        is_followup = parsed.get("is_followup", False)
        context_type = parsed.get("context_type", "NEW_QUERY")
        user_context = parsed.get("user_context", None)
        inherited_elements = parsed.get("inherited_elements", {})
        
        print(f"[INTENT] Follow-up: {is_followup}, Type: {context_type}")
        if user_context:
            print(f"[INTENT] Context for schema selection: {user_context}")
        
        # Return enhanced context structure
        return {
            "is_followup": is_followup,
            "user_context": user_context,
            "context_type": context_type,
            "inherited_elements": inherited_elements
        }
        
    except Exception as ex:
        print(f"[WARN] Intent classification error: {ex}")
        return {
            "is_followup": False,
            "user_context": None,
            "context_type": "NEW_QUERY",
            "inherited_elements": {}
        }

def extract_sql_context_elements(sql_query):
    """
    Helper function to extract reusable elements from previous SQL queries.
    This helps the intent classifier provide better context to schema selection.
    """
    elements = {
        "tables": [],
        "time_filters": [],
        "geo_filters": [],
        "premium_filters": [],
        "grouping_patterns": []
    }
    
    if not sql_query:
        return elements
    
    sql_lower = sql_query.lower()
    
    # Extract table patterns (simple pattern matching)
    common_tables = ['inforce_data_final', 'auto_vehicle_premium_detail', 'auto_vehicle_level_data', 
                    'coverage_limit_details', 'cat_policy', 'claims_data']
    for table in common_tables:
        if table in sql_lower:
            elements["tables"].append(table)
    
    # Extract filter patterns
    if 'year(' in sql_lower or 'date' in sql_lower:
        elements["time_filters"].append("date_filtering_used")
    
    if 'state' in sql_lower or 'tx' in sql_lower or 'ca' in sql_lower:
        elements["geo_filters"].append("state_filtering_used")
        
    if 'premium' in sql_lower and ('>' in sql_lower or '<' in sql_lower):
        elements["premium_filters"].append("premium_threshold_used")
        
    if 'group by' in sql_lower:
        elements["grouping_patterns"].append("grouping_applied")
    
    return elements

# ------------------------------- LOAD TABLES SCHEMA (UNCHANGED) -------------------------------------

# Load table descriptions (schema, table, and per-table details)
table_catalog = pd.read_excel(
    "hackathon_final_schema_file_v1.xlsx",
    sheet_name='Table_descriptions'
)[['DATABASE', 'SCHEMA', 'TABLE', 'Brief_Description', 'Detailed_Comments']]

# Load column descriptions (per-table, per-column, and EDA)
column_catalog_df = pd.read_excel(
    "hackathon_final_schema_file_v1.xlsx",
    sheet_name="Table's Column Summaries"
)[['Table Name', 'Feature Name', 'Data Type', 'Description', 'sample_100_distinct']]

SAMPLE_100_VALUE_FIELD = "sample_100_distinct"  

# -- Convert to list-of-dicts for uniform access --
all_tables = [
    {
        "database": row["DATABASE"],
        "schema": row["SCHEMA"],
        "table_name": row["TABLE"],
        "brief_description": row["Brief_Description"],
        "detailed_comments": row["Detailed_Comments"]
    }
    for idx, row in table_catalog.iterrows()
]

all_columns = [
    {
        "table_name": row["Table Name"],
        "column_name": row["Feature Name"],
        "data_type": row["Data Type"],
        "description": row["Description"],
        "sample_100_distinct": row[SAMPLE_100_VALUE_FIELD]
        
    }
    for idx, row in column_catalog_df.iterrows()
]

# ------------------ PROMPT UTILITIES (UNCHANGED) -------------------------

def build_column_context_block(relevant_columns, all_columns, max_sample_vals=15):
    """
    Return a string giving, for all columns-of-interest, precise metadata block for GPT prompt.
    Shows at most `max_sample_vals` sample values for each column.
    """
    blocks = []
    for rel in relevant_columns:
        tname, cname = rel["table_name"], rel["column_name"]
        col_row = next(
            (c for c in all_columns if c["table_name"] == tname.split('.')[-1] and c["column_name"] == cname),
            None
        )
        if not col_row:
            continue  # missing in catalog (shouldn't happen)

        # --- Process sample_100_distinct string to limit values ---
        raw_samples = col_row.get('sample_100_distinct', '')
        # Try to parse it either as comma separated, or from a string list
        if isinstance(raw_samples, str):
            # Remove outer brackets if present (e.g. "[a, b, c]")
            s = raw_samples.strip()
            if s.startswith("[") and s.endswith("]"):
                s = s[1:-1]
            # Now, split
            vals = [v.strip("'\" ") for v in s.split(",") if v.strip().strip("'\"")]
        elif isinstance(raw_samples, list):
            vals = raw_samples
        else:
            vals = []

        # Limit the number of sample values displayed
        limited_vals = vals[:max_sample_vals]
        if len(vals) > max_sample_vals:
            limited_vals.append("...")  # Indicate truncation

        block = (
            f"\n ||| Table_name: {tname} "
            f"|| Feature_name: {col_row['column_name']} "
            f"|| Data_Type: {col_row['data_type']} "
            f"|| Description: {col_row['description']} "
            f"|| Sample Values (separated by ,): {', '.join(limited_vals)} ||| \n"
        )
        blocks.append(block)
    return "".join(blocks)

# ------------------ ENHANCED SCHEMA SELECTION AGENT --------------------------

def extract_unique_joins(columns):
    """
    Given a list of column dicts, extract unique {table_name, join_key} pairs.
    """
    join_pairs = set()
    for col in columns:
        # Safely handle 'JOIN_KEY' possibly missing in fallback scenario
        join_key = col.get('JOIN_KEY', None)
        table_name = col.get('table_name', None)
        if join_key and table_name:
            join_pairs.add( (table_name, join_key) )
    # Convert set of tuples to list of dicts if preferred
    return [ {"table_name": t, "join_key": k} for (t,k) in sorted(join_pairs) ]

def match_query_to_schema(user_question, all_tables, all_columns, gpt_object=None, 
                         user_context=None, intent_info=None):
    """
    Enhanced schema selection that uses intent classification results for better decisions.
    The intent_info parameter now provides structured guidance for schema selection.
    
    Args:
        user_question: The natural language query
        all_tables: Available database tables
        all_columns: Available database columns  
        gpt_object: GPT API object for LLM-based selection
        user_context: Basic context string (legacy parameter)
        intent_info: Enhanced intent information with structured context
    
    Returns:
        Tuple of (matched_tables_joins, matched_columns, matched_joins)
    """
    uq = user_question.lower()
    matched_tables = []
    matched_columns = []
    print(f"[SCHEMA] Processing query: {user_question}")

    # --- Enhanced LLM Option with Intent Integration ---
    if gpt_object is not None:
        # Build context exactly as before
        tables_context = "\n".join(
            [f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}: {tbl['brief_description'] }" for tbl in all_tables])
        columns_context = "\n".join(
            [f"{col['table_name']}.{col['column_name']}: {col['description']}" for col in all_columns])
        
        # Enhanced context building using intent information
        context_text = ""
        domain_hints = ""
        
        if intent_info and intent_info.get("is_followup"):
            context_type = intent_info.get("context_type", "")
            inherited_elements = intent_info.get("inherited_elements", {})
            
            context_text = f"\nPREVIOUS QUERY CONTEXT:\n"
            context_text += f"Context Type: {context_type}\n"
            
            if user_context:
                context_text += f"Context Details: {user_context}\n"
            
            # Provide specific guidance based on context type
            if context_type == "FILTER_MODIFICATION":
                domain_hints = "\n[GUIDANCE] This is a filter modification - preserve table relationships but adjust filter conditions.\n"
            elif context_type == "ENTITY_EXPANSION":
                domain_hints = "\n[GUIDANCE] This expands the entity scope - maintain similar table structure but consider additional entity tables.\n"
            elif context_type == "METRIC_CHANGE":
                domain_hints = "\n[GUIDANCE] This changes metrics/calculations - focus on tables that support the new metric requirements.\n"
            
            # Add specific inherited element guidance
            if inherited_elements.get("preserve_time_filters"):
                domain_hints += "[HINT] Preserve time-based filtering capabilities from previous query.\n"
            if inherited_elements.get("preserve_geographic_filters"):
                domain_hints += "[HINT] Maintain geographic filtering capabilities.\n"
            if inherited_elements.get("table_relationship_hint"):
                domain_hints += f"[HINT] Table relationships: {inherited_elements['table_relationship_hint']}\n"
        
        # ENHANCED PROMPT - This is the key improvement
        gpt_prompt = f"""
You are an expert insurance database architect with deep knowledge of actuarial data modeling and Snowflake SQL optimization.

CRITICAL DOMAIN KNOWLEDGE:
- Schema 'RAW_CI_CAT_ANALYSIS': Contains catastrophic insurance policies, CAT events, CAT-specific coverage limits
- Schema 'RAW_CI_INFORCE': Contains active policies, auto vehicle details, standard premium information
- Schema 'RAW_CI_CLAIMS': Contains claims data, settlements, adjustments
- Key relationship patterns: POL_TX_ID connects policies across schemas, PKEY links policy details, KEY_POL_RSK_ITM_ID connects policy items

{context_text}

{domain_hints}

USER QUESTION: {user_question}

AVAILABLE SCHEMA:
Tables: {tables_context}

Columns: {columns_context}

STEP-BY-STEP REASONING PROCESS:

STEP 1 - DOMAIN CLASSIFICATION:
First, classify the query type:
- AUTO: Auto insurance policies and vehicles (use RAW_CI_INFORCE schema primarily)
- CAT: Catastrophic events and policies (use RAW_CI_CAT_ANALYSIS schema)
- CLAIMS: Claims processing and settlements (use RAW_CI_CLAIMS schema)  
- GENERAL: Cross-domain analysis (may need multiple schemas)

STEP 2 - TABLE IDENTIFICATION:
Based on domain classification and any context guidance, identify the MINIMUM set of tables needed:
- Start with the primary table that contains the main entity (policies, claims, vehicles)
- Add only tables that provide additional required attributes
- Avoid redundant tables that duplicate information
- Consider context preservation requirements if this is a follow-up query

STEP 3 - JOIN PATH VALIDATION:
For each pair of selected tables, identify the exact join relationship:
- Primary join keys: POL_TX_ID (policy transaction ID), PKEY (policy key), KEY_POL_RSK_ITM_ID (policy risk item)
- Verify that join keys exist in BOTH tables before including the relationship
- Prefer direct joins over multi-hop joins when possible
- If tables cannot be joined directly, identify the minimum intermediate tables needed

STEP 4 - COLUMN PRECISION:
Select only columns that directly answer the question:
- For filtering: columns used in WHERE clauses
- For display: columns requested in output  
- For grouping: columns used for aggregation
- For calculation: columns needed for mathematical operations

VALIDATION RULES:
1. Every selected table MUST be connected to at least one other table through a valid join key
2. All join keys MUST exist as actual columns in the respective tables
3. Do not select tables just because they seem related - they must provide specific required data
4. For CAT queries, prioritize RAW_CI_CAT_ANALYSIS tables; for auto queries, prioritize RAW_CI_INFORCE
5. Double-check that column names match exactly (including spaces and special characters)
6. If context guidance suggests preserving certain relationships, prioritize those patterns

COMMON JOIN PATTERNS (use these as templates):
- Policy + Premium: INFORCE_DATA_FINAL.POL_TX_ID = AUTO_VEHICLE_PREMIUM_DETAIL.POL_TX_ID
- Policy + Vehicle: INFORCE_DATA_FINAL.POL_TX_ID = AUTO_VEHICLE_LEVEL_DATA.POL_TX_ID  
- Vehicle + Premium: AUTO_VEHICLE_LEVEL_DATA.KEY_POL_RSK_ITM_ID = AUTO_VEHICLE_PREMIUM_DETAIL.KEY_POL_RSK_ITM_ID
- Policy + Coverage: COVERAGE_LIMIT_DETAILS.POL_TX_ID = INFORCE_DATA_FINAL.POL_TX_ID
- CAT Policy + Events: CAT_POLICY_TABLE.POLICY_ID = CAT_EVENTS_TABLE.POLICY_ID

OUTPUT REQUIREMENTS:
Provide your response as valid JSON with this exact structure:

{{
    "domain_classification": "AUTO|CAT|CLAIMS|GENERAL",
    "context_application": "How you applied the context guidance to your selection",
    "reasoning_steps": {{
        "domain_rationale": "Why you classified the query this way",
        "table_selection_logic": "Why these specific tables are needed",
        "join_path_explanation": "How the tables connect and why these joins work",
        "column_selection_rationale": "Why these specific columns answer the question"
    }},
    "relevant_tables_joins": [
        {{
            "table_name": "SCHEMA.TABLE_NAME",
            "join_keys": ["KEY1", "KEY2"],
            "selection_reason": "Why this table is essential"
        }}
    ],
    "relevant_columns": [
        {{
            "table_name": "TABLE_NAME", 
            "column_name": "COLUMN_NAME",
            "usage_purpose": "filtering|display|grouping|calculation"
        }}
    ]
}}

CRITICAL: Validate that every join_key you specify actually exists in the corresponding table's column list. Double-check column name spelling and spacing.

Respond with ONLY valid JSON. No additional text or explanations outside the JSON structure.
        """
        
        messages = [{"role": "user", "content": gpt_prompt}]
        payload = {
            "username": "ENHANCED_SCHEMA_AGENT",
            "session_id": "1",
            "messages": messages,
            "temperature": 0.001,  # Very low temperature for consistent schema selection
            "max_tokens": 2048
        }
        try:
            resp = gpt_object.get_gpt_response_non_streaming(payload)
            content = resp.json()['choices'][0]['message']['content']
            first = content.find('{')
            last = content.rfind('}') + 1
            parsed = json.loads(content[first:last])
            
            # Extract results with enhanced error checking
            if "relevant_tables_joins" in parsed:
                matched_tables_joins = parsed["relevant_tables_joins"]
                # Validate that all specified join keys exist in the column list
                validated_tables_joins = []
                for table_info in matched_tables_joins:
                    table_name = table_info.get("table_name", "")
                    join_keys = table_info.get("join_keys", [])
                    
                    # Check if join keys actually exist in columns for this table
                    table_columns = [col['column_name'] for col in all_columns 
                                   if col['table_name'] == table_name.split('.')[-1]]
                    
                    valid_keys = [key for key in join_keys if key in table_columns]
                    if valid_keys:  # Only include if at least one valid key exists
                        validated_table = table_info.copy()
                        validated_table["join_keys"] = valid_keys
                        validated_tables_joins.append(validated_table)
                    else:
                        print(f"[WARN] No valid join keys found for table {table_name}")
                
                matched_tables_joins = validated_tables_joins
            else:
                matched_tables_joins = []
                
            matched_columns = parsed.get("relevant_columns", [])
            
            # Print enhanced debugging information
            print(f"[SCHEMA] Domain Classification: {parsed.get('domain_classification', 'Unknown')}")
            if parsed.get('context_application'):
                print(f"[SCHEMA] Context Applied: {parsed['context_application']}")
            
            if "reasoning_steps" in parsed:
                reasoning = parsed["reasoning_steps"]
                print(f"[SCHEMA] Table Selection Logic: {reasoning.get('table_selection_logic', 'Not provided')}")
                print(f"[SCHEMA] Join Path: {reasoning.get('join_path_explanation', 'Not provided')}")
                
        except Exception as ex:
            print(f"[WARN] Enhanced GPT table/column mapping error: {ex}")
            matched_tables_joins = []
            matched_columns = []

    # --- Fallback logic remains exactly the same as original ---
    if not matched_tables_joins and not matched_columns:
        print("[SCHEMA] Falling back to keyword-based matching")
        keywords = set(uq.replace(",", " ").replace("_", " ").split())
        # Find relevant tables
        matched_table_objs = [
            tbl
            for tbl in all_tables
            if any(k in (tbl['table_name'].lower() + " " +
                         (str(tbl['brief_description']) or "")).lower()
                   for k in keywords)
        ]
        matched_tables_joins = [
            {
                "table_name": f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}",
                "join_keys": []  # No way to infer join keys in fallback
            }
            for tbl in matched_table_objs
        ]
        matched_columns = [
            {"table_name": col['table_name'], "column_name": col['column_name']}
            for col in all_columns
            if any(k in (col['column_name'] + " " +
                         (str(col['description']) or "")).lower()
                   for k in keywords)
        ]

    # Generate matched_joins exactly as in original code
    matched_joins = []
    seen_pairs = set()
    for t in matched_tables_joins:
        tname = t.get("table_name") or t  # might just be a string in fallback
        join_keys = t.get("join_keys", [])
        if isinstance(join_keys, list):
            for key in join_keys:
                if (tname, key) not in seen_pairs:
                    matched_joins.append({"table_name": tname, "join_key": key})
                    seen_pairs.add((tname, key))
    if not matched_joins:
        # As a backup, still extract possible join keys from columns if present
        matched_joins = extract_unique_joins(matched_columns)

    print(f"[SCHEMA] Selected {len(matched_tables_joins)} tables, {len(matched_columns)} columns")
    return matched_tables_joins, matched_columns, matched_joins

# ------------------ ENHANCED FIRST TOOL CALL --------------------------

def first_tool_call(state):
    """
    Enhanced Node 1: Given NLQ and catalogs, pick tables and columns using intent context.
    Now integrates intent classification results for better schema selection.
    """
    user_question = state.get("user_question")
    all_tables    = state.get("table_schema")
    all_columns   = state.get("columns_info")
    gpt_object    = state.get("gpt_object", None)

    # Enhanced: get both legacy and new intent context
    user_context = state.get("user_context", None)
    intent_info = state.get("intent_info", None)  # New structured intent information
    
    print(f"[TOOL1] Processing schema selection with enhanced context")
    
    # Call enhanced schema selection with intent information
    relevant_tables_joins, relevant_columns, relevant_joins = match_query_to_schema(
        user_question, all_tables, all_columns, gpt_object, 
        user_context=user_context, intent_info=intent_info)
    
    state["relevant_tables_joins"] = relevant_tables_joins
    state["relevant_columns"] = relevant_columns
    state["relevant_joins"] = relevant_joins
    
    print(f"[TOOL1] Schema selection complete: {len(relevant_tables_joins)} tables identified")
    return state

# -------------------- SQL GENERATION AGENT (SLIGHTLY ENHANCED) ----------------------

def query_gen_node(state):
    """
    Enhanced Node 2: Given selected columns and question, ask GPT to reason and write SQL.
    Now includes better context integration and improved validation.
    """
    user_question = state.get("user_question")
    all_columns = state.get("columns_info")
    relevant_columns = state.get("relevant_columns")
    relevant_joins=state.get("relevant_tables_joins")
    gpt_object = state.get("gpt_object")
    intent_info = state.get("intent_info", {})

    # Build context for the question
    context_block = build_column_context_block(relevant_columns, all_columns, 15)
    print(f"[TOOL2] Generating SQL with {len(relevant_columns)} selected columns")

    # Enhanced context building for SQL generation
    context_guidance = ""
    if intent_info.get("is_followup"):
        context_type = intent_info.get("context_type", "")
        inherited_elements = intent_info.get("inherited_elements", {})
        
        context_guidance = f"\nCONTEXT GUIDANCE FOR SQL GENERATION:\n"
        context_guidance += f"Query Type: {context_type}\n"
        
        if context_type == "FILTER_MODIFICATION":
            context_guidance += "- This modifies filters from a previous query - maintain similar structure but adjust WHERE conditions\n"
        elif context_type == "ENTITY_EXPANSION":
            context_guidance += "- This expands the scope of entities - consider similar grouping and aggregation patterns\n"
        elif context_type == "METRIC_CHANGE":
            context_guidance += "- This changes metrics/calculations - focus on new aggregation requirements\n"
        
        if inherited_elements.get("modify_premium_threshold"):
            context_guidance += f"- Premium threshold guidance: {inherited_elements['modify_premium_threshold']}\n"

    with open('few_shot_examples.txt', 'r', encoding='utf-8') as f:
        few_shot_examples = f.read()

    # Enhanced SQL generation prompt
    gpt_prompt = f"""
You are a strict SQL query generator specialized in insurance analytics on Snowflake.
Use ONLY the supplied metadata, join graph, and date templates. DO NOT invent, guess, or assume any tables, columns, relationships, or data types not explicitly shown.

{context_guidance}

**User Question:**
```{user_question}```

**Relevant Tables and Join_Keys:**
```{relevant_joins}```

**Relevant Columns and Tables with Sample Values:**
```{context_block}```

INPUT STRUCTURE VALIDATION:
1) TABLE_METADATA - Verified from provided schema
2) JOIN_GRAPH - Validated join relationships from schema selection
3) DATE_TEMPLATES - Snowflake-specific date handling patterns
4) USER_REQUEST - Natural language requirement analysis

ENHANCED SQL GENERATION WORKFLOW:

Step 1 - PARSE USER REQUEST WITH CONTEXT
‚Ä¢ Identify all metrics, filters, and aggregation needs from the USER_REQUEST
‚Ä¢ Apply any context guidance for follow-up queries
‚Ä¢ For every filter or metric: Map explicitly to a (TABLE.COLUMN)
‚Ä¢ If not available, mark as "N/A: Not found", and DO NOT use or guess in SQL

Step 2 - SELECT TABLES & OPTIMIZE JOIN PLAN  
‚Ä¢ Determine the minimal set of tables needed to supply all required columns
‚Ä¢ Using provided JOIN_GRAPH, find the shortest valid join chain connecting all required tables
‚Ä¢ Record each join as: <TABLE1> JOIN <TABLE2> ON <TABLE1>.<KEY> = <TABLE2>.<KEY>
‚Ä¢ If a key/join path cannot be confirmed, state "Join not possible" for that path

Step 3 - PREPARE DATE CONVERSIONS (SNOWFLAKE-SPECIFIC)
‚Ä¢ For VARCHAR date columns: Use TRY_TO_DATE(<date_field>, 'YYYY-MM-DD')
‚Ä¢ For DATE/TIMESTAMP columns: Use as-is or CAST(<date_field> AS DATE) if needed
‚Ä¢ Never apply TRY_TO_DATE to columns already in DATE/TIMESTAMP format
‚Ä¢ Apply filters using appropriate format for the column type

Step 4 - BUILD OPTIMIZED SQL QUERY  
‚Ä¢ SELECT: Choose mapped fields and aggregations
‚Ä¢ FROM: Use the validated reference tables   
‚Ä¢ JOIN: Add all join steps confirmed above using ONLY listed join keys
‚Ä¢ WHERE: Add mapped, available filters with proper data type handling
‚Ä¢ GROUP BY: Add if user aggregation/grouping is requested and columns are mapped
‚Ä¢ ORDER BY: If required in user request
‚Ä¢ STRICT: DO NOT use subqueries or invented SQL logic

Step 5 - ENHANCED VALIDATION CHECKLIST  
‚ñ° Only listed tables/columns used  
‚ñ° All join steps follow provided JOIN_GRAPH‚Äîno invented joins  
‚ñ° All date logic follows Snowflake conventions  
‚ñ° No columns/filters/joins/objects invented or guessed  
‚ñ° All SQL clauses pass previous mapping and join plan checks
‚ñ° Context guidance properly applied if this is a follow-up query

**Enhanced Output Structure:**
**Mapping:**  
(User requirement ‚Üí Actual DB column mapping. If not found, do not use as filter.)

**Reasoning:**  
(Explain: 1. how context guidance was applied, 2. table join strategy, 3. column/filter selection logic, 4. optimization decisions. Max 150 words.)

**SQL Query:**  
(Production-ready SQL using only validated mappings and joins.)

****FEW-SHOT EXAMPLES:****
{few_shot_examples}
****END EXAMPLES****

From the above, provide valid JSON:
{{
    "Reasoning": "detailed explanation including context application",
    "SQL Query": "complete optimized SQL query"
}}

Strictly output JSON only. Validate that all columns in the SQL exist in relevant_columns.
"""
    
    messages = [{"role": "user", "content": gpt_prompt}]
    payload = {
        "username": "ENHANCED_SQL_AGENT",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.001,
        "max_tokens": 2048
    }
    matched_sql_query = []
    matched_reasoning = []

    try:
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        gpt_output = resp.json()['choices'][0]['message']['content']

        first = gpt_output.find('{')
        last = gpt_output.rfind('}')+1
        parsed = json.loads(gpt_output[first:last])
        matched_reasoning = parsed.get("Reasoning", [])
        matched_sql_query = parsed.get("SQL Query", [])
        
        print(f"[TOOL2] SQL generation complete with reasoning")
        
    except Exception as ex:
        print(f"[WARN] GPT reasoning/sqlquery mapping error: {ex}")
        
    state["query_llm_prompt"] = gpt_prompt
    state["query_llm_result"] = gpt_output
    state["query_reasoning"] = matched_reasoning
    state["query_sql"] = matched_sql_query
    return state

# ------------------ SNOWFLAKE SQL EXECUTION (UNCHANGED) --------------------

def run_query_and_return_df(state):
    """
    Executes the SQL query found in state['query_sql'] on Snowflake and returns DataFrame result.
    Updates state['snowflake_result'] with a pandas DataFrame (or None on error).
    """
    query_sql = state.get("query_sql", None)
    if not query_sql:
        print("[ERROR] No SQL found in state['query_sql']")
        state['snowflake_result'] = None
        return state
    
    # Connect to Snowflake
    conn = None
    try:
        print(f"[EXEC] Executing SQL on Snowflake...")
        conn = create_snowflake_connection()
        with conn.cursor() as cursor:
            cursor.execute(query_sql)
            # Fetch all rows and columns
            rows = cursor.fetchall()
            colnames = [d[0] for d in cursor.description]
            df = pd.DataFrame(rows, columns=colnames)
        state['snowflake_result'] = df
        print(f"[EXEC] Query executed successfully, returned {len(df)} rows")
    except Exception as ex:
        print(f"[ERROR] Snowflake query execution failed: {ex}")
        state['snowflake_result'] = None
    finally:
        if conn is not None:
            conn.close()
    return state
    
# ----------- ENHANCED PIPELINE ORCHESTRATION -----------

if __name__ == "__main__":
    print("=== Enhanced NLP-to-SQL Agent Pipeline ===")
    
    # Example agent history - this simulates previous interactions
    agent_history = [{
        "story": """This chart visualizes auto policies with premium over 10000 in Texas from year 2018 onwards, grouped by year""",
        "reasoning": """The tables INFORCE_DATA_FINAL, AUTO_VEHICLE_PREMIUM_DETAIL, and AUTO_VEHICLE_LEVEL_DATA are joined using POL_TX_ID and KEY_POL_RSK_ITM_ID. The query filters policies with premiums over 10000 in Texas from 2018 onwards. The EFFECTIVEDATE is converted to a date format for filtering. The results are grouped by year.""",
        "sql_query": """SELECT YEAR(TRY_TO_DATE(idf.EFFECTIVEDATE, 'YYYY-MM-DD')) AS policy_year, COUNT(*) AS policy_count FROM INFORCE_DATA_FINAL idf JOIN AUTO_VEHICLE_LEVEL_DATA avld ON idf.POL_TX_ID = avld.POL_TX_ID JOIN AUTO_VEHICLE_PREMIUM_DETAIL avpd ON avld.KEY_POL_RSK_ITM_ID = avpd.KEY_POL_RSK_ITM_ID WHERE avpd.ITM_TERM_PRM_AMT > 10000 AND idf."INSURED STATE" = 'TX' AND TRY_TO_DATE(idf.EFFECTIVEDATE, 'YYYY-MM-DD') >= '2018-01-01' GROUP BY policy_year;""",
        "charts": [
            {
                "title": "Auto policies with premium over 10000 in Texas from year 2018 onwards, grouped by year",
                "type": "bar",
                "chart_code": "plt.bar(data['policy_year'], data['policy_count'])",
                "dataframe": {
                    'policy_year': [2019, 2020, 2021, 2022, 2023],
                    'policy_count': [1500, 1750, 2000, 2250, 2500]
                }
            }
        ]
    }] 
    
    # Current user question - this is a follow-up that modifies the premium threshold
    user_question = "What about policies with premium over 50000 across all United States?"

    print(f"\nüîç Step 1: Enhanced Intent Classification")
    intent_info = classify_intent_and_context_gpt(user_question, agent_history, gpt_object)

    print(f"\nüèóÔ∏è  Step 2: Building Enhanced Pipeline State")
    # Initialize enhanced pipeline state
    state = {
        "user_question": user_question,
        "table_schema": all_tables,
        "columns_info": all_columns,
        "gpt_object": gpt_object,
        "intent_info": intent_info  # Add enhanced intent information to state
    }

    # Add legacy context for backward compatibility
    if intent_info["is_followup"] and intent_info["user_context"]:
        state["user_context"] = intent_info["user_context"]

    print(f"\nüóÇÔ∏è  Step 3: Enhanced Schema Selection")
    state = first_tool_call(state)
    print(f"Selected tables: {[t.get('table_name', 'Unknown') for t in state['relevant_tables_joins']]}")

    print(f"\n‚ö° Step 4: Enhanced SQL Generation")
    state = query_gen_node(state)
    
    print(f"\nüìä Step 5: Query Execution")
    state = run_query_and_return_df(state)

    # Display comprehensive results
    print(f"\n" + "="*60)
    print(f"ENHANCED PIPELINE RESULTS")
    print(f"="*60)
    
    print(f"\nüß† REASONING:")
    print(state["query_reasoning"])
    
    print(f"\nüíæ GENERATED SQL:")
    print(state["query_sql"])
    
    print(f"\nüìà EXECUTION RESULTS:")
    if isinstance(state['snowflake_result'], pd.DataFrame):
        print(f"Query returned {len(state['snowflake_result'])} rows")
        print(state['snowflake_result'].head())
    else:
        print("Query execution failed or returned no results")
    
    print(f"\n‚úÖ Pipeline completed successfully!")
```

```python
import json
import time
import pandas as pd
from typing import Dict, List, Tuple, Set
from src.snowflake_connection import create_snowflake_connection

# ------------------ INITIAL SETUP: LOAD DATA --------------------

from src.gpt_class import GptApi

gpt_object = GptApi()

# -------------------------------- ENHANCED INTENT AGENT ------------------

def classify_intent_and_context_gpt(user_question, agent_history, gpt_object):
    """
    Enhanced intent classification that provides structured context for better schema selection.
    
    Teaching Note: This function demonstrates how to use GPT for structured analysis tasks.
    Instead of trying to write rules for every possible way someone might express a follow-up
    question, we let GPT use its natural language understanding to categorize relationships.
    """
    # Build more structured history representation
    chat_history_str = ""
    inherited_elements = {}
    
    if agent_history:
        last = agent_history[-1]
        # Extract structured information from previous interaction
        prev_reasoning = last.get('reasoning', '')
        prev_sql = last.get('sql_query', '')
        prev_story = last.get('story', '')
        
        # Analyze previous SQL to extract reusable elements
        if prev_sql:
            inherited_elements = extract_sql_context_elements(prev_sql)
        
        chat_history_str = f"""
Previous Query Analysis:
- Story: {prev_story}
- Key Reasoning: {prev_reasoning}
- SQL Elements Used: Tables={inherited_elements.get('tables', [])}, 
  Time Filters={inherited_elements.get('time_filters', [])}, 
  Geographic Filters={inherited_elements.get('geo_filters', [])},
  Premium Filters={inherited_elements.get('premium_filters', [])}
        """
    else:
        chat_history_str = "(No previous context)"

    # Teaching Point: Notice how we structure this prompt to guide GPT's thinking process
    # We're not just asking "is this a follow-up?" - we're teaching GPT to analyze
    # the relationship systematically through specific steps
    prompt = f"""
You are an expert context analyst for insurance analytics queries. Your analysis will guide 
database schema selection, so provide structured, actionable context information.

PREVIOUS INTERACTION CONTEXT:
{chat_history_str}

CURRENT USER QUESTION:
"{user_question}"

ANALYSIS TASKS:

STEP 1 - RELATIONSHIP ANALYSIS:
Determine if the current question builds upon, modifies, or references the previous context.
Look for these relationship indicators:
- Comparative terms: "what about", "how about", "similar", "same but"
- Referential terms: "those", "them", "it", "that data"
- Modification terms: "increase to", "change to", "instead of"
- Expansion terms: "also show", "include", "add"

STEP 2 - CONTEXT TYPE CLASSIFICATION:
If this is a followup, classify the type:
- FILTER_MODIFICATION: Changing filters (amount thresholds, date ranges, locations)
- ENTITY_EXPANSION: Same analysis but different entity set (states, policy types)
- METRIC_CHANGE: Same data but different calculations or groupings
- COMPARISON_REQUEST: Comparing current request to previous results
- REFINEMENT: Narrowing down or expanding previous results

STEP 3 - ACTIONABLE CONTEXT EXTRACTION:
Extract specific elements that should inform schema selection:
- Time periods to maintain or modify
- Geographic constraints to keep or change
- Policy types or categories to focus on
- Premium ranges or other numeric filters
- Table relationships that should be preserved

OUTPUT REQUIREMENTS:
{{
    "is_followup": true/false,
    "confidence_level": "high/medium/low",
    "context_type": "FILTER_MODIFICATION|ENTITY_EXPANSION|METRIC_CHANGE|COMPARISON_REQUEST|REFINEMENT|NEW_QUERY",
    "inherited_elements": {{
        "preserve_time_filters": true/false,
        "preserve_geographic_filters": true/false, 
        "preserve_policy_types": true/false,
        "modify_premium_threshold": "specific instruction or null",
        "table_relationship_hint": "suggestion for schema selection"
    }},
    "user_context": "concise instruction for schema selection agent"
}}

Respond with valid JSON only. Focus on actionable context that helps with table/column selection.
"""

    messages = [{"role": "user", "content": prompt}]
    payload = {
        "username": "ENHANCED_INTENT_CLASSIFIER",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.05,
        "max_tokens": 1024
    }
    
    try:
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        content = resp.json()['choices'][0]['message']['content']
        
        # Parse JSON safely
        start = content.find("{")
        end = content.rfind("}") + 1
        parsed = json.loads(content[start:end])
        
        is_followup = parsed.get("is_followup", False)
        context_type = parsed.get("context_type", "NEW_QUERY")
        user_context = parsed.get("user_context", None)
        inherited_elements = parsed.get("inherited_elements", {})
        
        print(f"[INTENT] Follow-up: {is_followup}, Type: {context_type}")
        if user_context:
            print(f"[INTENT] Context for schema selection: {user_context}")
        
        return {
            "is_followup": is_followup,
            "user_context": user_context,
            "context_type": context_type,
            "inherited_elements": inherited_elements
        }
        
    except Exception as ex:
        print(f"[WARN] Intent classification error: {ex}")
        return {
            "is_followup": False,
            "user_context": None,
            "context_type": "NEW_QUERY",
            "inherited_elements": {}
        }

def extract_sql_context_elements(sql_query):
    """
    Helper function to extract reusable elements from previous SQL queries.
    
    Teaching Note: This function demonstrates a hybrid approach - using simple pattern
    matching for well-defined elements while preparing context for more sophisticated
    GPT analysis elsewhere in the pipeline.
    """
    elements = {
        "tables": [],
        "time_filters": [],
        "geo_filters": [],
        "premium_filters": [],
        "grouping_patterns": []
    }
    
    if not sql_query:
        return elements
    
    sql_lower = sql_query.lower()
    
    # Extract table patterns - this is straightforward pattern matching
    # Teaching Point: We use simple approaches where they work well, and GPT where they don't
    common_tables = ['inforce_data_final', 'auto_vehicle_premium_detail', 'auto_vehicle_level_data', 
                    'coverage_limit_details', 'cat_policy', 'claims_data']
    for table in common_tables:
        if table in sql_lower:
            elements["tables"].append(table)
    
    # Extract filter patterns
    if 'year(' in sql_lower or 'date' in sql_lower:
        elements["time_filters"].append("date_filtering_used")
    
    if 'state' in sql_lower or 'tx' in sql_lower or 'ca' in sql_lower:
        elements["geo_filters"].append("state_filtering_used")
        
    if 'premium' in sql_lower and ('>' in sql_lower or '<' in sql_lower):
        elements["premium_filters"].append("premium_threshold_used")
        
    if 'group by' in sql_lower:
        elements["grouping_patterns"].append("grouping_applied")
    
    return elements

# ------------------------------- LOAD TABLES SCHEMA (UNCHANGED) -------------------------------------

# Load table descriptions (schema, table, and per-table details)
table_catalog = pd.read_excel(
    "hackathon_final_schema_file_v1.xlsx",
    sheet_name='Table_descriptions'
)[['DATABASE', 'SCHEMA', 'TABLE', 'Brief_Description', 'Detailed_Comments']]

# Load column descriptions (per-table, per-column, and EDA)
column_catalog_df = pd.read_excel(
    "hackathon_final_schema_file_v1.xlsx",
    sheet_name="Table's Column Summaries"
)[['Table Name', 'Feature Name', 'Data Type', 'Description', 'sample_100_distinct']]

SAMPLE_100_VALUE_FIELD = "sample_100_distinct"  

# -- Convert to list-of-dicts for uniform access --
all_tables = [
    {
        "database": row["DATABASE"],
        "schema": row["SCHEMA"],
        "table_name": row["TABLE"],
        "brief_description": row["Brief_Description"],
        "detailed_comments": row["Detailed_Comments"]
    }
    for idx, row in table_catalog.iterrows()
]

all_columns = [
    {
        "table_name": row["Table Name"],
        "column_name": row["Feature Name"],
        "data_type": row["Data Type"],
        "description": row["Description"],
        "sample_100_distinct": row[SAMPLE_100_VALUE_FIELD]
        
    }
    for idx, row in column_catalog_df.iterrows()
]

# ------------------ CORE FOCUS: GPT-POWERED JOIN EXTRACTION FROM BRIEF DESCRIPTIONS -------------------------

def analyze_table_relationships_with_gpt(tables_context, available_columns, gpt_object):
    """
    This is the heart of our GPT-focused approach. Instead of trying to write complex
    rules to parse every possible way that join relationships might be described,
    we leverage GPT's natural language understanding to read your brief descriptions
    and extract structured relationship information.
    
    Teaching Concept: This function demonstrates the power of using AI for what it does
    best - understanding natural language and extracting structured information from
    unstructured text. Think of GPT as a skilled database analyst who can read your
    documentation and immediately understand how different tables connect.
    
    Args:
        tables_context: Your exact tables_context string with brief descriptions
        available_columns: Dictionary of available columns for validation
        gpt_object: GPT API object for natural language analysis
        
    Returns:
        Dictionary containing GPT's structured analysis of table relationships
    """
    print("[GPT_ANALYSIS] Using GPT to analyze table relationships from brief descriptions...")
    
    # Teaching Point: We prepare the available columns information to help GPT
    # make realistic suggestions. This is like giving a human analyst a reference
    # sheet of what columns exist in each table.
    columns_reference = ""
    for table_name, columns in available_columns.items():
        # Show first 8 columns to manage prompt size while providing key information
        column_names = [col['name'] for col in columns[:8]]
        columns_reference += f"\n{table_name}: {', '.join(column_names)}"
        if len(columns) > 8:
            columns_reference += f" (and {len(columns) - 8} more columns)"
    
    # Teaching Concept: This prompt is carefully structured to guide GPT through
    # the same thinking process a human database analyst would use when reading
    # documentation and identifying relationships.
    gpt_analysis_prompt = f"""
You are a senior database architect specializing in insurance systems. Your task is to analyze 
table descriptions and extract concrete join relationships using your natural language understanding.

Your expertise allows you to:
1. Understand the business purpose of each table from its description
2. Identify explicit and implicit relationships between tables
3. Recognize common insurance domain patterns and join keys
4. Validate relationships against available columns
5. Assess confidence levels based on evidence quality

INSURANCE DOMAIN CONTEXT:
- Policy data typically serves as the central hub with other tables linking to it
- Common join patterns: POL_TX_ID (policy transaction), PKEY (policy key), KEY_POL_RSK_ITM_ID (policy risk item)
- Auto insurance often separates policy details, vehicle information, and premium calculations
- CAT (catastrophic) insurance has specialized tables for events and coverage
- Claims data connects to policies through policy identifiers

TABLE DESCRIPTIONS TO ANALYZE:
{tables_context}

AVAILABLE COLUMNS FOR VALIDATION:
{columns_reference}

ANALYSIS INSTRUCTIONS:

STEP 1 - SEMANTIC COMPREHENSION:
Read each table description carefully. Use your natural language understanding to:
- Identify the business purpose and data scope of each table
- Look for explicit relationship mentions ("joins with", "links to", "connected via")
- Infer implicit relationships based on business logic and data flow
- Recognize common naming patterns that suggest relationships

STEP 2 - RELATIONSHIP EXTRACTION:
For each relationship you identify:
- Determine the specific column names used for joining
- Cross-reference with available columns to ensure suggested keys exist
- Consider both explicit mentions and reasonable inferences
- Apply insurance domain knowledge for unstated but logical connections

STEP 3 - EVIDENCE ASSESSMENT:
Rate confidence for each relationship:
- HIGH: Explicit mention of joins with specific column names
- MEDIUM: Clear business relationship with logical join keys
- LOW: Inferred relationship requiring validation

STEP 4 - DOMAIN VALIDATION:
Ensure relationships make business sense:
- Policy-centric architecture (other tables link to policy data)
- Logical parent-child relationships (claims link to policies, not vice versa)
- Consistent with insurance industry patterns

OUTPUT FORMAT:
Provide comprehensive JSON analysis:

{{
    "analysis_summary": {{
        "total_tables": number,
        "relationships_found": number,
        "high_confidence_count": number,
        "key_insights": "your main observations about the table architecture"
    }},
    "table_join_capabilities": {{
        "TABLE_NAME": {{
            "join_keys": ["key1", "key2"],
            "relationship_role": "central|supporting|leaf",
            "business_purpose": "brief description of table's role"
        }}
    }},
    "relationship_pairs": [
        {{
            "table_a": "TABLE_NAME_A",
            "table_b": "TABLE_NAME_B",
            "join_key_a": "COLUMN_NAME_A",
            "join_key_b": "COLUMN_NAME_B",
            "relationship_type": "one-to-many|many-to-one|one-to-one",
            "confidence": "high|medium|low",
            "evidence": "specific text from description supporting this relationship",
            "business_logic": "explanation of why this relationship exists"
        }}
    ]
}}

CRITICAL REQUIREMENTS:
- Only suggest join keys that exist in the available columns
- Provide specific evidence from descriptions for each relationship
- Focus on relationships that make business sense in insurance context
- Be conservative with confidence ratings - accuracy over completeness

Use your natural language understanding to extract maximum intelligence from these descriptions.
Respond with ONLY the JSON structure - no additional text.
"""

    # Execute GPT analysis using your existing API structure
    messages = [{"role": "user", "content": gpt_analysis_prompt}]
    payload = {
        "username": "GPT_TABLE_RELATIONSHIP_ANALYST",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.05,  # Very low temperature for consistent, focused analysis
        "max_tokens": 3500    # Generous token limit for comprehensive analysis
    }
    
    try:
        print("[GPT_ANALYSIS] Sending table descriptions to GPT for intelligent relationship analysis...")
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        content = resp.json()['choices'][0]['message']['content']
        
        # Parse GPT's structured response with robust error handling
        start = content.find('{')
        end = content.rfind('}') + 1
        if start == -1 or end == 0:
            raise ValueError("No valid JSON found in GPT response")
            
        gpt_analysis = json.loads(content[start:end])
        
        # Extract and validate the key components
        analysis_summary = gpt_analysis.get('analysis_summary', {})
        table_join_capabilities = gpt_analysis.get('table_join_capabilities', {})
        relationship_pairs = gpt_analysis.get('relationship_pairs', [])
        
        # Teaching Point: We log GPT's insights to help you understand what it discovered
        print(f"[GPT_ANALYSIS] ‚úÖ Analysis Complete - GPT's Insights:")
        print(f"  üìä Tables Analyzed: {analysis_summary.get('total_tables', 'Unknown')}")
        print(f"  üîó Relationships Found: {analysis_summary.get('relationships_found', 'Unknown')}")
        print(f"  üíé High Confidence: {analysis_summary.get('high_confidence_count', 'Unknown')}")
        
        if analysis_summary.get('key_insights'):
            print(f"  üß† Key Insights: {analysis_summary['key_insights']}")
        
        # Show examples of what GPT discovered
        high_confidence_rels = [r for r in relationship_pairs if r.get('confidence') == 'high']
        if high_confidence_rels:
            print(f"[GPT_ANALYSIS] üü¢ High Confidence Relationships (first 3):")
            for rel in high_confidence_rels[:3]:
                print(f"    {rel['table_a']}.{rel['join_key_a']} ‚ü∑ {rel['table_b']}.{rel['join_key_b']}")
                print(f"      Evidence: {rel.get('evidence', 'Not provided')[:70]}...")
        
        # Transform GPT's analysis into the format expected by your system
        # Teaching Point: We bridge between GPT's rich analysis and your system's needs
        processed_results = {
            'table_join_keys': {},
            'join_relationships': [],
            'gpt_insights': analysis_summary,
            'extraction_method': 'gpt_natural_language_analysis'
        }
        
        # Process table join capabilities
        for table_name, capabilities in table_join_capabilities.items():
            processed_results['table_join_keys'][table_name] = capabilities.get('join_keys', [])
        
        # Process relationship pairs
        for pair in relationship_pairs:
            processed_results['join_relationships'].append({
                'table_a': pair['table_a'],
                'table_b': pair['table_b'],
                'join_key_a': pair['join_key_a'],
                'join_key_b': pair['join_key_b'],
                'join_type': 'INNER',  # Default join type
                'confidence': pair.get('confidence', 'medium'),
                'evidence': pair.get('evidence', ''),
                'business_logic': pair.get('business_logic', ''),
                'relationship_type': pair.get('relationship_type', 'unknown')
            })
        
        return processed_results
        
    except Exception as ex:
        print(f"[ERROR] GPT relationship analysis failed: {ex}")
        # Provide fallback structure so pipeline can continue
        return {
            'table_join_keys': {},
            'join_relationships': [],
            'gpt_insights': {'error': str(ex)},
            'extraction_method': 'failed_gpt_analysis'
        }

def format_gpt_join_guidance(gpt_analysis, selected_tables=None):
    """
    Transform GPT's relationship analysis into clear, actionable guidance for schema selection.
    
    Teaching Note: This function demonstrates how to take GPT's rich, nuanced analysis
    and format it into concrete guidance that downstream processes can use effectively.
    Think of this as translating between GPT's comprehensive understanding and your
    system's operational needs.
    """
    relationship_pairs = gpt_analysis.get('join_relationships', [])
    gpt_insights = gpt_analysis.get('gpt_insights', {})
    
    # Filter to relevant tables if specified
    if selected_tables:
        relevant_table_names = [t.split('.')[-1] for t in selected_tables]
        relevant_relationships = []
        for rel in relationship_pairs:
            if (rel['table_a'] in relevant_table_names or 
                rel['table_b'] in relevant_table_names):
                relevant_relationships.append(rel)
        relationship_pairs = relevant_relationships
    
    if not relationship_pairs:
        return "No GPT-analyzed relationships found for the selected tables."
    
    # Organize relationships by GPT's confidence assessment
    high_confidence = [r for r in relationship_pairs if r.get('confidence') == 'high']
    medium_confidence = [r for r in relationship_pairs if r.get('confidence') == 'medium']
    low_confidence = [r for r in relationship_pairs if r.get('confidence') == 'low']
    
    # Build comprehensive guidance string
    guidance_parts = ["GPT RELATIONSHIP ANALYSIS (from brief description intelligence):"]
    
    # Include GPT's high-level insights
    if gpt_insights.get('key_insights'):
        guidance_parts.append(f"\nüß† GPT'S ARCHITECTURAL INSIGHTS:")
        guidance_parts.append(f"   {gpt_insights['key_insights']}")
    
    # High confidence relationships - GPT found strong evidence
    if high_confidence:
        guidance_parts.append("\nüü¢ HIGH CONFIDENCE RELATIONSHIPS (GPT found explicit evidence):")
        for rel in high_confidence:
            guidance_parts.append(f"   {rel['table_a']}.{rel['join_key_a']} ‚ü∑ {rel['table_b']}.{rel['join_key_b']}")
            if rel.get('evidence'):
                evidence_snippet = rel['evidence'][:80] + "..." if len(rel['evidence']) > 80 else rel['evidence']
                guidance_parts.append(f"     üìù Evidence: {evidence_snippet}")
            if rel.get('business_logic'):
                logic_snippet = rel['business_logic'][:80] + "..." if len(rel['business_logic']) > 80 else rel['business_logic']
                guidance_parts.append(f"     üíº Business Logic: {logic_snippet}")
    
    # Medium confidence relationships - GPT made logical inferences
    if medium_confidence:
        guidance_parts.append("\nüü° MEDIUM CONFIDENCE RELATIONSHIPS (GPT inferred from business logic):")
        for rel in medium_confidence:
            guidance_parts.append(f"   {rel['table_a']}.{rel['join_key_a']} ‚ü∑ {rel['table_b']}.{rel['join_key_b']}")
            if rel.get('business_logic'):
                logic_snippet = rel['business_logic'][:80] + "..." if len(rel['business_logic']) > 80 else rel['business_logic']
                guidance_parts.append(f"     üíº Logic: {logic_snippet}")
    
    # Low confidence relationships - use with caution
    if low_confidence:
        guidance_parts.append("\nüü† LOW CONFIDENCE RELATIONSHIPS (GPT suggests validation needed):")
        for rel in low_confidence:
            guidance_parts.append(f"   {rel['table_a']}.{rel['join_key_a']} ‚ü∑ {rel['table_b']}.{rel['join_key_b']}")
    
    # Usage guidance based on GPT's analysis
    guidance_parts.append("\nüìã USAGE GUIDANCE (based on GPT's analysis):")
    guidance_parts.append("‚Ä¢ Prioritize high confidence relationships - GPT found explicit evidence")
    guidance_parts.append("‚Ä¢ Medium confidence relationships are business-logic based - generally reliable")
    guidance_parts.append("‚Ä¢ GPT has validated all suggested keys against available columns")
    guidance_parts.append("‚Ä¢ Relationships reflect GPT's understanding of insurance domain patterns")
    
    return "\n".join(guidance_parts)

# ------------------ PROMPT UTILITIES (UNCHANGED) -------------------------

def build_column_context_block(relevant_columns, all_columns, max_sample_vals=15):
    """
    Build column context block for GPT prompts with sample values.
    
    Teaching Note: This function demonstrates how to prepare structured data
    for GPT consumption while managing prompt size constraints.
    """
    blocks = []
    for rel in relevant_columns:
        tname, cname = rel["table_name"], rel["column_name"]
        col_row = next(
            (c for c in all_columns if c["table_name"] == tname.split('.')[-1] and c["column_name"] == cname),
            None
        )
        if not col_row:
            continue  # missing in catalog

        # Process sample values for display
        raw_samples = col_row.get('sample_100_distinct', '')
        if isinstance(raw_samples, str):
            s = raw_samples.strip()
            if s.startswith("[") and s.endswith("]"):
                s = s[1:-1]
            vals = [v.strip("'\" ") for v in s.split(",") if v.strip().strip("'\"")]
        elif isinstance(raw_samples, list):
            vals = raw_samples
        else:
            vals = []

        # Limit sample values for prompt size management
        limited_vals = vals[:max_sample_vals]
        if len(vals) > max_sample_vals:
            limited_vals.append("...")

        block = (
            f"\n ||| Table_name: {tname} "
            f"|| Feature_name: {col_row['column_name']} "
            f"|| Data_Type: {col_row['data_type']} "
            f"|| Description: {col_row['description']} "
            f"|| Sample Values (separated by ,): {', '.join(limited_vals)} ||| \n"
        )
        blocks.append(block)
    return "".join(blocks)

# ------------------ CORE FUNCTION: GPT-POWERED SCHEMA SELECTION --------------------------

def extract_unique_joins(columns):
    """
    Fallback function to extract unique join pairs from column information.
    
    Teaching Note: This represents the "safety net" approach that we fall back to
    if GPT analysis fails. It's simpler but less intelligent than GPT's analysis.
    """
    join_pairs = set()
    for col in columns:
        join_key = col.get('JOIN_KEY', None)
        table_name = col.get('table_name', None)
        if join_key and table_name:
            join_pairs.add( (table_name, join_key) )
    return [ {"table_name": t, "join_key": k} for (t,k) in sorted(join_pairs) ]

def match_query_to_schema(user_question, all_tables, all_columns, gpt_object=None, 
                         user_context=None, intent_info=None):
    """
    THE CENTERPIECE: GPT-focused schema selection that primarily relies on GPT's
    natural language understanding to extract join relationships from brief descriptions.
    
    Teaching Concept: This function represents the evolution from rule-based to 
    intelligence-based database analysis. Instead of trying to anticipate every
    possible way join relationships might be described, we leverage GPT's natural
    language understanding to comprehend your documentation like a human would.
    
    The key insight: GPT has been trained on millions of examples of technical
    documentation and database schemas. When it reads your brief descriptions,
    it applies this vast experience to understand relationships, even when they're
    expressed in subtle or indirect ways.
    
    Think of this transformation:
    - OLD: "Let me write rules to catch every possible join pattern"
    - NEW: "Let me use GPT to understand what the documentation is telling me"
    
    Args:
        user_question: The natural language query to analyze
        all_tables: Your table catalog with brief descriptions
        all_columns: Your column catalog
        gpt_object: GPT API object for intelligent analysis
        user_context: Legacy context parameter for compatibility
        intent_info: Enhanced intent information from previous steps
        
    Returns:
        Tuple of (table_joins, relevant_columns, matched_joins) with GPT-extracted intelligence
    """
    uq = user_question.lower()
    print(f"[SCHEMA] Processing query with GPT-focused relationship extraction: {user_question}")

    # Teaching Point: We prepare the exact tables_context format you specified
    # This is the string that contains all your brief descriptions
    tables_context = "\n".join(
        [f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}: {tbl['brief_description']}" for tbl in all_tables])
    
    # Prepare available columns information for GPT validation
    available_columns = {}
    for col in all_columns:
        table_name = col['table_name']
        if table_name not in available_columns:
            available_columns[table_name] = []
        available_columns[table_name].append({
            'name': col['column_name'],
            'type': col['data_type'],
            'description': col['description']
        })
    
    # --- PRIMARY APPROACH: GPT-POWERED RELATIONSHIP EXTRACTION ---
    if gpt_object is not None:
        # Teaching Concept: This is where the magic happens - we use GPT's natural
        # language understanding to analyze your tables_context and extract join relationships
        print("[SCHEMA] üß† Leveraging GPT's natural language understanding for relationship extraction...")
        
        gpt_relationship_analysis = analyze_table_relationships_with_gpt(
            tables_context, available_columns, gpt_object)
        
        # Format GPT's analysis into guidance for schema selection
        gpt_join_guidance = format_gpt_join_guidance(gpt_relationship_analysis)
        
        # Build columns context for schema selection
        columns_context = "\n".join(
            [f"{col['table_name']}.{col['column_name']}: {col['description']}" for col in all_columns])
        
        # Prepare context guidance from intent analysis
        context_guidance = ""
        if intent_info and intent_info.get("is_followup"):
            context_type = intent_info.get("context_type", "")
            context_guidance = f"\nCONTEXT FROM INTENT ANALYSIS:\n"
            context_guidance += f"Query Type: {context_type}\n"
            if intent_info.get("user_context"):
                context_guidance += f"Specific Guidance: {intent_info['user_context']}\n"
        
        # Teaching Concept: This prompt demonstrates how to combine GPT's relationship
        # analysis with schema selection logic. We're not asking GPT to guess about
        # relationships - we're providing it with GPT's own analysis as authoritative input.
        schema_selection_prompt = f"""
You are an expert insurance database architect with access to GPT's intelligent analysis 
of table relationships extracted from brief descriptions. You have concrete, analyzed 
relationship information rather than needing to guess about connections.

CRITICAL DOMAIN KNOWLEDGE:
- Schema 'RAW_CI_CAT_ANALYSIS': Catastrophic insurance policies and events
- Schema 'RAW_CI_INFORCE': Active policies, auto vehicle details, standard premium data
- Schema 'RAW_CI_CLAIMS': Claims processing, settlements, adjustments

{context_guidance}

USER QUESTION: {user_question}

AVAILABLE TABLES:
{tables_context}

GPT'S RELATIONSHIP ANALYSIS:
{gpt_join_guidance}

AVAILABLE COLUMNS:
{columns_context}

SCHEMA SELECTION PROCESS WITH GPT INTELLIGENCE:

STEP 1 - DOMAIN CLASSIFICATION:
Based on the user question, classify the query domain:
- AUTO: Auto insurance (use RAW_CI_INFORCE primarily)
- CAT: Catastrophic insurance (use RAW_CI_CAT_ANALYSIS)
- CLAIMS: Claims processing (use RAW_CI_CLAIMS)
- GENERAL: Cross-domain analysis

STEP 2 - INTELLIGENT TABLE SELECTION:
Using GPT's relationship analysis as your authoritative guide:
- Select tables based on their relevance to the user question
- Ensure selected tables can be connected via GPT's analyzed relationships
- Prioritize tables with high confidence relationship connections
- Consider the table's business purpose as identified by GPT

STEP 3 - GPT-VALIDATED JOIN PLANNING:
For table connections:
- Use ONLY the relationships that GPT identified in its analysis
- Prefer high confidence relationships over medium/low confidence ones
- Trust GPT's business logic reasoning about why relationships exist
- Ensure the join path creates a connected graph of selected tables

STEP 4 - TARGETED COLUMN SELECTION:
Select columns based on query requirements:
- Filtering: columns needed for WHERE clauses
- Display: columns for SELECT output
- Grouping: columns for GROUP BY operations
- Calculation: columns for mathematical operations

VALIDATION WITH GPT INTELLIGENCE:
- Every table connection must use GPT's analyzed relationships
- All suggested join keys have been validated by GPT against available columns
- Relationships reflect GPT's understanding of insurance domain patterns
- Business logic for each relationship has been verified by GPT

OUTPUT FORMAT:
{{
    "domain_classification": "AUTO|CAT|CLAIMS|GENERAL",
    "gpt_intelligence_application": "how you used GPT's relationship analysis in your selection",
    "selection_reasoning": {{
        "domain_rationale": "why you classified the query this way",
        "table_selection_logic": "how you selected tables using GPT's analysis",
        "relationship_validation": "how you validated connections using GPT's findings",
        "column_selection_rationale": "why these columns serve the query requirements"
    }},
    "relevant_tables_joins": [
        {{
            "table_name": "FULL.SCHEMA.TABLE_NAME",
            "join_keys": ["key1", "key2"],
            "selection_reason": "business justification for including this table",
            "gpt_relationship_support": "how GPT's analysis supports this table's inclusion"
        }}
    ],
    "relevant_columns": [
        {{
            "table_name": "TABLE_NAME",
            "column_name": "COLUMN_NAME", 
            "usage_purpose": "filtering|display|grouping|calculation"
        }}
    ]
}}

CRITICAL SUCCESS FACTOR: Leverage GPT's relationship analysis as your authoritative 
source for table connections. This eliminates guesswork and ensures reliable schema selection.

Respond with ONLY valid JSON. No additional text.
"""
        
        # Execute schema selection with GPT intelligence
        messages = [{"role": "user", "content": schema_selection_prompt}]
        payload = {
            "username": "GPT_INTELLIGENCE_SCHEMA_SELECTOR",
            "session_id": "1",
            "messages": messages,
            "temperature": 0.001,  # Very low temperature for consistent selection
            "max_tokens": 2048
        }
        
        try:
            resp = gpt_object.get_gpt_response_non_streaming(payload)
            content = resp.json()['choices'][0]['message']['content']
            first = content.find('{')
            last = content.rfind('}') + 1
            parsed = json.loads(content[first:last])
            
            # Extract and validate results using GPT's relationship analysis
            if "relevant_tables_joins" in parsed:
                matched_tables_joins = parsed["relevant_tables_joins"]
                # Validate against GPT's analyzed relationships
                validated_tables_joins = []
                for table_info in matched_tables_joins:
                    table_name = table_info.get("table_name", "")
                    join_keys = table_info.get("join_keys", [])
                    
                    # Validate against GPT's relationship analysis
                    table_name_clean = table_name.split('.')[-1]
                    if table_name_clean in gpt_relationship_analysis['table_join_keys']:
                        gpt_validated_keys = gpt_relationship_analysis['table_join_keys'][table_name_clean]
                        valid_keys = [key for key in join_keys if key in gpt_validated_keys]
                        if valid_keys:
                            validated_table = table_info.copy()
                            validated_table["join_keys"] = valid_keys
                            validated_tables_joins.append(validated_table)
                            print(f"[SCHEMA] ‚úÖ GPT-validated: {table_name_clean} with keys {valid_keys}")
                        else:
                            print(f"[SCHEMA] ‚ùå No GPT-validated keys for {table_name_clean}")
                    else:
                        print(f"[SCHEMA] ‚ö†Ô∏è  {table_name_clean} not in GPT relationship analysis")
                
                matched_tables_joins = validated_tables_joins
            else:
                matched_tables_joins = []
                
            matched_columns = parsed.get("relevant_columns", [])
            
            # Log the results of GPT intelligence application
            print(f"[SCHEMA] Domain: {parsed.get('domain_classification', 'Unknown')}")
            if parsed.get('gpt_intelligence_application'):
                print(f"[SCHEMA] üß† GPT Intelligence: {parsed['gpt_intelligence_application']}")
            
            if "selection_reasoning" in parsed:
                reasoning = parsed["selection_reasoning"]
                print(f"[SCHEMA] Logic: {reasoning.get('table_selection_logic', 'Not provided')}")
                
        except Exception as ex:
            print(f"[WARN] GPT-powered schema selection error: {ex}")
            matched_tables_joins = []
            matched_columns = []
    
    else:
        # Teaching Point: This is the fallback when GPT is not available
        # We still try to be intelligent, but without GPT's natural language understanding
        print("[SCHEMA] GPT not available - using fallback keyword matching")
        matched_tables_joins = []
        matched_columns = []
    
    # --- FALLBACK APPROACH: KEYWORD MATCHING WITH GPT ENHANCEMENT ---
    if not matched_tables_joins and not matched_columns:
        print("[SCHEMA] Using enhanced fallback with GPT relationship analysis...")
        
        # Even in fallback, we can leverage GPT's relationship analysis if available
        if gpt_object:
            gpt_relationship_analysis = analyze_table_relationships_with_gpt(
                tables_context, available_columns, gpt_object)
        else:
            gpt_relationship_analysis = {'table_join_keys': {}, 'join_relationships': []}
        
        # Keyword-based table matching
        keywords = set(uq.replace(",", " ").replace("_", " ").split())
        matched_table_objs = [
            tbl for tbl in all_tables
            if any(k in (tbl['table_name'].lower() + " " + 
                        str(tbl.get('brief_description', ''))).lower()
                   for k in keywords)
        ]
        
        # Build table joins using GPT's relationship analysis even in fallback
        matched_tables_joins = []
        for tbl in matched_table_objs:
            table_name = tbl['table_name']
            join_keys = gpt_relationship_analysis['table_join_keys'].get(table_name, [])
            matched_tables_joins.append({
                "table_name": f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}",
                "join_keys": join_keys
            })
        
        # Column matching with keyword approach
        matched_columns = [
            {"table_name": col['table_name'], "column_name": col['column_name']}
            for col in all_columns
            if any(k in (col['column_name'] + " " + str(col.get('description', ''))).lower()
                   for k in keywords)
        ]
    
    # --- GENERATE MATCHED JOINS USING GPT INTELLIGENCE ---
    # Teaching Concept: This section demonstrates how to use GPT's relationship analysis
    # to generate concrete join information for downstream SQL generation
    matched_joins = []
    seen_pairs = set()
    
    # First, extract joins from our table information
    for t in matched_tables_joins:
        tname = t.get("table_name", "")
        join_keys = t.get("join_keys", [])
        if isinstance(join_keys, list):
            for key in join_keys:
                if (tname, key) not in seen_pairs:
                    matched_joins.append({"table_name": tname, "join_key": key})
                    seen_pairs.add((tname, key))
    
    # If we need more join information, use GPT's relationship analysis
    if len(matched_joins) < 2 and len(matched_tables_joins) > 1:
        if 'gpt_relationship_analysis' not in locals() and gpt_object:
            gpt_relationship_analysis = analyze_table_relationships_with_gpt(
                tables_context, available_columns, gpt_object)
        
        if 'gpt_relationship_analysis' in locals():
            table_names = [t.get("table_name", "").split('.')[-1] for t in matched_tables_joins]
            
            # Use GPT's relationship pairs to generate joins
            for relationship in gpt_relationship_analysis.get('join_relationships', []):
                if (relationship['table_a'] in table_names and 
                    relationship['table_b'] in table_names):
                    # Prefer high confidence relationships
                    if relationship.get('confidence') in ['high', 'medium']:
                        for table_name, join_key in [(relationship['table_a'], relationship['join_key_a']),
                                                     (relationship['table_b'], relationship['join_key_b'])]:
                            if (table_name, join_key) not in seen_pairs:
                                matched_joins.append({"table_name": table_name, "join_key": join_key})
                                seen_pairs.add((table_name, join_key))
    
    # Final fallback to original logic if still insufficient
    if not matched_joins:
        matched_joins = extract_unique_joins(matched_columns)
    
    # Results summary
    print(f"[SCHEMA] üéØ GPT-powered selection complete:")
    print(f"  üìä Tables: {len(matched_tables_joins)}")
    print(f"  üìã Columns: {len(matched_columns)}")
    print(f"  üîó Join Points: {len(matched_joins)}")
    
    return matched_tables_joins, matched_columns, matched_joins

# ------------------ ENHANCED FIRST TOOL CALL --------------------------

def first_tool_call(state):
    """
    Enhanced Node 1: Schema selection that primarily leverages GPT's natural language
    understanding to extract join relationships from your brief descriptions.
    
    Teaching Note: This function represents the integration point where GPT's
    intelligence becomes operational in your pipeline. The key insight is that
    we're not just using GPT as a better search engine - we're using it as a
    database relationship analyst.
    """
    user_question = state.get("user_question")
    all_tables = state.get("table_schema")
    all_columns = state.get("columns_info")
    gpt_object = state.get("gpt_object", None)
    user_context = state.get("user_context", None)
    intent_info = state.get("intent_info", None)
    
    print(f"[TOOL1] Schema selection with GPT relationship intelligence")
    
    # Call our GPT-focused schema selection
    relevant_tables_joins, relevant_columns, relevant_joins = match_query_to_schema(
        user_question, all_tables, all_columns, gpt_object, 
        user_context=user_context, intent_info=intent_info)
    
    # Update state with GPT-derived results
    state["relevant_tables_joins"] = relevant_tables_joins
    state["relevant_columns"] = relevant_columns
    state["relevant_joins"] = relevant_joins
    
    print(f"[TOOL1] ‚úÖ GPT-powered schema selection complete")
    return state

# -------------------- SQL GENERATION WITH GPT RELATIONSHIP INTELLIGENCE ----------------------

def query_gen_node(state):
    """
    Enhanced Node 2: SQL generation that leverages GPT's relationship analysis
    for more accurate and reliable query construction.
    
    Teaching Concept: This function demonstrates how GPT's relationship intelligence
    flows through your entire pipeline. The SQL generator now has access to
    GPT's understanding of how tables connect, leading to more reliable queries.
    """
    user_question = state.get("user_question")
    all_columns = state.get("columns_info")
    relevant_columns = state.get("relevant_columns")
    relevant_joins = state.get("relevant_tables_joins")
    gpt_object = state.get("gpt_object")
    intent_info = state.get("intent_info", {})

    print(f"[TOOL2] SQL generation with GPT relationship intelligence")
    
    # Build column context for SQL generation
    context_block = build_column_context_block(relevant_columns, all_columns, 15)
    
    # Get GPT's relationship analysis for SQL generation guidance
    all_tables = state.get("table_schema", [])
    if gpt_object:
        # Rebuild tables_context for GPT analysis
        tables_context = "\n".join(
            [f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}: {tbl['brief_description']}" for tbl in all_tables])
        
        # Prepare columns information
        available_columns = {}
        for col in all_columns:
            table_name = col['table_name']
            if table_name not in available_columns:
                available_columns[table_name] = []
            available_columns[table_name].append({
                'name': col['column_name'],
                'type': col['data_type'],
                'description': col['description']
            })
        
        # Get GPT's relationship analysis for SQL context
        gpt_relationship_analysis = analyze_table_relationships_with_gpt(
            tables_context, available_columns, gpt_object)
        
        # Format relationship guidance for SQL generation
        selected_table_names = [t.get("table_name", "").split('.')[-1] for t in relevant_joins]
        sql_relationship_guidance = format_gpt_join_guidance(
            gpt_relationship_analysis, selected_table_names)
    else:
        sql_relationship_guidance = "GPT relationship analysis not available"
    
    # Build context guidance from intent analysis
    context_guidance = ""
    if intent_info.get("is_followup"):
        context_type = intent_info.get("context_type", "")
        context_guidance = f"\nCONTEXT FROM INTENT ANALYSIS:\n"
        context_guidance += f"Query Type: {context_type}\n"
        
        if context_type == "FILTER_MODIFICATION":
            context_guidance += "- Modify filter conditions while preserving table relationships\n"
        elif context_type == "ENTITY_EXPANSION":
            context_guidance += "- Expand entity scope while maintaining similar analytical structure\n"
        elif context_type == "METRIC_CHANGE":
            context_guidance += "- Focus on new metric requirements while preserving data relationships\n"
        
        if intent_info.get("inherited_elements", {}).get("modify_premium_threshold"):
            context_guidance += f"- Premium threshold: {intent_info['inherited_elements']['modify_premium_threshold']}\n"
    
    # Load few-shot examples
    with open('few_shot_examples.txt', 'r', encoding='utf-8') as f:
        few_shot_examples = f.read()
    
    # Enhanced SQL generation prompt leveraging GPT relationship intelligence
    sql_generation_prompt = f"""
You are an expert SQL generator for insurance analytics on Snowflake, with access to 
GPT's intelligent analysis of table relationships from documentation.

{context_guidance}

USER QUESTION: {user_question}

GPT'S RELATIONSHIP ANALYSIS FOR SELECTED TABLES:
{sql_relationship_guidance}

SELECTED TABLES AND KEYS:
{relevant_joins}

COLUMN DETAILS:
{context_block}

SQL GENERATION WITH GPT RELATIONSHIP INTELLIGENCE:

STEP 1 - REQUIREMENT ANALYSIS:
- Parse user question for metrics, filters, aggregations, and grouping needs
- Apply any context guidance from intent analysis
- Map requirements to specific columns from the available set

STEP 2 - INTELLIGENT JOIN CONSTRUCTION:
- Use GPT's relationship analysis as the authoritative source for table connections
- Prioritize high confidence relationships identified by GPT
- Build join paths using GPT's validated join keys
- Trust GPT's business logic reasoning about relationship purposes

STEP 3 - SNOWFLAKE-SPECIFIC OPTIMIZATION:
- Use TRY_TO_DATE() for VARCHAR date columns
- Use native DATE/TIMESTAMP handling for proper date columns
- Apply appropriate data type conversions
- Implement efficient filtering strategies

STEP 4 - QUERY CONSTRUCTION:
- SELECT: Include required columns for output and calculations
- FROM: Start with primary table identified by GPT's analysis
- JOIN: Use GPT's analyzed relationships with validated keys
- WHERE: Apply filters with proper data type handling
- GROUP BY: Add grouping as required by user question
- ORDER BY: Include ordering if specified

VALIDATION CHECKLIST:
‚ñ° All joins use GPT-analyzed relationships
‚ñ° All columns exist in the provided column set
‚ñ° Date handling follows Snowflake conventions
‚ñ° No invented or assumed relationships
‚ñ° Context guidance properly applied

FEW-SHOT EXAMPLES:
{few_shot_examples}

OUTPUT FORMAT:
{{
    "GPT_Relationship_Usage": "how GPT's relationship analysis guided join construction",
    "Reasoning": "step-by-step explanation of query construction approach",
    "SQL Query": "complete optimized SQL query"
}}

Generate production-ready SQL leveraging GPT's relationship intelligence.
Respond with ONLY valid JSON.
"""
    
    # Execute SQL generation
    messages = [{"role": "user", "content": sql_generation_prompt}]
    payload = {
        "username": "GPT_RELATIONSHIP_SQL_GENERATOR",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.001,
        "max_tokens": 2048
    }
    
    try:
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        gpt_output = resp.json()['choices'][0]['message']['content']
        
        first = gpt_output.find('{')
        last = gpt_output.rfind('}') + 1
        parsed = json.loads(gpt_output[first:last])
        
        matched_reasoning = parsed.get("Reasoning", "")
        matched_sql_query = parsed.get("SQL Query", "")
        gpt_relationship_usage = parsed.get("GPT_Relationship_Usage", "")
        
        print(f"[TOOL2] ‚úÖ SQL generation complete")
        if gpt_relationship_usage:
            print(f"[TOOL2] üß† GPT Relationship Intelligence: {gpt_relationship_usage}")
        
    except Exception as ex:
        print(f"[WARN] SQL generation error: {ex}")
        matched_reasoning = f"Error in SQL generation: {ex}"
        matched_sql_query = ""
    
    # Update state with results
    state["query_llm_prompt"] = sql_generation_prompt
    state["query_llm_result"] = gpt_output
    state["query_reasoning"] = matched_reasoning
    state["query_sql"] = matched_sql_query
    
    return state

# ------------------ SNOWFLAKE SQL EXECUTION (UNCHANGED) --------------------

def run_query_and_return_df(state):
    """
    Execute the generated SQL query on Snowflake and return results.
    
    Teaching Note: This function remains unchanged because once we have
    reliable SQL generation (thanks to GPT's relationship intelligence),
    the execution process is straightforward.
    """
    query_sql = state.get("query_sql", None)
    if not query_sql:
        print("[ERROR] No SQL query generated")
        state['snowflake_result'] = None
        return state
    
    conn = None
    try:
        print(f"[EXEC] Executing SQL query on Snowflake...")
        conn = create_snowflake_connection()
        with conn.cursor() as cursor:
            cursor.execute(query_sql)
            rows = cursor.fetchall()
            colnames = [d[0] for d in cursor.description]
            df = pd.DataFrame(rows, columns=colnames)
        
        state['snowflake_result'] = df
        print(f"[EXEC] ‚úÖ Query executed successfully: {len(df)} rows returned")
        
    except Exception as ex:
        print(f"[ERROR] ‚ùå Query execution failed: {ex}")
        state['snowflake_result'] = None
        
    finally:
        if conn is not None:
            conn.close()
    
    return state

# ----------- COMPLETE PIPELINE ORCHESTRATION -----------

if __name__ == "__main__":
    print("=== üéØ GPT-Focused Join Extraction Pipeline ===")
    print("This pipeline demonstrates the power of leveraging GPT's natural language")
    print("understanding to extract join relationships from brief descriptions,")
    print("representing a fundamental shift from rule-based to intelligence-based analysis.\n")
    
    # Teaching Example: This represents a typical interaction history
    agent_history = [{
        "story": "Auto policies with premium over 10000 in Texas from 2018 onwards, grouped by year",
        "reasoning": "Tables joined using POL_TX_ID and KEY_POL_RSK_ITM_ID with date filtering and geographical constraints",
        "sql_query": "SELECT YEAR(TRY_TO_DATE(idf.EFFECTIVEDATE, 'YYYY-MM-DD')) AS policy_year, COUNT(*) AS policy_count FROM INFORCE_DATA_FINAL idf JOIN AUTO_VEHICLE_LEVEL_DATA avld ON idf.POL_TX_ID = avld.POL_TX_ID JOIN AUTO_VEHICLE_PREMIUM_DETAIL avpd ON avld.KEY_POL_RSK_ITM_ID = avpd.KEY_POL_RSK_ITM_ID WHERE avpd.ITM_TERM_PRM_AMT > 10000 AND idf.\"INSURED STATE\" = 'TX' AND TRY_TO_DATE(idf.EFFECTIVEDATE, 'YYYY-MM-DD') >= '2018-01-01' GROUP BY policy_year;",
        "charts": [{"title": "Auto policies analysis", "type": "bar", "dataframe": {"policy_year": [2019, 2020, 2021, 2022, 2023], "policy_count": [1500, 1750, 2000, 2250, 2500]}}]
    }]
    
    # Teaching Example: This demonstrates a follow-up query pattern
    user_question = "What about policies with premium over 50000 across all United States?"
    
    print(f"üîç Step 1: Intent Classification and Context Analysis")
    intent_info = classify_intent_and_context_gpt(user_question, agent_history, gpt_object)
    
    print(f"\nüèóÔ∏è  Step 2: Building GPT-Focused Pipeline State")
    state = {
        "user_question": user_question,
        "table_schema": all_tables,
        "columns_info": all_columns,
        "gpt_object": gpt_object,
        "intent_info": intent_info
    }
    
    # Legacy compatibility
    if intent_info["is_followup"] and intent_info["user_context"]:
        state["user_context"] = intent_info["user_context"]
    
    print(f"\nüß† Step 3: GPT-Powered Schema Selection")
    state = first_tool_call(state)
    selected_tables = [t.get('table_name', 'Unknown') for t in state['relevant_tables_joins']]
    print(f"Selected tables: {selected_tables}")
    
    print(f"\n‚ö° Step 4: SQL Generation with GPT Relationship Intelligence")
    state = query_gen_node(state)
    
    print(f"\nüìä Step 5: Query Execution")
    state = run_query_and_return_df(state)
    
    # Display comprehensive results
    print(f"\n" + "="*80)
    print(f"üéØ GPT-FOCUSED JOIN EXTRACTION RESULTS")
    print(f"="*80)
    
    print(f"\nüß† REASONING (Enhanced by GPT Relationship Intelligence):")
    print(state["query_reasoning"])
    
    print(f"\nüíæ GENERATED SQL (Using GPT-Extracted Relationships):")
    print(state["query_sql"])
    
    print(f"\nüìà EXECUTION RESULTS:")
    if isinstance(state['snowflake_result'], pd.DataFrame):
        print(f"‚úÖ Query executed successfully: {len(state['snowflake_result'])} rows")
        print(state['snowflake_result'].head())
    else:
        print("‚ùå Query execution failed or returned no results")
    
    print(f"\nüéâ GPT-focused pipeline completed successfully!")
    print(f"üîë Key Innovation: GPT's natural language understanding extracts join relationships")
    print(f"üìä Impact: Intelligence-based schema selection with validated relationships")
    print(f"üöÄ Result: More accurate and reliable SQL generation through GPT analysis")
```

```python
import json
import time
import pandas as pd
from typing import Dict, List, Tuple, Set
from src.snowflake_connection import create_snowflake_connection

# ------------------ INITIAL SETUP: LOAD DATA --------------------

from src.gpt_class import GptApi

gpt_object = GptApi()

# -------------------------------- ENHANCED INTENT AGENT ------------------

def classify_intent_and_context_gpt(user_question, agent_history, gpt_object):
    """
    Enhanced intent classification that provides structured context for better schema selection.
    This function now extracts specific types of context that help the schema selection 
    agent make better decisions about tables and joins.
    """
    # Build more structured history representation
    chat_history_str = ""
    inherited_elements = {}
    
    if agent_history:
        last = agent_history[-1]
        # Extract structured information from previous interaction
        prev_reasoning = last.get('reasoning', '')
        prev_sql = last.get('sql_query', '')
        prev_story = last.get('story', '')
        
        # Analyze previous SQL to extract reusable elements
        if prev_sql:
            inherited_elements = extract_sql_context_elements(prev_sql)
        
        chat_history_str = f"""
Previous Query Analysis:
- Story: {prev_story}
- Key Reasoning: {prev_reasoning}
- SQL Elements Used: Tables={inherited_elements.get('tables', [])}, 
  Time Filters={inherited_elements.get('time_filters', [])}, 
  Geographic Filters={inherited_elements.get('geo_filters', [])},
  Premium Filters={inherited_elements.get('premium_filters', [])}
        """
    else:
        chat_history_str = "(No previous context)"

    # Enhanced prompt for better context extraction
    prompt = f"""
You are an expert context analyst for insurance analytics queries. Your analysis will guide 
database schema selection, so provide structured, actionable context information.

PREVIOUS INTERACTION CONTEXT:
{chat_history_str}

CURRENT USER QUESTION:
"{user_question}"

ANALYSIS TASKS:

STEP 1 - RELATIONSHIP ANALYSIS:
Determine if the current question builds upon, modifies, or references the previous context.
Look for these relationship indicators:
- Comparative terms: "what about", "how about", "similar", "same but"
- Referential terms: "those", "them", "it", "that data"
- Modification terms: "increase to", "change to", "instead of"
- Expansion terms: "also show", "include", "add"

STEP 2 - CONTEXT TYPE CLASSIFICATION:
If this is a followup, classify the type:
- FILTER_MODIFICATION: Changing filters (amount thresholds, date ranges, locations)
- ENTITY_EXPANSION: Same analysis but different entity set (states, policy types)
- METRIC_CHANGE: Same data but different calculations or groupings
- COMPARISON_REQUEST: Comparing current request to previous results
- REFINEMENT: Narrowing down or expanding previous results

STEP 3 - ACTIONABLE CONTEXT EXTRACTION:
Extract specific elements that should inform schema selection:
- Time periods to maintain or modify
- Geographic constraints to keep or change
- Policy types or categories to focus on
- Premium ranges or other numeric filters
- Table relationships that should be preserved

OUTPUT REQUIREMENTS:
{{
    "is_followup": true/false,
    "confidence_level": "high/medium/low",
    "context_type": "FILTER_MODIFICATION|ENTITY_EXPANSION|METRIC_CHANGE|COMPARISON_REQUEST|REFINEMENT|NEW_QUERY",
    "inherited_elements": {{
        "preserve_time_filters": true/false,
        "preserve_geographic_filters": true/false, 
        "preserve_policy_types": true/false,
        "modify_premium_threshold": "specific instruction or null",
        "table_relationship_hint": "suggestion for schema selection"
    }},
    "user_context": "concise instruction for schema selection agent"
}}

Respond with valid JSON only. Focus on actionable context that helps with table/column selection.
"""

    messages = [{"role": "user", "content": prompt}]
    payload = {
        "username": "ENHANCED_INTENT_CLASSIFIER",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.05,
        "max_tokens": 1024
    }
    
    try:
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        content = resp.json()['choices'][0]['message']['content']
        
        # Parse JSON safely
        start = content.find("{")
        end = content.rfind("}") + 1
        parsed = json.loads(content[start:end])
        
        is_followup = parsed.get("is_followup", False)
        context_type = parsed.get("context_type", "NEW_QUERY")
        user_context = parsed.get("user_context", None)
        inherited_elements = parsed.get("inherited_elements", {})
        
        print(f"[INTENT] Follow-up: {is_followup}, Type: {context_type}")
        if user_context:
            print(f"[INTENT] Context for schema selection: {user_context}")
        
        # Return enhanced context structure
        return {
            "is_followup": is_followup,
            "user_context": user_context,
            "context_type": context_type,
            "inherited_elements": inherited_elements
        }
        
    except Exception as ex:
        print(f"[WARN] Intent classification error: {ex}")
        return {
            "is_followup": False,
            "user_context": None,
            "context_type": "NEW_QUERY",
            "inherited_elements": {}
        }

def extract_sql_context_elements(sql_query):
    """
    Helper function to extract reusable elements from previous SQL queries.
    This helps the intent classifier provide better context to schema selection.
    """
    elements = {
        "tables": [],
        "time_filters": [],
        "geo_filters": [],
        "premium_filters": [],
        "grouping_patterns": []
    }
    
    if not sql_query:
        return elements
    
    sql_lower = sql_query.lower()
    
    # Extract table patterns (simple pattern matching)
    common_tables = ['inforce_data_final', 'auto_vehicle_premium_detail', 'auto_vehicle_level_data', 
                    'coverage_limit_details', 'cat_policy', 'claims_data']
    for table in common_tables:
        if table in sql_lower:
            elements["tables"].append(table)
    
    # Extract filter patterns
    if 'year(' in sql_lower or 'date' in sql_lower:
        elements["time_filters"].append("date_filtering_used")
    
    if 'state' in sql_lower or 'tx' in sql_lower or 'ca' in sql_lower:
        elements["geo_filters"].append("state_filtering_used")
        
    if 'premium' in sql_lower and ('>' in sql_lower or '<' in sql_lower):
        elements["premium_filters"].append("premium_threshold_used")
        
    if 'group by' in sql_lower:
        elements["grouping_patterns"].append("grouping_applied")
    
    return elements

# ------------------------------- LOAD TABLES SCHEMA (UNCHANGED) -------------------------------------

# Load table descriptions (schema, table, and per-table details)
table_catalog = pd.read_excel(
    "hackathon_final_schema_file_v1.xlsx",
    sheet_name='Table_descriptions'
)[['DATABASE', 'SCHEMA', 'TABLE', 'Brief_Description', 'Detailed_Comments']]

# Load column descriptions (per-table, per-column, and EDA)
column_catalog_df = pd.read_excel(
    "hackathon_final_schema_file_v1.xlsx",
    sheet_name="Table's Column Summaries"
)[['Table Name', 'Feature Name', 'Data Type', 'Description', 'sample_100_distinct']]

SAMPLE_100_VALUE_FIELD = "sample_100_distinct"  

# -- Convert to list-of-dicts for uniform access --
all_tables = [
    {
        "database": row["DATABASE"],
        "schema": row["SCHEMA"],
        "table_name": row["TABLE"],
        "brief_description": row["Brief_Description"],
        "detailed_comments": row["Detailed_Comments"]
    }
    for idx, row in table_catalog.iterrows()
]

all_columns = [
    {
        "table_name": row["Table Name"],
        "column_name": row["Feature Name"],
        "data_type": row["Data Type"],
        "description": row["Description"],
        "sample_100_distinct": row[SAMPLE_100_VALUE_FIELD]
        
    }
    for idx, row in column_catalog_df.iterrows()
]

# ------------------ REVOLUTIONARY: GPT-POWERED JOIN EXTRACTION -------------------------

def extract_join_relationships_using_gpt(all_tables, all_columns, gpt_object):
    """
    This is our revolutionary approach: instead of using complex regex patterns to extract
    join information, we leverage GPT's natural language understanding to read brief 
    descriptions and identify table relationships.
    
    Think of this function as teaching GPT to be a skilled database analyst who can read
    documentation and immediately understand how different tables connect to each other.
    This approach is far more flexible and intelligent than pattern matching.
    
    The key insight: GPT excels at understanding context and extracting structured information
    from unstructured text. Brief descriptions are exactly the kind of natural language
    that GPT handles beautifully.
    
    Args:
        all_tables: Your table catalog with brief descriptions
        all_columns: Your column catalog for validation
        gpt_object: GPT API object for intelligent extraction
        
    Returns:
        Dictionary containing GPT-extracted join relationships with confidence scores
    """
    print("[GPT_JOIN_EXTRACT] Using GPT's natural language understanding to extract join relationships...")
    
    # First, let's prepare the context that GPT will analyze
    # We're building the same tables_context that you mentioned in your request
    tables_context = "\n".join(
        [f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}: {tbl['brief_description']}" for tbl in all_tables])
    
    # Create a quick lookup of available columns for validation
    # This helps GPT verify that suggested join keys actually exist
    available_columns_by_table = {}
    for col in all_columns:
        table_name = col['table_name']
        if table_name not in available_columns_by_table:
            available_columns_by_table[table_name] = []
        available_columns_by_table[table_name].append({
            'name': col['column_name'],
            'type': col['data_type'],
            'description': col['description']
        })
    
    # Build column context for GPT to understand what columns are available
    # This prevents GPT from suggesting join keys that don't exist
    column_context = ""
    for table_name, columns in available_columns_by_table.items():
        column_names = [col['name'] for col in columns[:10]]  # Limit to first 10 to manage prompt size
        column_context += f"\n{table_name}: Available columns include {', '.join(column_names)}"
    
    # Now we create a sophisticated prompt that teaches GPT to be a database relationship analyst
    # This prompt leverages GPT's strengths in natural language understanding and reasoning
    gpt_prompt = f"""
You are an expert database relationship analyst specializing in insurance domain systems.
Your task is to analyze table descriptions and extract concrete join relationships.

You excel at understanding natural language descriptions and identifying how different 
database entities connect to each other. Use your language understanding capabilities
to read between the lines and infer relationships even when they're not explicitly stated.

INSURANCE DOMAIN CONTEXT:
- Policy data is typically central, with other tables linking to it
- Common join patterns include POL_TX_ID, PKEY, KEY_POL_RSK_ITM_ID, POLICY_ID
- Auto insurance often separates policy, vehicle, and premium information
- CAT (catastrophic) insurance has specialized tables and relationships
- Claims data connects to policies through policy identifiers

TABLE DESCRIPTIONS TO ANALYZE:
{tables_context}

AVAILABLE COLUMNS FOR VALIDATION:
{column_context}

ANALYSIS INSTRUCTIONS:

STEP 1 - SEMANTIC UNDERSTANDING:
Read each table description carefully. Understand what business entity or process each table represents.
Look for explicit mentions of relationships like "joins with", "links to", "connected via".
Also infer implicit relationships based on business logic and naming patterns.

STEP 2 - JOIN KEY IDENTIFICATION:
For each relationship you identify, determine the specific column names used for joining.
Pay attention to mentions of specific field names in the descriptions.
Cross-reference with the available columns to ensure suggested join keys actually exist.
Consider common insurance domain patterns when keys aren't explicitly mentioned.

STEP 3 - CONFIDENCE ASSESSMENT:
Rate each relationship's confidence based on:
- HIGH: Explicitly mentioned joins with specific column names
- MEDIUM: Clear business relationship with likely join keys
- LOW: Inferred relationship that may need validation

STEP 4 - BUSINESS LOGIC VALIDATION:
Ensure that identified relationships make business sense in insurance context.
Policy tables should be central, with other entities linking to them.
Verify that join directions are logical (child tables join to parent tables).

OUTPUT REQUIREMENTS:
Provide a comprehensive JSON response with this structure:

{{
    "analysis_summary": {{
        "total_tables_analyzed": number,
        "relationships_identified": number,
        "high_confidence_joins": number,
        "domain_insights": "key insights about the table relationships"
    }},
    "table_join_keys": {{
        "TABLE_NAME": ["list", "of", "join", "keys", "for", "this", "table"],
        // ... for each table that has join capabilities
    }},
    "join_relationships": [
        {{
            "table_a": "TABLE_NAME_A",
            "table_b": "TABLE_NAME_B", 
            "join_key_a": "COLUMN_NAME_A",
            "join_key_b": "COLUMN_NAME_B",
            "relationship_type": "one-to-many|many-to-one|one-to-one",
            "confidence": "high|medium|low",
            "source_evidence": "quote or paraphrase from description that indicates this relationship",
            "business_logic": "explanation of why this relationship makes sense"
        }}
        // ... for each identified relationship
    ],
    "validation_notes": [
        "Any concerns or recommendations about the identified relationships"
    ]
}}

CRITICAL SUCCESS FACTORS:
1. Only suggest join keys that exist in the available columns list
2. Focus on relationships that make business sense in insurance domain
3. Provide clear evidence for each relationship you identify
4. Be conservative with confidence ratings - it's better to be cautious

Use your natural language understanding to extract maximum intelligence from these descriptions.
Respond with ONLY the JSON structure above - no additional text.
"""

    # Execute the GPT analysis using your existing API structure
    messages = [{"role": "user", "content": gpt_prompt}]
    payload = {
        "username": "GPT_JOIN_RELATIONSHIP_ANALYST",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.1,  # Low temperature for consistent analysis
        "max_tokens": 3072   # Larger token limit for comprehensive analysis
    }
    
    try:
        print("[GPT_JOIN_EXTRACT] Sending brief descriptions to GPT for intelligent analysis...")
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        content = resp.json()['choices'][0]['message']['content']
        
        # Parse GPT's structured response
        start = content.find('{')
        end = content.rfind('}') + 1
        if start == -1 or end == 0:
            raise ValueError("No valid JSON found in GPT response")
            
        parsed_response = json.loads(content[start:end])
        
        # Extract and validate the results
        analysis_summary = parsed_response.get('analysis_summary', {})
        table_join_keys = parsed_response.get('table_join_keys', {})
        join_relationships = parsed_response.get('join_relationships', [])
        validation_notes = parsed_response.get('validation_notes', [])
        
        # Log GPT's analysis insights
        print(f"[GPT_JOIN_EXTRACT] ‚úÖ GPT Analysis Complete:")
        print(f"  üìä Tables Analyzed: {analysis_summary.get('total_tables_analyzed', 'Unknown')}")
        print(f"  üîó Relationships Found: {analysis_summary.get('relationships_identified', 'Unknown')}")
        print(f"  üíé High Confidence: {analysis_summary.get('high_confidence_joins', 'Unknown')}")
        
        if analysis_summary.get('domain_insights'):
            print(f"  üß† Domain Insights: {analysis_summary['domain_insights']}")
        
        # Display some example relationships for debugging
        high_conf_relationships = [r for r in join_relationships if r.get('confidence') == 'high']
        if high_conf_relationships:
            print(f"[GPT_JOIN_EXTRACT] üü¢ Sample High Confidence Relationships:")
            for rel in high_conf_relationships[:3]:  # Show first 3
                print(f"    {rel['table_a']}.{rel['join_key_a']} = {rel['table_b']}.{rel['join_key_b']}")
                print(f"      Evidence: {rel.get('source_evidence', 'Not provided')[:80]}...")
        
        # Return structured results in the format expected by your system
        return {
            'table_join_keys': table_join_keys,
            'join_relationships': join_relationships,
            'available_columns': available_columns_by_table,
            'gpt_analysis': {
                'analysis_summary': analysis_summary,
                'validation_notes': validation_notes,
                'extraction_method': 'gpt_natural_language_understanding'
            }
        }
        
    except Exception as ex:
        print(f"[ERROR] GPT join extraction failed: {ex}")
        # Provide a fallback empty structure so the system can continue
        return {
            'table_join_keys': {},
            'join_relationships': [],
            'available_columns': available_columns_by_table,
            'gpt_analysis': {
                'error': str(ex),
                'extraction_method': 'failed_gpt_extraction'
            }
        }

def build_gpt_extracted_join_context(join_info, relevant_tables=None):
    """
    Transform GPT's extracted join analysis into clear guidance for schema selection.
    This function takes GPT's intelligent analysis and formats it in a way that 
    provides concrete, actionable guidance to downstream processes.
    
    Think of this as translating GPT's analysis into a format that's optimized
    for decision-making in your schema selection process.
    """
    join_relationships = join_info.get('join_relationships', [])
    gpt_analysis = join_info.get('gpt_analysis', {})
    
    # Filter to relevant tables if specified
    if relevant_tables:
        relevant_table_names = [t.split('.')[-1] for t in relevant_tables]
        relevant_joins = []
        for join in join_relationships:
            if (join['table_a'] in relevant_table_names or 
                join['table_b'] in relevant_table_names):
                relevant_joins.append(join)
        join_relationships = relevant_joins
    
    if not join_relationships:
        return "No GPT-extracted join relationships found for the selected tables."
    
    # Organize by confidence level as GPT determined them
    high_confidence = [j for j in join_relationships if j.get('confidence') == 'high']
    medium_confidence = [j for j in join_relationships if j.get('confidence') == 'medium']
    low_confidence = [j for j in join_relationships if j.get('confidence') == 'low']
    
    # Build comprehensive context string
    context_parts = ["GPT-EXTRACTED JOIN RELATIONSHIPS (from brief description analysis):"]
    
    # Add GPT's domain insights if available
    if gpt_analysis.get('analysis_summary', {}).get('domain_insights'):
        context_parts.append(f"\nüß† GPT DOMAIN INSIGHTS:")
        context_parts.append(f"   {gpt_analysis['analysis_summary']['domain_insights']}")
    
    if high_confidence:
        context_parts.append("\nüü¢ HIGH CONFIDENCE JOINS (GPT identified with strong evidence):")
        for join in high_confidence:
            evidence = join.get('source_evidence', 'Direct evidence from description')[:60] + "..."
            context_parts.append(f"   {join['table_a']}.{join['join_key_a']} = {join['table_b']}.{join['join_key_b']}")
            context_parts.append(f"     üìù Evidence: {evidence}")
            if join.get('business_logic'):
                context_parts.append(f"     üíº Logic: {join['business_logic'][:60]}...")
    
    if medium_confidence:
        context_parts.append("\nüü° MEDIUM CONFIDENCE JOINS (GPT inferred from business logic):")
        for join in medium_confidence:
            context_parts.append(f"   {join['table_a']}.{join['join_key_a']} = {join['table_b']}.{join['join_key_b']}")
            if join.get('business_logic'):
                context_parts.append(f"     üíº Logic: {join['business_logic'][:60]}...")
    
    if low_confidence:
        context_parts.append("\nüü† LOW CONFIDENCE JOINS (use with caution):")
        for join in low_confidence:
            context_parts.append(f"   {join['table_a']}.{join['join_key_a']} = {join['table_b']}.{join['join_key_b']}")
    
    # Add GPT's validation notes if available
    validation_notes = gpt_analysis.get('validation_notes', [])
    if validation_notes:
        context_parts.append("\n‚ö†Ô∏è  GPT VALIDATION NOTES:")
        for note in validation_notes[:3]:  # Limit to first 3 notes
            context_parts.append(f"   ‚Ä¢ {note}")
    
    # Add usage guidance based on GPT's analysis
    context_parts.append("\nüìã USAGE GUIDANCE (based on GPT analysis):")
    context_parts.append("‚Ä¢ Prefer high confidence joins - GPT found strong evidence in descriptions")
    context_parts.append("‚Ä¢ Medium confidence joins are business-logic inferred - validate carefully")
    context_parts.append("‚Ä¢ All suggested keys have been validated against available columns")
    context_parts.append("‚Ä¢ GPT has applied insurance domain knowledge to relationship identification")
    
    return "\n".join(context_parts)

# ------------------ PROMPT UTILITIES (UNCHANGED) -------------------------

def build_column_context_block(relevant_columns, all_columns, max_sample_vals=15):
    """
    Return a string giving, for all columns-of-interest, precise metadata block for GPT prompt.
    Shows at most `max_sample_vals` sample values for each column.
    """
    blocks = []
    for rel in relevant_columns:
        tname, cname = rel["table_name"], rel["column_name"]
        col_row = next(
            (c for c in all_columns if c["table_name"] == tname.split('.')[-1] and c["column_name"] == cname),
            None
        )
        if not col_row:
            continue  # missing in catalog (shouldn't happen)

        # --- Process sample_100_distinct string to limit values ---
        raw_samples = col_row.get('sample_100_distinct', '')
        # Try to parse it either as comma separated, or from a string list
        if isinstance(raw_samples, str):
            # Remove outer brackets if present (e.g. "[a, b, c]")
            s = raw_samples.strip()
            if s.startswith("[") and s.endswith("]"):
                s = s[1:-1]
            # Now, split
            vals = [v.strip("'\" ") for v in s.split(",") if v.strip().strip("'\"")]
        elif isinstance(raw_samples, list):
            vals = raw_samples
        else:
            vals = []

        # Limit the number of sample values displayed
        limited_vals = vals[:max_sample_vals]
        if len(vals) > max_sample_vals:
            limited_vals.append("...")  # Indicate truncation

        block = (
            f"\n ||| Table_name: {tname} "
            f"|| Feature_name: {col_row['column_name']} "
            f"|| Data_Type: {col_row['data_type']} "
            f"|| Description: {col_row['description']} "
            f"|| Sample Values (separated by ,): {', '.join(limited_vals)} ||| \n"
        )
        blocks.append(block)
    return "".join(blocks)

# ------------------ GPT-POWERED SCHEMA SELECTION --------------------------

def extract_unique_joins(columns):
    """
    Given a list of column dicts, extract unique {table_name, join_key} pairs.
    """
    join_pairs = set()
    for col in columns:
        # Safely handle 'JOIN_KEY' possibly missing in fallback scenario
        join_key = col.get('JOIN_KEY', None)
        table_name = col.get('table_name', None)
        if join_key and table_name:
            join_pairs.add( (table_name, join_key) )
    # Convert set of tuples to list of dicts if preferred
    return [ {"table_name": t, "join_key": k} for (t,k) in sorted(join_pairs) ]

def match_query_to_schema(user_question, all_tables, all_columns, gpt_object=None, 
                         user_context=None, intent_info=None):
    """
    REVOLUTIONIZED schema selection using GPT's natural language understanding
    to extract join relationships from brief descriptions.
    
    This represents a fundamental shift from pattern-matching approaches to leveraging
    the core strength of language models: understanding natural language and extracting
    structured intelligence from unstructured text.
    
    The key breakthrough: instead of teaching our system complex rules about how
    join information might be written, we let GPT use its natural language understanding
    to read your documentation and extract the relationships intelligently.
    
    Think of this as the difference between programming a robot to recognize faces
    by writing rules about nose shapes and eye distances, versus training it to 
    understand faces the way humans do - through pattern recognition and contextual understanding.
    """
    uq = user_question.lower()
    matched_tables = []
    matched_columns = []
    print(f"[SCHEMA] Processing query with GPT-powered join extraction: {user_question}")

    # --- GPT-POWERED APPROACH: NATURAL LANGUAGE UNDERSTANDING ---
    if gpt_object is not None:
        # Build the standard contexts as before
        tables_context = "\n".join(
            [f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}: {tbl['brief_description']}" for tbl in all_tables])
        columns_context = "\n".join(
            [f"{col['table_name']}.{col['column_name']}: {col['description']}" for col in all_columns])
        
        # üöÄ THE BREAKTHROUGH: Use GPT to intelligently extract join relationships
        # This leverages what GPT does best - understanding natural language narratives
        print("[SCHEMA] üß† Leveraging GPT's natural language understanding for join extraction...")
        gpt_join_info = extract_join_relationships_using_gpt(all_tables, all_columns, gpt_object)
        
        # Transform GPT's analysis into actionable guidance
        gpt_join_context = build_gpt_extracted_join_context(gpt_join_info)
        
        # Enhanced context building using intent information
        context_text = ""
        domain_hints = ""
        
        if intent_info and intent_info.get("is_followup"):
            context_type = intent_info.get("context_type", "")
            inherited_elements = intent_info.get("inherited_elements", {})
            
            context_text = f"\nPREVIOUS QUERY CONTEXT:\n"
            context_text += f"Context Type: {context_type}\n"
            
            if user_context:
                context_text += f"Context Details: {user_context}\n"
            
            # Provide specific guidance based on context type
            if context_type == "FILTER_MODIFICATION":
                domain_hints = "\n[GUIDANCE] This is a filter modification - preserve table relationships but adjust filter conditions.\n"
            elif context_type == "ENTITY_EXPANSION":
                domain_hints = "\n[GUIDANCE] This expands the entity scope - maintain similar table structure but consider additional entity tables.\n"
            elif context_type == "METRIC_CHANGE":
                domain_hints = "\n[GUIDANCE] This changes metrics/calculations - focus on tables that support the new metric requirements.\n"
            
            # Add specific inherited element guidance
            if inherited_elements.get("preserve_time_filters"):
                domain_hints += "[HINT] Preserve time-based filtering capabilities from previous query.\n"
            if inherited_elements.get("preserve_geographic_filters"):
                domain_hints += "[HINT] Maintain geographic filtering capabilities.\n"
        
        # üéØ REVOLUTIONARY PROMPT: Leveraging GPT's join intelligence
        gpt_prompt = f"""
You are an expert insurance database architect with access to intelligent join relationship
analysis performed by GPT on your table brief descriptions. You no longer need to guess
how tables connect - you have GPT's natural language understanding working for you.

CRITICAL DOMAIN KNOWLEDGE:
- Schema 'RAW_CI_CAT_ANALYSIS': Contains catastrophic insurance policies, CAT events, CAT-specific coverage limits
- Schema 'RAW_CI_INFORCE': Contains active policies, auto vehicle details, standard premium information
- Schema 'RAW_CI_CLAIMS': Contains claims data, settlements, adjustments

{context_text}

{domain_hints}

USER QUESTION: {user_question}

AVAILABLE SCHEMA:
Tables and Descriptions: {tables_context}

{gpt_join_context}

Available Columns: {columns_context}

ENHANCED REASONING WITH GPT JOIN INTELLIGENCE:

STEP 1 - DOMAIN CLASSIFICATION:
Classify the query type based on insurance domain:
- AUTO: Auto insurance policies and vehicles (use RAW_CI_INFORCE schema primarily)
- CAT: Catastrophic events and policies (use RAW_CI_CAT_ANALYSIS schema)
- CLAIMS: Claims processing and settlements (use RAW_CI_CLAIMS schema)  
- GENERAL: Cross-domain analysis (may need multiple schemas)

STEP 2 - INTELLIGENT TABLE SELECTION WITH GPT JOIN ANALYSIS:
Based on domain classification and GPT's relationship analysis:
- Start with the primary table that contains the main entity (policies, claims, vehicles)
- Add only tables that provide additional required attributes
- Use GPT's extracted join relationships to ensure selected tables can be properly connected
- Prefer tables that GPT identified with high confidence join relationships
- Consider context preservation requirements if this is a follow-up query

STEP 3 - GPT-GUIDED JOIN PATH SELECTION:
Using GPT's intelligent relationship extraction:
- Select only join relationships that GPT identified from brief description analysis
- Strongly prefer HIGH CONFIDENCE joins (üü¢) - GPT found strong evidence
- Use MEDIUM CONFIDENCE joins (üü°) with validation - GPT inferred from business logic
- Avoid LOW CONFIDENCE joins unless absolutely necessary
- Trust GPT's business logic reasoning about why relationships exist

STEP 4 - PRECISE COLUMN SELECTION:
Select only columns that directly answer the question:
- For filtering: columns used in WHERE clauses
- For display: columns requested in output  
- For grouping: columns used for aggregation
- For calculation: columns needed for mathematical operations

ENHANCED VALIDATION RULES WITH GPT INTELLIGENCE:
1. Every selected table MUST be connected through GPT-extracted join relationships
2. Use ONLY the exact join keys that GPT identified and validated
3. Prioritize relationships where GPT provided strong evidence from descriptions
4. Trust GPT's domain knowledge application in relationship identification
5. Double-check that column names match exactly (GPT has already validated existence)
6. If context guidance suggests preserving relationships, prioritize those patterns
7. LEVERAGE: GPT's natural language understanding has eliminated guesswork

OUTPUT REQUIREMENTS:
Provide your response as valid JSON with this exact structure:

{{
    "domain_classification": "AUTO|CAT|CLAIMS|GENERAL",
    "context_application": "How you applied the context guidance to your selection",
    "gpt_join_utilization": "How you leveraged GPT's relationship analysis",
    "reasoning_steps": {{
        "domain_rationale": "Why you classified the query this way",
        "table_selection_logic": "Why these specific tables using GPT's relationship insights",
        "join_path_explanation": "Specific GPT-extracted joins used and their confidence levels",
        "column_selection_rationale": "Why these specific columns answer the question"
    }},
    "relevant_tables_joins": [
        {{
            "table_name": "SCHEMA.TABLE_NAME",
            "join_keys": ["KEY1", "KEY2"],
            "selection_reason": "Why this table is essential",
            "gpt_join_evidence": ["GPT evidence that supports including this table"]
        }}
    ],
    "relevant_columns": [
        {{
            "table_name": "TABLE_NAME", 
            "column_name": "COLUMN_NAME",
            "usage_purpose": "filtering|display|grouping|calculation"
        }}
    ]
}}

CRITICAL SUCCESS FACTOR: Leverage GPT's natural language understanding of your brief descriptions.
This represents the evolution from rule-based to intelligence-based schema selection.

Respond with ONLY valid JSON. No additional text or explanations outside the JSON structure.
        """
        
        messages = [{"role": "user", "content": gpt_prompt}]
        payload = {
            "username": "GPT_INTELLIGENT_SCHEMA_AGENT",
            "session_id": "1",
            "messages": messages,
            "temperature": 0.001,  # Very low temperature for consistent schema selection
            "max_tokens": 2048
        }
        try:
            resp = gpt_object.get_gpt_response_non_streaming(payload)
            content = resp.json()['choices'][0]['message']['content']
            first = content.find('{')
            last = content.rfind('}') + 1
            parsed = json.loads(content[first:last])
            
            # Extract results with GPT join intelligence validation
            if "relevant_tables_joins" in parsed:
                matched_tables_joins = parsed["relevant_tables_joins"]
                # Enhanced validation using GPT's extracted join relationships
                validated_tables_joins = []
                for table_info in matched_tables_joins:
                    table_name = table_info.get("table_name", "")
                    join_keys = table_info.get("join_keys", [])
                    
                    # Validate against GPT's extracted join information
                    table_name_clean = table_name.split('.')[-1]
                    if table_name_clean in gpt_join_info['table_join_keys']:
                        gpt_available_keys = gpt_join_info['table_join_keys'][table_name_clean]
                        valid_keys = [key for key in join_keys if key in gpt_available_keys]
                        if valid_keys:
                            validated_table = table_info.copy()
                            validated_table["join_keys"] = valid_keys
                            validated_tables_joins.append(validated_table)
                            print(f"[SCHEMA] ‚úÖ GPT-validated table {table_name_clean} with keys {valid_keys}")
                        else:
                            print(f"[SCHEMA] ‚ùå No GPT-extracted join keys found for table {table_name_clean}")
                    else:
                        print(f"[SCHEMA] ‚ö†Ô∏è  Table {table_name_clean} not in GPT's relationship analysis")
                
                matched_tables_joins = validated_tables_joins
            else:
                matched_tables_joins = []
                
            matched_columns = parsed.get("relevant_columns", [])
            
            # Print enhanced debugging information showing GPT intelligence utilization
            print(f"[SCHEMA] Domain Classification: {parsed.get('domain_classification', 'Unknown')}")
            if parsed.get('context_application'):
                print(f"[SCHEMA] Context Applied: {parsed['context_application']}")
            if parsed.get('gpt_join_utilization'):
                print(f"[SCHEMA] üß† GPT Intelligence Used: {parsed['gpt_join_utilization']}")
            
            if "reasoning_steps" in parsed:
                reasoning = parsed["reasoning_steps"]
                print(f"[SCHEMA] Table Selection Logic: {reasoning.get('table_selection_logic', 'Not provided')}")
                print(f"[SCHEMA] Join Path: {reasoning.get('join_path_explanation', 'Not provided')}")
                
        except Exception as ex:
            print(f"[WARN] GPT-powered schema selection error: {ex}")
            matched_tables_joins = []
            matched_columns = []

    # --- Enhanced fallback that still uses GPT join extraction ---
    if not matched_tables_joins and not matched_columns:
        print("[SCHEMA] Falling back to keyword matching with GPT join enhancement")
        
        # Even in fallback, we can leverage GPT's join extraction
        if gpt_object:
            gpt_join_info = extract_join_relationships_using_gpt(all_tables, all_columns, gpt_object)
        else:
            gpt_join_info = {'table_join_keys': {}, 'join_relationships': []}
        
        keywords = set(uq.replace(",", " ").replace("_", " ").split())
        # Find relevant tables
        matched_table_objs = [
            tbl
            for tbl in all_tables
            if any(k in (tbl['table_name'].lower() + " " +
                         (str(tbl['brief_description']) or "")).lower()
                   for k in keywords)
        ]
        matched_tables_joins = [
            {
                "table_name": f"{tbl['database']}.{tbl['schema']}.{tbl['table_name']}",
                # Use GPT-extracted join keys even in fallback
                "join_keys": gpt_join_info['table_join_keys'].get(tbl['table_name'], [])
            }
            for tbl in matched_table_objs
        ]
        matched_columns = [
            {"table_name": col['table_name'], "column_name": col['column_name']}
            for col in all_columns
            if any(k in (col['column_name'] + " " +
                         (str(col['description']) or "")).lower()
                   for k in keywords)
        ]

    # Generate matched_joins using GPT's intelligent extraction
    matched_joins = []
    seen_pairs = set()
    
    # First, try to get joins from our enhanced table information
    for t in matched_tables_joins:
        tname = t.get("table_name") or t
        join_keys = t.get("join_keys", [])
        if isinstance(join_keys, list):
            for key in join_keys:
                if (tname, key) not in seen_pairs:
                    matched_joins.append({"table_name": tname, "join_key": key})
                    seen_pairs.add((tname, key))
    
    # If we need more joins and have GPT analysis, use those relationships
    if len(matched_joins) < 2 and len(matched_tables_joins) > 1:
        # Get GPT join info if we don't already have it
        if 'gpt_join_info' not in locals() and gpt_object:
            gpt_join_info = extract_join_relationships_using_gpt(all_tables, all_columns, gpt_object)
        elif 'gpt_join_info' not in locals():
            gpt_join_info = {'join_relationships': []}
        
        table_names = [t.get("table_name", "").split('.')[-1] for t in matched_tables_joins]
        for join_rel in gpt_join_info.get('join_relationships', []):
            if join_rel['table_a'] in table_names and join_rel['table_b'] in table_names:
                # Prefer high confidence joins from GPT
                if join_rel.get('confidence') in ['high', 'medium']:
                    if (join_rel['table_a'], join_rel['join_key_a']) not in seen_pairs:
                        matched_joins.append({
                            "table_name": join_rel['table_a'], 
                            "join_key": join_rel['join_key_a']
                        })
                        seen_pairs.add((join_rel['table_a'], join_rel['join_key_a']))
                    if (join_rel['table_b'], join_rel['join_key_b']) not in seen_pairs:
                        matched_joins.append({
                            "table_name": join_rel['table_b'], 
                            "join_key": join_rel['join_key_b']
                        })
                        seen_pairs.add((join_rel['table_b'], join_rel['join_key_b']))
    
    # Final fallback to original logic if still no joins
    if not matched_joins:
        matched_joins = extract_unique_joins(matched_columns)

    print(f"[SCHEMA] üéØ GPT-powered schema selection complete: {len(matched_tables_joins)} tables, {len(matched_columns)} columns, {len(matched_joins)} join points")
    return matched_tables_joins, matched_columns, matched_joins

# ------------------ ENHANCED FIRST TOOL CALL --------------------------

def first_tool_call(state):
    """
    Enhanced Node 1: Schema selection powered by GPT's natural language understanding
    of your brief descriptions. This represents the evolution from rule-based to 
    intelligence-based database relationship discovery.
    """
    user_question = state.get("user_question")
    all_tables    = state.get("table_schema")
    all_columns   = state.get("columns_info")
    gpt_object    = state.get("gpt_object", None)

    # Enhanced: get both legacy and new intent context
    user_context = state.get("user_context", None)
    intent_info = state.get("intent_info", None)
    
    print(f"[TOOL1] Processing schema selection with GPT-powered natural language understanding")
    
    # Call our revolutionized schema selection that leverages GPT's language capabilities
    relevant_tables_joins, relevant_columns, relevant_joins = match_query_to_schema(
        user_question, all_tables, all_columns, gpt_object, 
        user_context=user_context, intent_info=intent_info)
    
    state["relevant_tables_joins"] = relevant_tables_joins
    state["relevant_columns"] = relevant_columns
    state["relevant_joins"] = relevant_joins
    
    print(f"[TOOL1] üéØ GPT-powered schema selection complete: {len(relevant_tables_joins)} tables identified")
    return state

# -------------------- SQL GENERATION AGENT (ENHANCED FOR GPT JOIN INTELLIGENCE) ----------------------

def query_gen_node(state):
    """
    Enhanced Node 2: SQL generation leveraging GPT's intelligent join relationship analysis.
    The SQL generator now benefits from GPT's natural language understanding of your
    table relationships, leading to more accurate and reliable query construction.
    """
    user_question = state.get("user_question")
    all_columns = state.get("columns_info")
    relevant_columns = state.get("relevant_columns")
    relevant_joins=state.get("relevant_tables_joins")
    gpt_object = state.get("gpt_object")
    intent_info = state.get("intent_info", {})

    # Build context for the question
    context_block = build_column_context_block(relevant_columns, all_columns, 15)
    print(f"[TOOL2] Generating SQL with {len(relevant_columns)} selected columns and GPT-analyzed joins")

    # Get GPT's join intelligence for SQL generation context
    all_tables = state.get("table_schema", [])
    if gpt_object:
        gpt_join_info = extract_join_relationships_using_gpt(all_tables, all_columns, gpt_object)
        selected_table_names = [t.get("table_name", "").split('.')[-1] for t in relevant_joins]
        sql_join_context = build_gpt_extracted_join_context(gpt_join_info, selected_table_names)
    else:
        sql_join_context = "GPT join analysis not available - using fallback approach"

    # Enhanced context building for SQL generation
    context_guidance = ""
    if intent_info.get("is_followup"):
        context_type = intent_info.get("context_type", "")
        inherited_elements = intent_info.get("inherited_elements", {})
        
        context_guidance = f"\nCONTEXT GUIDANCE FOR SQL GENERATION:\n"
        context_guidance += f"Query Type: {context_type}\n"
        
        if context_type == "FILTER_MODIFICATION":
            context_guidance += "- This modifies filters from a previous query - maintain similar structure but adjust WHERE conditions\n"
        elif context_type == "ENTITY_EXPANSION":
            context_guidance += "- This expands the scope of entities - consider similar grouping and aggregation patterns\n"
        elif context_type == "METRIC_CHANGE":
            context_guidance += "- This changes metrics/calculations - focus on new aggregation requirements\n"
        
        if inherited_elements.get("modify_premium_threshold"):
            context_guidance += f"- Premium threshold guidance: {inherited_elements['modify_premium_threshold']}\n"

    with open('few_shot_examples.txt', 'r', encoding='utf-8') as f:
        few_shot_examples = f.read()

    # Enhanced SQL generation prompt leveraging GPT's join intelligence
    gpt_prompt = f"""
You are a strict SQL query generator specialized in insurance analytics on Snowflake.
You have access to GPT's intelligent analysis of table relationships extracted from brief descriptions.
This represents the evolution from guesswork to intelligence-based join selection.

{context_guidance}

**User Question:**
```{user_question}```

**GPT-Analyzed Join Relationships for Selected Tables:**
{sql_join_context}

**Relevant Tables and Join_Keys:**
```{relevant_joins}```

**Relevant Columns and Tables with Sample Values:**
```{context_block}```

ENHANCED SQL GENERATION WITH GPT JOIN INTELLIGENCE:

Step 1 - PARSE USER REQUEST WITH CONTEXT
‚Ä¢ Identify all metrics, filters, and aggregation needs from the USER_REQUEST
‚Ä¢ Apply any context guidance for follow-up queries
‚Ä¢ For every filter or metric: Map explicitly to a (TABLE.COLUMN)
‚Ä¢ If not available, mark as "N/A: Not found", and DO NOT use or guess in SQL

Step 2 - LEVERAGE GPT'S INTELLIGENT JOIN ANALYSIS  
‚Ä¢ Use ONLY the join relationships that GPT extracted through natural language understanding
‚Ä¢ Determine the minimal set of tables needed to supply all required columns
‚Ä¢ Connect tables using GPT's analyzed relationships, prioritizing high confidence joins
‚Ä¢ Record each join using exact syntax: <TABLE1> JOIN <TABLE2> ON <TABLE1>.<KEY> = <TABLE2>.<KEY>
‚Ä¢ Trust GPT's business logic reasoning about why relationships exist

Step 3 - PREPARE DATE CONVERSIONS (SNOWFLAKE-SPECIFIC)
‚Ä¢ For VARCHAR date columns: Use TRY_TO_DATE(<date_field>, 'YYYY-MM-DD')
‚Ä¢ For DATE/TIMESTAMP columns: Use as-is or CAST(<date_field> AS DATE) if needed
‚Ä¢ Never apply TRY_TO_DATE to columns already in DATE/TIMESTAMP format
‚Ä¢ Apply filters using appropriate format for the column type

Step 4 - BUILD OPTIMIZED SQL WITH GPT-VALIDATED JOINS
‚Ä¢ SELECT: Choose mapped fields and aggregations
‚Ä¢ FROM: Use the primary table from GPT's relationship analysis
‚Ä¢ JOIN: Add ONLY the GPT-analyzed join relationships with their confidence levels
‚Ä¢ WHERE: Add mapped, available filters with proper data type handling
‚Ä¢ GROUP BY: Add if user aggregation/grouping is requested and columns are mapped
‚Ä¢ ORDER BY: If required in user request
‚Ä¢ INTELLIGENCE: Leverage GPT's domain knowledge application in relationship selection

Step 5 - COMPREHENSIVE VALIDATION WITH GPT INTELLIGENCE  
‚ñ° Only listed tables/columns used  
‚ñ° All join steps use GPT-analyzed relationships‚Äîno invented joins  
‚ñ° All date logic follows Snowflake conventions  
‚ñ° No columns/filters/joins/objects invented or guessed  
‚ñ° All SQL clauses pass previous mapping and join plan checks
‚ñ° Context guidance properly applied if this is a follow-up query
‚ñ° CRITICAL: All joins validated by GPT's natural language understanding

**Enhanced Output Structure:**
**Mapping:**  
(User requirement ‚Üí Actual DB column mapping. If not found, do not use as filter.)

**GPT Join Intelligence:**
(How GPT's relationship analysis was applied to join selection and validation.)

**Reasoning:**  
(Explain: 1. how context guidance was applied, 2. GPT-guided join strategy, 3. column/filter selection logic, 4. optimization decisions. Max 150 words.)

**SQL Query:**  
(Production-ready SQL using GPT-validated mappings and intelligent joins.)

****FEW-SHOT EXAMPLES:****
{few_shot_examples}
****END EXAMPLES****

From the above, provide valid JSON:
{{
    "GPT Join Intelligence": "how GPT's relationship analysis guided join selection",
    "Reasoning": "detailed explanation including context and GPT intelligence application",
    "SQL Query": "complete optimized SQL query"
}}

Strictly output JSON only. Leverage GPT's natural language understanding for maximum accuracy.
"""
    
    messages = [{"role": "user", "content": gpt_prompt}]
    payload = {
        "username": "GPT_INTELLIGENT_SQL_AGENT",
        "session_id": "1",
        "messages": messages,
        "temperature": 0.001,
        "max_tokens": 2048
    }
    matched_sql_query = []
    matched_reasoning = []

    try:
        resp = gpt_object.get_gpt_response_non_streaming(payload)
        gpt_output = resp.json()['choices'][0]['message']['content']

        first = gpt_output.find('{')
        last = gpt_output.rfind('}')+1
        parsed = json.loads(gpt_output[first:last])
        matched_reasoning = parsed.get("Reasoning", [])
        matched_sql_query = parsed.get("SQL Query", [])
        gpt_join_intelligence = parsed.get("GPT Join Intelligence", "")
        
        print(f"[TOOL2] üéØ SQL generation complete with GPT join intelligence")
        if gpt_join_intelligence:
            print(f"[TOOL2] üß† GPT Intelligence Applied: {gpt_join_intelligence}")
        
    except Exception as ex:
        print(f"[WARN] GPT-powered SQL generation error: {ex}")
        
    state["query_llm_prompt"] = gpt_prompt
    state["query_llm_result"] = gpt_output
    state["query_reasoning"] = matched_reasoning
    state["query_sql"] = matched_sql_query
    return state

# ------------------ SNOWFLAKE SQL EXECUTION (UNCHANGED) --------------------

def run_query_and_return_df(state):
    """
    Executes the SQL query found in state['query_sql'] on Snowflake and returns DataFrame result.
    Updates state['snowflake_result'] with a pandas DataFrame (or None on error).
    """
    query_sql = state.get("query_sql", None)
    if not query_sql:
        print("[ERROR] No SQL found in state['query_sql']")
        state['snowflake_result'] = None
        return state
    
    # Connect to Snowflake
    conn = None
    try:
        print(f"[EXEC] Executing SQL on Snowflake...")
        conn = create_snowflake_connection()
        with conn.cursor() as cursor:
            cursor.execute(query_sql)
            # Fetch all rows and columns
            rows = cursor.fetchall()
            colnames = [d[0] for d in cursor.description]
            df = pd.DataFrame(rows, columns=colnames)
        state['snowflake_result'] = df
        print(f"[EXEC] ‚úÖ Query executed successfully, returned {len(df)} rows")
    except Exception as ex:
        print(f"[ERROR] ‚ùå Snowflake query execution failed: {ex}")
        state['snowflake_result'] = None
    finally:
        if conn is not None:
            conn.close()
    return state
    
# ----------- COMPLETE PIPELINE WITH GPT NATURAL LANGUAGE JOIN INTELLIGENCE -----------

if __name__ == "__main__":
    print("=== üß† GPT-Powered Natural Language Join Intelligence Pipeline ===")
    print("This revolutionary pipeline leverages GPT's natural language understanding")
    print("to extract join relationships directly from your brief descriptions,")
    print("representing the evolution from rule-based to intelligence-based database analysis.\n")
    
    # Example agent history - this simulates previous interactions
    agent_history = [{
        "story": """This chart visualizes auto policies with premium over 10000 in Texas from year 2018 onwards, grouped by year""",
        "reasoning": """The tables INFORCE_DATA_FINAL, AUTO_VEHICLE_PREMIUM_DETAIL, and AUTO_VEHICLE_LEVEL_DATA are joined using POL_TX_ID and KEY_POL_RSK_ITM_ID. The query filters policies with premiums over 10000 in Texas from 2018 onwards. The EFFECTIVEDATE is converted to a date format for filtering. The results are grouped by year.""",
        "sql_query": """SELECT YEAR(TRY_TO_DATE(idf.EFFECTIVEDATE, 'YYYY-MM-DD')) AS policy_year, COUNT(*) AS policy_count FROM INFORCE_DATA_FINAL idf JOIN AUTO_VEHICLE_LEVEL_DATA avld ON idf.POL_TX_ID = avld.POL_TX_ID JOIN AUTO_VEHICLE_PREMIUM_DETAIL avpd ON avld.KEY_POL_RSK_ITM_ID = avpd.KEY_POL_RSK_ITM_ID WHERE avpd.ITM_TERM_PRM_AMT > 10000 AND idf."INSURED STATE" = 'TX' AND TRY_TO_DATE(idf.EFFECTIVEDATE, 'YYYY-MM-DD') >= '2018-01-01' GROUP BY policy_year;""",
        "charts": [
            {
                "title": "Auto policies with premium over 10000 in Texas from year 2018 onwards, grouped by year",
                "type": "bar",
                "chart_code": "plt.bar(data['policy_year'], data['policy_count'])",
                "dataframe": {
                    'policy_year': [2019, 2020, 2021, 2022, 2023],
                    'policy_count': [1500, 1750, 2000, 2250, 2500]
                }
            }
        ]
    }] 
    
    # Current user question - this is a follow-up that modifies the premium threshold
    user_question = "What about policies with premium over 50000 across all United States?"

    print(f"üîç Step 1: Enhanced Intent Classification")
    intent_info = classify_intent_and_context_gpt(user_question, agent_history, gpt_object)

    print(f"\nüèóÔ∏è  Step 2: Building GPT Intelligence-Powered Pipeline State")
    # Initialize the revolutionary pipeline state
    state = {
        "user_question": user_question,
        "table_schema": all_tables,
        "columns_info": all_columns,
        "gpt_object": gpt_object,
        "intent_info": intent_info
    }

    # Add legacy context for backward compatibility
    if intent_info["is_followup"] and intent_info["user_context"]:
        state["user_context"] = intent_info["user_context"]

    print(f"\nüß† Step 3: GPT Natural Language Join Intelligence Schema Selection")
    state = first_tool_call(state)
    print(f"Selected tables: {[t.get('table_name', 'Unknown') for t in state['relevant_tables_joins']]}")

    print(f"\n‚ö° Step 4: SQL Generation with GPT Join Intelligence")
    state = query_gen_node(state)
    
    print(f"\nüìä Step 5: Query Execution")
    state = run_query_and_return_df(state)

    # Display comprehensive results showcasing the GPT intelligence benefits
    print(f"\n" + "="*80)
    print(f"üéØ GPT NATURAL LANGUAGE JOIN INTELLIGENCE RESULTS")
    print(f"="*80)
    
    print(f"\nüß† REASONING (Enhanced with GPT Intelligence):")
    print(state["query_reasoning"])
    
    print(f"\nüíæ GENERATED SQL (Using GPT-Extracted Joins):")
    print(state["query_sql"])
    
    print(f"\nüìà EXECUTION RESULTS:")
    if isinstance(state['snowflake_result'], pd.DataFrame):
        print(f"‚úÖ Query returned {len(state['snowflake_result'])} rows")
        print(state['snowflake_result'].head())
    else:
        print("‚ùå Query execution failed or returned no results")
    
    print(f"\nüéâ GPT-powered natural language join intelligence pipeline completed!")
    print("üîë Revolutionary Enhancement: GPT's natural language understanding extracts joins")
    print("üìä Result: Intelligence-based rather than rule-based database relationship discovery")
    print("üöÄ Impact: Dramatic improvement in accuracy and reliability of schema selection")
```
