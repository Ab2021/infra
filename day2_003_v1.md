# Day 2 - Part 3a: Inverted Index Fundamentals and Architecture

## Table of Contents
1. [Introduction to Document Indexing](#introduction-to-document-indexing)
2. [The Inverted Index Concept](#inverted-index-concept)
3. [Index Architecture Components](#index-architecture-components)
4. [Forward vs Inverted Index](#forward-vs-inverted-index)
5. [Posting Lists Structure](#posting-lists-structure)
6. [Term Dictionary Organization](#term-dictionary-organization)
7. [Index Design Trade-offs](#index-design-trade-offs)
8. [Study Questions](#study-questions)
9. [Code Examples](#code-examples)

---

## Introduction to Document Indexing

Document indexing is the process of **organizing and structuring documents** to enable efficient retrieval. It transforms a collection of unstructured text documents into a **searchable data structure** that supports fast query processing.

### The Scale Challenge

#### **Modern Search Engine Scale**
- **Google**: Indexes over 130 trillion web pages
- **Elasticsearch**: Single clusters handle petabytes of data
- **Academic Search**: Microsoft Academic indexes 200+ million papers
- **Enterprise Search**: Corporate systems index millions of documents

#### **Performance Requirements**
**Query Latency**: Sub-second response times expected
- Google: Average query response < 0.2 seconds
- Enterprise search: < 1 second acceptable
- Real-time applications: < 100ms required

**Update Frequency**: Indexes must handle continuous updates
- Web crawling: Millions of pages updated daily
- Social media: Thousands of posts per second
- News feeds: Continuous content ingestion

**Concurrent Users**: Systems serve thousands of simultaneous queries
- Search engines: Millions of concurrent users
- Enterprise systems: Hundreds to thousands of users
- Database systems: High concurrency requirements

### The Naive Approach Problem

#### **Linear Search Limitations**
Without indexing, search requires **scanning every document**:

```python
def naive_search(query_terms, document_collection):
    results = []
    for doc in document_collection:
        if all(term in doc for term in query_terms):
            results.append(doc)
    return results
```

**Time Complexity**: O(N × M × L)
- N = number of documents
- M = average document length
- L = query length

**For Large Collections**:
```
1 million documents × 1000 words/doc × 3 query terms
= 3 billion term comparisons per query
```

This is computationally prohibitive for real-time search.

#### **Storage and Memory Issues**
**Memory Requirements**: Loading entire collection into memory
**I/O Bottlenecks**: Reading documents from disk for each query
**No Ranking**: Difficult to implement relevance scoring efficiently

---

## The Inverted Index Concept

The inverted index **reverses the natural document-term relationship**, creating a term-centric view that enables efficient retrieval.

### Core Concept

#### **Natural Document Structure**
```
Document 1: "machine learning algorithms"
Document 2: "deep learning networks"  
Document 3: "machine vision systems"
```

#### **Inverted Structure**
```
Term "machine" → [Document 1, Document 3]
Term "learning" → [Document 1, Document 2]
Term "algorithms" → [Document 1]
Term "deep" → [Document 2] 
Term "networks" → [Document 2]
Term "vision" → [Document 3]
Term "systems" → [Document 3]
```

### Theoretical Foundation

#### **Set Theory Perspective**
Each term defines a **set of documents** containing that term:
```
S_machine = {Doc1, Doc3}
S_learning = {Doc1, Doc2}
S_algorithms = {Doc1}
```

**Boolean Queries** become **set operations**:
```
Query: "machine AND learning"
Result: S_machine ∩ S_learning = {Doc1}

Query: "machine OR learning"  
Result: S_machine ∪ S_learning = {Doc1, Doc2, Doc3}
```

#### **Information Retrieval Perspective**
The inverted index supports **efficient term lookup**:
- **Term → Documents**: Direct access to documents containing a term
- **Frequency Information**: Store term frequencies for ranking
- **Position Information**: Support phrase queries and proximity

### Historical Context

#### **Library Science Origins**
**Book Indexes**: Back-of-book indexes map terms to page numbers
**Card Catalogs**: Subject cards group books by topic
**Concordances**: Biblical and literary concordances list word occurrences

#### **Computer Science Development**
**1960s**: Early IR systems (SMART, MEDLARS)
**1970s**: Salton's vector space model
**1990s**: Web search engines (AltaVista, Excite)
**2000s**: Google's PageRank + inverted index

---

## Index Architecture Components

A complete inverted index consists of several interconnected components working together to enable efficient search.

### High-Level Architecture

```
┌─────────────────┐    ┌──────────────────┐    ┌─────────────────┐
│   Term          │    │   Posting        │    │   Document      │
│   Dictionary    │───▶│   Lists          │    │   Store         │
│                 │    │                  │    │                 │
│ term₁ → ptr₁    │    │ doc₁, tf₁, pos₁  │    │ doc₁: metadata  │
│ term₂ → ptr₂    │    │ doc₃, tf₃, pos₃  │    │ doc₂: content   │
│ term₃ → ptr₃    │    │ doc₅, tf₅, pos₅  │    │ doc₃: URL       │
│ ...             │    │ ...              │    │ ...             │
└─────────────────┘    └──────────────────┘    └─────────────────┘
```

### Component Details

#### **1. Term Dictionary (Vocabulary)**
**Purpose**: Maps terms to posting list locations
**Structure**: Hash table, B+ tree, or trie
**Contents**: 
- Term string
- Document frequency (df)
- Pointer to posting list
- Term statistics (IDF, etc.)

**Example**:
```
Term: "machine"
├── Document Frequency: 1,234,567
├── Collection Frequency: 5,678,901
├── Posting List Pointer: 0x7F8B4C2A1000
└── IDF Score: 2.4567
```

#### **2. Posting Lists**
**Purpose**: Store document IDs and term information for each vocabulary term
**Structure**: Sorted arrays, linked lists, or compressed sequences
**Contents**:
- Document ID
- Term frequency in document
- Term positions (for phrase queries)
- Additional metadata

**Example**:
```
Term: "machine"
Posting List: [
    (doc_id: 15, tf: 3, positions: [12, 45, 78]),
    (doc_id: 127, tf: 1, positions: [5]),
    (doc_id: 892, tf: 2, positions: [23, 67]),
    ...
]
```

#### **3. Document Store**
**Purpose**: Store document content and metadata
**Structure**: Key-value store, database, or file system
**Contents**:
- Original document content
- Document metadata (title, URL, date)
- Document length statistics
- Link information (for web documents)

### Memory Hierarchy Considerations

#### **Tiered Storage Strategy**
```
┌─────────────────┐  ← Fastest Access
│ RAM (Term Dict) │    (Hash table lookup)
├─────────────────┤
│ SSD (Hot Lists) │    (Frequently accessed posting lists)
├─────────────────┤  
│ HDD (Cold Lists)│    (Infrequently accessed posting lists)
├─────────────────┤
│ Archive Storage │    (Historical/backup data)
└─────────────────┘  ← Largest Capacity
```

#### **Cache Optimization**
**Term Dictionary Caching**: Keep entire vocabulary in memory
**Posting List Caching**: Cache frequently accessed lists
**Query Result Caching**: Cache popular query results
**Document Caching**: Cache frequently accessed document content

---

## Forward vs Inverted Index

Understanding the differences between forward and inverted indexes clarifies why inverted indexes are preferred for text retrieval.

### Forward Index Structure

#### **Document-Centric Organization**
```
Document 1 → [term₁, term₂, term₃, ...]
Document 2 → [term₄, term₁, term₅, ...]
Document 3 → [term₂, term₆, term₁, ...]
```

#### **Forward Index Example**
```
Doc 1 (id: 15): ["machine", "learning", "algorithms", "data"]
Doc 2 (id: 127): ["deep", "learning", "neural", "networks"]  
Doc 3 (id: 892): ["machine", "vision", "computer", "systems"]
```

### Inverted Index Structure

#### **Term-Centric Organization**
```
Term₁ → [doc_id₁, doc_id₃, doc_id₅, ...]
Term₂ → [doc_id₂, doc_id₄, doc_id₆, ...]
Term₃ → [doc_id₁, doc_id₂, doc_id₇, ...]
```

#### **Inverted Index Example**
```
"machine" → [15, 892]
"learning" → [15, 127]
"algorithms" → [15]
"data" → [15]
"deep" → [127]
"neural" → [127]
"networks" → [127]
"vision" → [892]
"computer" → [892]
"systems" → [892]
```

### Comparison Analysis

#### **Query Processing Efficiency**

**Forward Index Query Processing**:
```python
def forward_index_search(query_terms, forward_index):
    results = []
    for doc_id, doc_terms in forward_index.items():
        if all(term in doc_terms for term in query_terms):
            results.append(doc_id)
    return results
```
**Time Complexity**: O(N × M) where N = documents, M = avg document length

**Inverted Index Query Processing**:
```python
def inverted_index_search(query_terms, inverted_index):
    posting_lists = [inverted_index[term] for term in query_terms]
    return intersect_posting_lists(posting_lists)
```
**Time Complexity**: O(L₁ + L₂ + ... + Lₙ) where Lᵢ = posting list length

#### **Space Complexity**
**Forward Index**: O(N × M) - stores each term occurrence
**Inverted Index**: O(V + T) where V = vocabulary size, T = total term occurrences

For typical collections: **V << N × M**, making inverted indexes more space-efficient.

#### **Update Complexity**
**Forward Index Updates**: O(1) - append to document's term list
**Inverted Index Updates**: O(K) where K = unique terms in new document

### When to Use Each

#### **Forward Index Use Cases**
- **Document Clustering**: Need all terms for a document
- **Classification**: Feature extraction from documents
- **Recommendation**: Document similarity calculations
- **Analytics**: Term frequency analysis per document

#### **Inverted Index Use Cases**
- **Search Engines**: Primary use case
- **Boolean Queries**: Set operations on posting lists
- **Ranked Retrieval**: TF-IDF, BM25 scoring
- **Phrase Queries**: Position-based matching

---

## Posting Lists Structure

Posting lists are the heart of the inverted index, storing detailed information about term occurrences in documents.

### Basic Posting List Structure

#### **Minimal Structure** (Boolean Search)
```
Term: "machine"
Posting List: [15, 127, 892, 1205, 2367, ...]
```

**Storage**: Sorted array of document IDs
**Operations**: Binary search, set intersection/union
**Space**: 4 bytes per document ID (32-bit integers)

#### **Extended Structure** (Ranked Retrieval)
```
Term: "machine"  
Posting List: [
    (doc_id: 15, tf: 3),
    (doc_id: 127, tf: 1), 
    (doc_id: 892, tf: 2),
    ...
]
```

**Additional Data**: Term frequency for TF-IDF/BM25 scoring
**Space**: 8 bytes per posting (doc_id + tf)

#### **Full Structure** (Phrase Queries)
```
Term: "machine"
Posting List: [
    (doc_id: 15, tf: 3, positions: [12, 45, 78]),
    (doc_id: 127, tf: 1, positions: [23]),
    (doc_id: 892, tf: 2, positions: [5, 67]),
    ...
]
```

**Position Information**: Enables phrase and proximity queries
**Space**: Variable size depending on term frequency

### Posting List Operations

#### **Intersection (AND Queries)**
```python
def intersect_posting_lists(list1, list2):
    """Merge-based intersection of sorted posting lists"""
    result = []
    i, j = 0, 0
    
    while i < len(list1) and j < len(list2):
        if list1[i].doc_id == list2[j].doc_id:
            # Combine posting information
            combined_posting = combine_postings(list1[i], list2[j])
            result.append(combined_posting)
            i += 1
            j += 1
        elif list1[i].doc_id < list2[j].doc_id:
            i += 1
        else:
            j += 1
    
    return result
```

**Optimization**: Process shortest list first to minimize comparisons

#### **Union (OR Queries)**
```python
def union_posting_lists(list1, list2):
    """Merge-based union of sorted posting lists"""
    result = []
    i, j = 0, 0
    
    while i < len(list1) and j < len(list2):
        if list1[i].doc_id == list2[j].doc_id:
            combined_posting = combine_postings(list1[i], list2[j])
            result.append(combined_posting)
            i += 1
            j += 1
        elif list1[i].doc_id < list2[j].doc_id:
            result.append(list1[i])
            i += 1
        else:
            result.append(list2[j])
            j += 1
    
    # Add remaining elements
    result.extend(list1[i:])
    result.extend(list2[j:])
    
    return result
```

#### **Skip Lists Optimization**
Add skip pointers to enable faster intersection:

```
Posting List: [2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]
Skip Pointers:     ↓        ↓         ↓         ↓
                   8       32       128       512
```

**Benefit**: Reduce comparisons from O(n) to O(√n) in many cases
**Trade-off**: Additional space for skip pointers

### Position-Based Posting Lists

#### **Position Information Storage**
```python
class PositionalPosting:
    def __init__(self, doc_id, positions):
        self.doc_id = doc_id
        self.tf = len(positions)
        self.positions = sorted(positions)  # Sort for efficient processing
    
    def supports_phrase(self, other_posting, max_distance=1):
        """Check if this posting can form a phrase with another"""
        for pos1 in self.positions:
            for pos2 in other_posting.positions:
                if abs(pos1 - pos2) <= max_distance:
                    return True
        return False
```

#### **Phrase Query Processing**
```python
def phrase_query(term_postings, phrase_distance=1):
    """Process phrase query using positional information"""
    if len(term_postings) < 2:
        return term_postings[0] if term_postings else []
    
    # Start with intersection of document IDs
    result_docs = intersect_document_ids(term_postings)
    
    # Filter by phrase constraints
    phrase_results = []
    for doc_id in result_docs:
        doc_postings = [tp.get_posting(doc_id) for tp in term_postings]
        
        if satisfies_phrase_constraint(doc_postings, phrase_distance):
            phrase_results.append(doc_id)
    
    return phrase_results
```

---

## Term Dictionary Organization

The term dictionary (vocabulary) provides fast access to posting lists and stores term-level statistics essential for ranking.

### Dictionary Structure Options

#### **Hash Table Implementation**
```python
class HashTermDictionary:
    def __init__(self, initial_size=1000000):
        self.table = [None] * initial_size
        self.size = initial_size
        self.count = 0
        
    def hash_function(self, term):
        """Simple hash function for demonstration"""
        return hash(term) % self.size
    
    def insert(self, term, posting_list_ptr):
        """Insert term with pointer to posting list"""
        index = self.hash_function(term)
        
        # Handle collisions with chaining
        if self.table[index] is None:
            self.table[index] = []
        
        # Check if term already exists
        for entry in self.table[index]:
            if entry['term'] == term:
                entry['posting_list_ptr'] = posting_list_ptr
                return
        
        # Add new term
        self.table[index].append({
            'term': term,
            'posting_list_ptr': posting_list_ptr,
            'document_frequency': 0,
            'collection_frequency': 0
        })
        self.count += 1
    
    def lookup(self, term):
        """Find term and return posting list pointer"""
        index = self.hash_function(term)
        
        if self.table[index] is None:
            return None
            
        for entry in self.table[index]:
            if entry['term'] == term:
                return entry
        
        return None
```

**Advantages**: O(1) average lookup time
**Disadvantages**: No lexicographic ordering, hash collisions

#### **B+ Tree Implementation**
```python
class BTreeTermDictionary:
    def __init__(self, degree=100):
        self.root = BTreeNode(is_leaf=True)
        self.degree = degree
    
    def insert(self, term, posting_list_ptr):
        """Insert term maintaining sorted order"""
        if self.root.is_full():
            # Split root and create new root
            new_root = BTreeNode(is_leaf=False)
            new_root.children.append(self.root)
            self.split_child(new_root, 0)
            self.root = new_root
        
        self.insert_non_full(self.root, term, posting_list_ptr)
    
    def search(self, term):
        """Search for term in B+ tree"""
        return self.search_node(self.root, term)
    
    def range_query(self, start_term, end_term):
        """Find all terms in lexicographic range"""
        results = []
        self.range_search(self.root, start_term, end_term, results)
        return results
```

**Advantages**: Sorted order, range queries, guaranteed O(log n) performance
**Disadvantages**: More complex implementation, higher memory overhead

#### **Trie Implementation**
```python
class TrieTermDictionary:
    def __init__(self):
        self.root = TrieNode()
    
    def insert(self, term, posting_list_ptr):
        """Insert term into trie"""
        node = self.root
        
        for char in term:
            if char not in node.children:
                node.children[char] = TrieNode()
            node = node.children[char]
        
        node.is_terminal = True
        node.posting_list_ptr = posting_list_ptr
    
    def search(self, term):
        """Search for exact term"""
        node = self.root
        
        for char in term:
            if char not in node.children:
                return None
            node = node.children[char]
        
        return node.posting_list_ptr if node.is_terminal else None
    
    def prefix_search(self, prefix):
        """Find all terms with given prefix"""
        node = self.root
        
        # Navigate to prefix node
        for char in prefix:
            if char not in node.children:
                return []
            node = node.children[char]
        
        # Collect all terms from this node
        terms = []
        self.collect_terms(node, prefix, terms)
        return terms
```

**Advantages**: Memory efficient for common prefixes, supports prefix queries
**Disadvantages**: Can be slower for exact lookups, complex for Unicode

### Dictionary Statistics

#### **Term-Level Statistics**
```python
class TermEntry:
    def __init__(self, term):
        self.term = term
        self.document_frequency = 0      # Number of documents containing term
        self.collection_frequency = 0    # Total occurrences in collection
        self.posting_list_ptr = None     # Pointer to posting list
        self.idf_score = 0.0            # Precomputed IDF
        self.first_seen = None          # Timestamp of first occurrence
        self.last_updated = None        # Timestamp of last update
    
    def update_statistics(self, new_df, new_cf, total_docs):
        """Update term statistics"""
        self.document_frequency = new_df
        self.collection_frequency = new_cf
        self.idf_score = math.log(total_docs / new_df) if new_df > 0 else 0
        self.last_updated = datetime.now()
```

#### **Collection-Level Statistics**
```python
class CollectionStatistics:
    def __init__(self):
        self.total_documents = 0
        self.total_terms = 0            # Total term occurrences
        self.unique_terms = 0           # Vocabulary size
        self.average_document_length = 0
        self.longest_document = 0
        self.shortest_document = float('inf')
    
    def update_from_document(self, doc_length):
        """Update statistics when adding a document"""
        self.total_documents += 1
        self.total_terms += doc_length
        self.average_document_length = self.total_terms / self.total_documents
        self.longest_document = max(self.longest_document, doc_length)
        self.shortest_document = min(self.shortest_document, doc_length)
```

---

## Index Design Trade-offs

Designing an inverted index involves numerous trade-offs between space, time, and functionality.

### Space vs Time Trade-offs

#### **Minimal Index** (Space Optimized)
```
Term → [doc_id₁, doc_id₂, doc_id₃, ...]
```
**Space**: ~4 bytes per posting
**Query Support**: Boolean queries only
**Scoring**: Not supported

#### **Frequency Index** (Ranking Support)
```
Term → [(doc_id₁, tf₁), (doc_id₂, tf₂), ...]
```
**Space**: ~8 bytes per posting
**Query Support**: Boolean + ranked retrieval
**Scoring**: TF-IDF, BM25 supported

#### **Positional Index** (Full Text Search)
```
Term → [(doc_id₁, tf₁, [pos₁, pos₂, ...]), ...]
```
**Space**: Variable, typically 12+ bytes per posting
**Query Support**: All query types including phrases
**Scoring**: Full scoring + proximity

### Update Strategy Trade-offs

#### **Batch Updates** (Rebuild Index)
**Advantages**:
- Optimal index structure
- Better compression ratios
- Simpler implementation

**Disadvantages**:
- Service downtime during rebuild
- High memory requirements
- Slow to reflect new documents

#### **Incremental Updates** (Dynamic Index)
**Advantages**:
- Real-time document addition
- No service interruption
- Lower memory peaks

**Disadvantages**:
- Index fragmentation
- Complex implementation
- Potential performance degradation

#### **Hybrid Approach** (Multi-Level Index)
```
┌─────────────────┐
│   Main Index    │  ← Large, static, optimized
│  (Batch Built)  │
└─────────────────┘
┌─────────────────┐
│   Delta Index   │  ← Small, dynamic, real-time
│ (Incremental)   │
└─────────────────┘
```

**Query Processing**: Search both indexes and merge results
**Periodic Merge**: Rebuild main index incorporating delta changes

### Compression vs Access Speed

#### **Uncompressed Index**
**Advantages**: Direct memory access, simple implementation
**Disadvantages**: Large memory footprint, poor cache efficiency

#### **Compressed Index**
**Advantages**: Reduced storage, better cache behavior
**Disadvantages**: Decompression overhead, complex implementation

**Compression Techniques**:
- **Delta Encoding**: Store differences between consecutive document IDs
- **Variable-Byte Encoding**: Use fewer bytes for small numbers
- **PForDelta**: Optimized integer compression
- **Block-Based Compression**: Compress posting list blocks

---

## Study Questions

### Beginner Level
1. What is an inverted index and how does it differ from a forward index?
2. Why are posting lists sorted by document ID?
3. What are the main components of an inverted index architecture?
4. How does an inverted index enable faster search than linear scanning?

### Intermediate Level
1. Compare hash tables, B+ trees, and tries for term dictionary implementation. What are the trade-offs?
2. Explain how set intersection works on posting lists for AND queries.
3. What additional information is stored in posting lists for ranked retrieval vs Boolean search?
4. How do skip lists optimize posting list intersection operations?

### Advanced Level
1. Design an inverted index structure that supports both Boolean and phrase queries efficiently.
2. Analyze the space and time complexity trade-offs between different posting list compression schemes.
3. How would you implement incremental updates to an inverted index without blocking queries?
4. Design a distributed inverted index architecture for a web-scale search engine.

### Tricky Questions
1. **Memory Paradox**: Why might a larger inverted index sometimes be faster than a smaller one?
2. **Update Dilemma**: How do you handle the situation where a document update changes the vocabulary?
3. **Query Complexity**: What happens to query performance when posting lists have very different lengths?
4. **Storage Strategy**: When would you choose to store the full document content separately from the index?

---

## Code Examples

### Basic Inverted Index Implementation
```python
from collections import defaultdict
import re
from typing import List, Dict, Set, Tuple

class SimpleInvertedIndex:
    def __init__(self):
        self.term_dict = {}  # term -> TermEntry
        self.posting_lists = {}  # term -> posting list
        self.documents = {}  # doc_id -> document content
        self.collection_stats = CollectionStatistics()
    
    def add_document(self, doc_id: int, content: str):
        """Add a document to the index"""
        # Tokenize document
        terms = self.tokenize(content)
        
        # Store document
        self.documents[doc_id] = content
        
        # Update collection statistics
        self.collection_stats.update_from_document(len(terms))
        
        # Count term frequencies
        term_frequencies = defaultdict(int)
        for term in terms:
            term_frequencies[term] += 1
        
        # Update inverted index
        for term, tf in term_frequencies.items():
            # Update term dictionary
            if term not in self.term_dict:
                self.term_dict[term] = TermEntry(term)
                self.posting_lists[term] = []
            
            # Add posting to list
            posting = SimplePosting(doc_id, tf)
            self.posting_lists[term].append(posting)
            
            # Update term statistics
            self.term_dict[term].document_frequency += 1
            self.term_dict[term].collection_frequency += tf
        
        # Keep posting lists sorted by document ID
        for term in term_frequencies:
            self.posting_lists[term].sort(key=lambda p: p.doc_id)
    
    def tokenize(self, text: str) -> List[str]:
        """Simple tokenization"""
        text = text.lower()
        terms = re.findall(r'\b\w+\b', text)
        return terms
    
    def search(self, query: str) -> List[int]:
        """Simple AND query search"""
        query_terms = self.tokenize(query)
        
        if not query_terms:
            return []
        
        # Get posting lists for all query terms
        posting_lists = []
        for term in query_terms:
            if term in self.posting_lists:
                posting_lists.append(self.posting_lists[term])
            else:
                return []  # Term not found, no results
        
        # Intersect all posting lists
        result = posting_lists[0]
        for i in range(1, len(posting_lists)):
            result = self.intersect_postings(result, posting_lists[i])
        
        return [p.doc_id for p in result]
    
    def intersect_postings(self, list1: List, list2: List) -> List:
        """Intersect two posting lists"""
        result = []
        i, j = 0, 0
        
        while i < len(list1) and j < len(list2):
            if list1[i].doc_id == list2[j].doc_id:
                # Document appears in both lists
                result.append(list1[i])
                i += 1
                j += 1
            elif list1[i].doc_id < list2[j].doc_id:
                i += 1
            else:
                j += 1
        
        return result
    
    def get_term_statistics(self, term: str) -> Dict:
        """Get statistics for a term"""
        if term not in self.term_dict:
            return None
        
        entry = self.term_dict[term]
        return {
            'term': term,
            'document_frequency': entry.document_frequency,
            'collection_frequency': entry.collection_frequency,
            'idf': math.log(self.collection_stats.total_documents / entry.document_frequency)
        }

class SimplePosting:
    def __init__(self, doc_id: int, tf: int):
        self.doc_id = doc_id
        self.tf = tf
    
    def __repr__(self):
        return f"Posting(doc_id={self.doc_id}, tf={self.tf})"

class TermEntry:
    def __init__(self, term: str):
        self.term = term
        self.document_frequency = 0
        self.collection_frequency = 0

class CollectionStatistics:
    def __init__(self):
        self.total_documents = 0
        self.total_terms = 0
        self.average_document_length = 0
    
    def update_from_document(self, doc_length: int):
        self.total_documents += 1
        self.total_terms += doc_length
        self.average_document_length = self.total_terms / self.total_documents
```

### Positional Index Implementation
```python
class PositionalInvertedIndex:
    def __init__(self):
        self.term_dict = {}
        self.positional_postings = {}
        self.documents = {}
    
    def add_document(self, doc_id: int, content: str):
        """Add document with positional information"""
        terms = self.tokenize(content)
        self.documents[doc_id] = content
        
        # Track positions for each term
        term_positions = defaultdict(list)
        for position, term in enumerate(terms):
            term_positions[term].append(position)
        
        # Update positional index
        for term, positions in term_positions.items():
            if term not in self.term_dict:
                self.term_dict[term] = TermEntry(term)
                self.positional_postings[term] = []
            
            # Create positional posting
            posting = PositionalPosting(doc_id, positions)
            self.positional_postings[term].append(posting)
            
            # Update statistics
            self.term_dict[term].document_frequency += 1
            self.term_dict[term].collection_frequency += len(positions)
        
        # Sort posting lists
        for term in term_positions:
            self.positional_postings[term].sort(key=lambda p: p.doc_id)
    
    def phrase_search(self, phrase: str, max_distance: int = 1) -> List[int]:
        """Search for phrase with specified maximum distance between terms"""
        phrase_terms = self.tokenize(phrase)
        
        if len(phrase_terms) < 2:
            return self.search(" ".join(phrase_terms))
        
        # Get posting lists for phrase terms
        posting_lists = []
        for term in phrase_terms:
            if term in self.positional_postings:
                posting_lists.append(self.positional_postings[term])
            else:
                return []  # Term not found
        
        # Find documents containing all terms
        common_docs = self.intersect_positional_postings(posting_lists)
        
        # Check phrase constraints
        phrase_results = []
        for doc_postings in common_docs:
            if self.satisfies_phrase_constraint(doc_postings, phrase_terms, max_distance):
                phrase_results.append(doc_postings[0].doc_id)
        
        return phrase_results
    
    def intersect_positional_postings(self, posting_lists: List[List]) -> List[List]:
        """Intersect positional posting lists"""
        if not posting_lists:
            return []
        
        # Start with first list
        result_docs = {}
        for posting in posting_lists[0]:
            result_docs[posting.doc_id] = [posting]
        
        # Intersect with remaining lists
        for posting_list in posting_lists[1:]:
            new_result_docs = {}
            
            for posting in posting_list:
                if posting.doc_id in result_docs:
                    new_result_docs[posting.doc_id] = result_docs[posting.doc_id] + [posting]
            
            result_docs = new_result_docs
        
        return list(result_docs.values())
    
    def satisfies_phrase_constraint(self, doc_postings: List, phrase_terms: List[str], max_distance: int) -> bool:
        """Check if document satisfies phrase constraint"""
        # This is a simplified implementation
        # Real implementation would need to handle term order and distances
        
        positions_lists = [posting.positions for posting in doc_postings]
        
        # Check if we can find a sequence of positions that forms the phrase
        for start_pos in positions_lists[0]:
            current_pos = start_pos
            found_phrase = True
            
            for i in range(1, len(positions_lists)):
                # Look for next term within max_distance
                found_next = False
                for pos in positions_lists[i]:
                    if current_pos < pos <= current_pos + max_distance + 1:
                        current_pos = pos
                        found_next = True
                        break
                
                if not found_next:
                    found_phrase = False
                    break
            
            if found_phrase:
                return True
        
        return False
    
    def tokenize(self, text: str) -> List[str]:
        """Simple tokenization preserving order"""
        text = text.lower()
        terms = re.findall(r'\b\w+\b', text)
        return terms

class PositionalPosting:
    def __init__(self, doc_id: int, positions: List[int]):
        self.doc_id = doc_id
        self.positions = positions
        self.tf = len(positions)
    
    def __repr__(self):
        return f"PositionalPosting(doc_id={self.doc_id}, tf={self.tf}, positions={self.positions[:5]}{'...' if len(self.positions) > 5 else ''})"

# Example usage
def demo_inverted_index():
    # Create index
    index = PositionalInvertedIndex()
    
    # Add documents
    documents = {
        1: "machine learning algorithms for data analysis",
        2: "deep learning neural networks and machine intelligence",
        3: "machine learning is a subset of artificial intelligence", 
        4: "data science uses machine learning techniques",
        5: "artificial neural networks in deep learning applications"
    }
    
    for doc_id, content in documents.items():
        index.add_document(doc_id, content)
    
    # Search examples
    print("Simple search for 'machine learning':")
    simple_results = index.search("machine learning")
    for doc_id in simple_results:
        print(f"  Doc {doc_id}: {documents[doc_id]}")
    
    print("\nPhrase search for 'machine learning':")
    phrase_results = index.phrase_search("machine learning", max_distance=1)
    for doc_id in phrase_results:
        print(f"  Doc {doc_id}: {documents[doc_id]}")
    
    print("\nPhrase search for 'deep learning':")
    phrase_results = index.phrase_search("deep learning", max_distance=1)
    for doc_id in phrase_results:
        print(f"  Doc {doc_id}: {documents[doc_id]}")

if __name__ == "__main__":
    demo_inverted_index()
```

---

## Key Takeaways
1. **Efficiency**: Inverted indexes transform O(N×M) linear search into O(log V + L) indexed lookup
2. **Architecture**: Multi-component structure with term dictionary, posting lists, and document store
3. **Trade-offs**: Balance between space usage, query speed, and supported functionality
4. **Flexibility**: Different structures support different query types (Boolean, ranked, phrase)
5. **Scalability**: Foundation for web-scale search engines and enterprise search systems

---

**Next**: In day2_003_v2.md, we'll explore index construction algorithms, update strategies, and maintenance procedures for building and maintaining inverted indexes at scale.