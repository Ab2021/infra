# SQL Agent System: Agent & Memory Architecture Guide

## Table of Contents
1. [Introduction to Agentic Systems](#introduction-to-agentic-systems)
2. [System Overview](#system-overview)
3. [Agent Architecture](#agent-architecture)
4. [Memory System Architecture](#memory-system-architecture)
5. [Data Flow & Workflows](#data-flow--workflows)
6. [Implementation Details](#implementation-details)
7. [Beginner's Guide to Concepts](#beginners-guide-to-concepts)
8. [Advanced Patterns](#advanced-patterns)

---

## Introduction to Agentic Systems

### What is an Agentic System?

An **agentic system** is a software architecture where autonomous agents work together to solve complex problems. Each agent has:

- **Specialized capabilities** (like a team member with specific skills)
- **Autonomy** to make decisions within their domain
- **Communication** abilities to coordinate with other agents
- **Memory** to learn from past experiences
- **Goals** they work towards achieving

Think of it like a software development team where:
- One person handles UI design (UI Agent)
- Another handles backend logic (Logic Agent) 
- A third manages databases (Data Agent)
- They all communicate and coordinate to build software

### Why Use Agents for SQL Generation?

Traditional SQL generation approaches often struggle with:
- **Context understanding**: What does the user really want?
- **Data awareness**: What tables/columns are available and relevant?
- **Quality assurance**: Is the generated SQL safe and performant?
- **Visualization**: How should results be presented?

Our agent system breaks this complex problem into **specialized, manageable tasks**:

```
User Query: "Show me sales trends by region for 2024"
    ‚Üì
Agent 1: "This wants trend analysis using sales and region data from 2024"
    ‚Üì
Agent 2: "Sales data is in fact_sales table, regions in dim_geography, filter by year"
    ‚Üì
Agent 3: "Generate SQL with time series, create line chart visualization"
```

---

## System Overview

### Architecture at a Glance

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    SQL Agent System                             ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  User Query: "Show sales trends by region this year"            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ   SQLAgentSystem          ‚îÇ ‚óÑ‚îÄ‚îÄ Main Orchestrator
    ‚îÇ   (main.py:37)            ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ Memory System   ‚îÇ ‚óÑ‚îÄ‚îÄ Shared Intelligence
         ‚îÇ (memory_system) ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ                Agent Pipeline                        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                  ‚îÇ           ‚îÇ           ‚îÇ
         ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
         ‚îÇ   Agent 1     ‚îÇ   ‚îÇ  ‚îÇ   Agent 2     ‚îÇ   ‚îÇ   Agent 3     ‚îÇ
         ‚îÇQuery Understanding‚îÇ ‚îÇData Profiling ‚îÇ   ‚îÇSQL Visualization‚îÇ
         ‚îÇ               ‚îÇ   ‚îÇ  ‚îÇ               ‚îÇ   ‚îÇ               ‚îÇ
         ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ   Database    ‚îÇ
                    ‚îÇ  (Snowflake)  ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Core Components

| Component | Location | Purpose |
|-----------|----------|---------|
| **Main Orchestrator** | `main.py:37` | Coordinates all agents and manages system lifecycle |
| **Memory System** | `memory/memory_system.py:16` | Provides intelligence and learning capabilities |
| **Agent 1** | `agents/query_understanding_agent.py:24` | Understands user intent and identifies required data |
| **Agent 2** | `agents/data_profiling_agent.py:30` | Analyzes actual data to determine optimal filters |
| **Agent 3** | `agents/sql_visualization_agent.py:34` | Generates SQL and determines visualization |
| **Snowflake Agent** | `snowflake_agent.py:65` | Production Snowflake integration with your procedures |

---

## Agent Architecture

### Agent Design Principles

Each agent follows the **Single Responsibility Principle**:

```python
# Agent Interface Pattern
class BaseAgent:
    def __init__(self, llm_provider, memory_system, **dependencies):
        self.llm_provider = llm_provider      # AI capabilities
        self.memory_system = memory_system    # Shared memory
        # Agent-specific dependencies
    
    async def process(self, input_data, context) -> Dict:
        # 1. Analyze input
        # 2. Retrieve relevant memories
        # 3. Apply agent logic
        # 4. Store learnings
        # 5. Return structured output
        pass
```

### Agent 1: Query Understanding Agent

**File**: `agents/query_understanding_agent.py`  
**Purpose**: Transform natural language into structured intent

```
Input: "Show me sales trends by region for 2024"
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Query Understanding Agent                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Extract Basic Intent (Line 197)                             ‚îÇ
‚îÇ    ‚Ä¢ Keywords: "trends", "by region", "2024"                   ‚îÇ
‚îÇ    ‚Ä¢ Action: trend_analysis                                     ‚îÇ
‚îÇ    ‚Ä¢ Metrics: [sales]                                          ‚îÇ
‚îÇ    ‚Ä¢ Dimensions: [region]                                      ‚îÇ
‚îÇ    ‚Ä¢ Time: 2024                                                ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 2. Enhance with LLM (Line 236)                                 ‚îÇ
‚îÇ    ‚Ä¢ Use schema context                                         ‚îÇ
‚îÇ    ‚Ä¢ Previous conversation history                              ‚îÇ
‚îÇ    ‚Ä¢ Generate structured intent                                 ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 3. Map to Schema (Line 377)                                    ‚îÇ
‚îÇ    ‚Ä¢ Find relevant tables: fact_sales, dim_geography           ‚îÇ
‚îÇ    ‚Ä¢ Find columns: sales_amount, region_name, date             ‚îÇ
‚îÇ    ‚Ä¢ Calculate confidence scores                                ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 4. Validate & Store (Line 483)                                 ‚îÇ
‚îÇ    ‚Ä¢ Ensure tables/columns exist                               ‚îÇ
‚îÇ    ‚Ä¢ Store successful patterns                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Output: {
  "intent": QueryIntent(action="trend_analysis", metrics=["sales"], 
                       dimensions=["region"], time_scope="2024"),
  "identified_tables": [{"name": "fact_sales", "confidence": 0.9}],
  "required_columns": [{"name": "sales_amount", "table": "fact_sales"}]
}
```

**Key Intelligence**:
- **Keyword Mapping** (Line 56): Maps common words to SQL concepts
- **LLM Enhancement** (Line 236): Uses AI for complex understanding
- **Schema Awareness** (Line 377): Maps intent to actual database structure
- **Column Name Resolution** (Line 100): Handles spaces/underscores correctly
- **Pattern Learning** (Line 631): Stores successful mappings for future use

### Agent 2: Data Profiling Agent

**File**: `agents/data_profiling_agent.py`  
**Purpose**: Understand actual data characteristics and suggest optimal filters

```
Input: Tables[fact_sales, dim_geography], Columns[sales_amount, region_name]
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                Data Profiling Agent                            ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Profile Columns (Line 97)                                   ‚îÇ
‚îÇ    ‚Ä¢ Execute: SELECT COUNT(DISTINCT region_name),              ‚îÇ
‚îÇ               COUNT(*), MIN(sales_amount)... FROM fact_sales   ‚îÇ
‚îÇ    ‚Ä¢ Analyze: Data types, null counts, value distributions     ‚îÇ
‚îÇ    ‚Ä¢ Score: Data quality metrics                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 2. Analyze Relationships (Line 352)                            ‚îÇ
‚îÇ    ‚Ä¢ Find potential joins between tables                       ‚îÇ
‚îÇ    ‚Ä¢ Identify common columns                                    ‚îÇ
‚îÇ    ‚Ä¢ Assess data consistency                                    ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 3. Generate Smart Filters (Line 449)                           ‚îÇ
‚îÇ    ‚Ä¢ Time filters: "WHERE YEAR(date) = 2024"                  ‚îÇ
‚îÇ    ‚Ä¢ Quality filters: "WHERE sales_amount IS NOT NULL"         ‚îÇ
‚îÇ    ‚Ä¢ Categorical filters: "WHERE region IN (top_regions)"      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 4. Store Insights (Line 643)                                   ‚îÇ
‚îÇ    ‚Ä¢ Cache column profiles for performance                      ‚îÇ
‚îÇ    ‚Ä¢ Store successful filter patterns                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Output: {
  "column_profiles": {
    "fact_sales.sales_amount": {type: "float", quality: 0.95},
    "dim_geography.region_name": {type: "categorical", values: [...]}
  },
  "suggested_filters": [
    {type: "time_filter", condition: "YEAR(date) = 2024", confidence: 0.9}
  ]
}
```

### Agent 3: SQL Visualization Agent

**File**: `agents/sql_visualization_agent.py`  
**Purpose**: Generate secure SQL and determine optimal visualization

```
Input: Intent, Column Profiles, Filter Suggestions
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                SQL Visualization Agent                         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Generate SQL (Line 133)                                     ‚îÇ
‚îÇ    ‚Ä¢ Check memory for similar patterns                         ‚îÇ
‚îÇ    ‚Ä¢ Use templates: trend_analysis, comparison, etc.           ‚îÇ
‚îÇ    ‚Ä¢ Fill template with actual table/column names              ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 2. Apply Guardrails (Line 437)                                 ‚îÇ
‚îÇ    ‚Ä¢ Security: Block DELETE, DROP, injection patterns          ‚îÇ
‚îÇ    ‚Ä¢ Performance: Check row limits, complexity scores          ‚îÇ
‚îÇ    ‚Ä¢ Quality: Validate syntax and logic                        ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 3. Determine Visualization (Line 525)                          ‚îÇ
‚îÇ    ‚Ä¢ Analyze SQL structure (aggregation, grouping, time)       ‚îÇ
‚îÇ    ‚Ä¢ Map to chart types: line_chart, bar_chart, table          ‚îÇ
‚îÇ    ‚Ä¢ Generate chart configuration                               ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 4. Generate Code (Line 701)                                    ‚îÇ
‚îÇ    ‚Ä¢ Create Streamlit visualization code                       ‚îÇ
‚îÇ    ‚Ä¢ Include error handling and download options               ‚îÇ
‚îÇ    ‚Ä¢ Store successful patterns                                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Output: {
  "sql_query": "SELECT region_name, SUM(sales_amount) FROM fact_sales fs 
                JOIN dim_geography dg ON fs.geography_id = dg.id 
                WHERE YEAR(date) = 2024 GROUP BY region_name",
  "chart_config": {type: "line_chart", x_axis: "date", y_axis: "sales"},
  "plotting_code": "st.line_chart(df.set_index('date')['sales'])"
}
```

### Snowflake Production Agent

**File**: `snowflake_agent.py`  
**Purpose**: Production-ready Snowflake integration matching your input procedures

```
Input: "List auto policies with premium over 10000 in Texas in 2023"
    ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Snowflake Production Agent                        ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ 1. Load Schema from Excel (Line 63)                            ‚îÇ
‚îÇ    ‚Ä¢ Read hackathon_final_schema_file_v1.xlsx                  ‚îÇ
‚îÇ    ‚Ä¢ Parse Table_descriptions and Column Summaries             ‚îÇ
‚îÇ    ‚Ä¢ Create column mappings for space/underscore handling      ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 2. Schema Analysis (Line 169)                                  ‚îÇ
‚îÇ    ‚Ä¢ Use GPT with exact column names from Excel                ‚îÇ
‚îÇ    ‚Ä¢ Map user query to relevant tables and columns             ‚îÇ
‚îÇ    ‚Ä¢ Validate column name variations                           ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 3. SQL Generation (Line 232)                                   ‚îÇ
‚îÇ    ‚Ä¢ Build detailed context with sample values                 ‚îÇ
‚îÇ    ‚Ä¢ Use few-shot examples if available                        ‚îÇ
‚îÇ    ‚Ä¢ Generate SQL with proper column quoting                   ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ 4. Snowflake Execution (Line 366)                              ‚îÇ
‚îÇ    ‚Ä¢ Execute on actual Snowflake using your connection         ‚îÇ
‚îÇ    ‚Ä¢ Return pandas DataFrame with results                      ‚îÇ
‚îÇ    ‚Ä¢ Handle errors gracefully                                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
    ‚Üì
Output: QueryResult(
  success=True, sql_query="SELECT ...", 
  execution_result=DataFrame([...]), processing_time=2.1
)
```

---

## Memory System Architecture

### Memory Design Philosophy

The memory system acts as the **collective intelligence** of all agents:

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Memory System Architecture                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê                    ‚îÇ
‚îÇ  ‚îÇ  Session Memory ‚îÇ    ‚îÇ Knowledge Memory ‚îÇ                    ‚îÇ
‚îÇ  ‚îÇ  (Conversations)‚îÇ    ‚îÇ (Learning Store) ‚îÇ                    ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò                    ‚îÇ
‚îÇ           ‚îÇ                       ‚îÇ                             ‚îÇ
‚îÇ           ‚ñº                       ‚ñº                             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ               SQLite In-Memory Databases                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Ultra-fast: 50-100x faster than disk                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ACID compliant: Reliable transactions                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Optional persistence: Best of both worlds               ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                                 ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ                    Memory Operations                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Store successful patterns                               ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Find similar queries (text similarity)                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Cache column profiles                                    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Track conversation context                              ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### Memory Components

#### 1. Session Memory (Conversation Context)

**Tables**: `sessions`, `conversation_history`  
**Purpose**: Track user interactions and maintain context

```sql
-- Session tracking (memory_system.py:67)
CREATE TABLE sessions (
    id TEXT PRIMARY KEY,
    user_id TEXT NOT NULL,
    created_at TEXT NOT NULL,
    last_activity TEXT NOT NULL,
    context_data TEXT
);

-- Conversation history (memory_system.py:75)
CREATE TABLE conversation_history (
    id TEXT PRIMARY KEY,
    session_id TEXT NOT NULL,
    query TEXT NOT NULL,           -- Original user query
    intent TEXT,                   -- Structured intent (JSON)
    tables_used TEXT,              -- Tables accessed (JSON array)
    chart_type TEXT,               -- Visualization type
    success INTEGER DEFAULT 1,     -- Whether query succeeded
    timestamp TEXT NOT NULL
);
```

#### 2. Knowledge Memory (Learning Store)

**Tables**: `successful_queries`, `query_patterns`, `schema_insights`  
**Purpose**: Store learning and enable pattern reuse

```sql
-- Successful query patterns (memory_system.py:89)
CREATE TABLE successful_queries (
    id TEXT PRIMARY KEY,
    query_pattern TEXT NOT NULL,    -- Extracted pattern for similarity
    sql_query TEXT NOT NULL,        -- Generated SQL
    tables_used TEXT,               -- Tables involved
    chart_type TEXT,                -- Visualization used
    execution_time REAL DEFAULT 0.0, -- Performance metric
    result_count INTEGER DEFAULT 0,  -- Result size
    success_score REAL DEFAULT 1.0,  -- Quality score
    usage_count INTEGER DEFAULT 1,   -- Reuse frequency
    created_at TEXT NOT NULL,
    last_used TEXT NOT NULL,
    metadata TEXT                    -- Additional context (JSON)
);
```

---

## Data Flow & Workflows

### Complete Request Flow

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    User Request Workflow                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 1. User Input                      ‚îÇ
    ‚îÇ "Show sales trends by region 2024" ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 2. Session Setup (main.py:224)     ‚îÇ
    ‚îÇ ‚Ä¢ Create/retrieve session          ‚îÇ
    ‚îÇ ‚Ä¢ Get conversation history         ‚îÇ
    ‚îÇ ‚Ä¢ Initialize context               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 3. Agent 1: Query Understanding    ‚îÇ
    ‚îÇ ‚Ä¢ Extract intent: trend_analysis   ‚îÇ
    ‚îÇ ‚Ä¢ Identify metrics: [sales]        ‚îÇ
    ‚îÇ ‚Ä¢ Identify dimensions: [region]    ‚îÇ
    ‚îÇ ‚Ä¢ Map to schema: fact_sales table  ‚îÇ
    ‚îÇ ‚Ä¢ Confidence: 0.9                  ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 4. Agent 2: Data Profiling        ‚îÇ
    ‚îÇ ‚Ä¢ Query: SELECT COUNT DISTINCT...  ‚îÇ
    ‚îÇ ‚Ä¢ Profile sales_amount column      ‚îÇ
    ‚îÇ ‚Ä¢ Profile region_name column       ‚îÇ
    ‚îÇ ‚Ä¢ Suggest time filter: YEAR = 2024 ‚îÇ
    ‚îÇ ‚Ä¢ Quality score: 0.95              ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 5. Agent 3: SQL & Visualization   ‚îÇ
    ‚îÇ ‚Ä¢ Generate SQL from template       ‚îÇ
    ‚îÇ ‚Ä¢ Apply guardrails (security)      ‚îÇ
    ‚îÇ ‚Ä¢ Choose chart: line_chart         ‚îÇ
    ‚îÇ ‚Ä¢ Generate Streamlit code          ‚îÇ
    ‚îÇ ‚Ä¢ Validation: PASSED               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 6. Memory Storage                  ‚îÇ
    ‚îÇ ‚Ä¢ Store conversation history       ‚îÇ
    ‚îÇ ‚Ä¢ Cache successful patterns        ‚îÇ
    ‚îÇ ‚Ä¢ Update usage statistics          ‚îÇ
    ‚îÇ ‚Ä¢ Store schema insights            ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 7. Response Assembly               ‚îÇ
    ‚îÇ ‚Ä¢ Combine all agent outputs        ‚îÇ
    ‚îÇ ‚Ä¢ Include metadata and timing      ‚îÇ
    ‚îÇ ‚Ä¢ Format for user interface        ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ 8. Result Delivery                 ‚îÇ
    ‚îÇ ‚Ä¢ SQL: SELECT region, SUM(sales).. ‚îÇ
    ‚îÇ ‚Ä¢ Chart: Line chart config         ‚îÇ
    ‚îÇ ‚Ä¢ Code: st.line_chart(df)          ‚îÇ
    ‚îÇ ‚Ä¢ Time: 2.3 seconds                ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## Implementation Details

### Key Files and Their Roles

| File | Lines of Code | Key Responsibilities |
|------|---------------|---------------------|
| `main.py` | 540 | System orchestration, agent coordination, lifecycle management |
| `memory/memory_system.py` | 470 | Memory operations, pattern storage, similarity search |
| `agents/query_understanding_agent.py` | 647 | NLU, intent extraction, schema mapping, column resolution |
| `agents/data_profiling_agent.py` | 674 | Data analysis, column profiling, filter suggestions |
| `agents/sql_visualization_agent.py` | 837 | SQL generation, guardrails, visualization |
| `snowflake_agent.py` | 650 | Production Snowflake integration with your procedures |
| `test_main.py` | 800 | Comprehensive testing suite for all components |

### Agent Coordination (main.py)

```python
class SQLAgentSystem:
    async def process_query(self, user_query: str, user_id: str = "anonymous", 
                          session_id: str = None) -> Dict[str, Any]:
        """3-agent pipeline coordination"""
        
        # Step 1: Query Understanding Agent (Line 230)
        understanding_result = await self.query_agent.process(
            user_query=user_query,
            session_context=session_context
        )
        
        # Step 2: Data Profiling Agent (Line 244)  
        profiling_result = await self.profiling_agent.process(
            tables=understanding_result.get("identified_tables", []),
            columns=understanding_result.get("required_columns", []),
            intent=understanding_result.get("query_intent", {})
        )
        
        # Step 3: SQL Visualization Agent (Line 258)
        sql_viz_result = await self.sql_viz_agent.process(
            query_intent=understanding_result.get("query_intent", {}),
            column_profiles=profiling_result.get("column_profiles", {}),
            suggested_filters=profiling_result.get("suggested_filters", [])
        )
        
        # Combine and return results (Line 276)
        return self._combine_results(understanding_result, profiling_result, sql_viz_result)
```

### Column Name Handling System

The system now properly handles your Excel schema with spaces in column names:

```python
# Column name resolution (query_understanding_agent.py:100)
def resolve_column_name(self, column_name: str, table_name: str = None) -> str:
    """Resolve column name variations to correct database names"""
    # Handles: "Feature Name" ‚Üî "Feature_Name" ‚Üî "feature_name"
    
def quote_column_name(self, column_name: str) -> str:
    """Quote column names that contain spaces for SQL"""
    if ' ' in column_name:
        return f'"{column_name}"'  # "Feature Name"
    return column_name            # regular_column
```

---

## Beginner's Guide to Concepts

### Core Concepts Explained

#### 1. What is an "Agent"?

Think of an agent as a **specialized worker** in a team:

```python
# Agent Concept
class Agent:
    def __init__(self, specialty, tools, memory):
        self.specialty = specialty  # What I'm good at
        self.tools = tools         # What I can use (LLM, database, etc.)
        self.memory = memory       # What I remember
    
    async def process(self, task):
        # 1. Understand the task
        # 2. Use my specialty and tools
        # 3. Learn from the result
        # 4. Return my contribution
        pass
```

#### 2. Why Memory Matters

```
‚ùå Without Memory:
User: "Show sales by region"
System: Starts from scratch every time, slow and inefficient

‚úÖ With Memory:
User: "Show sales by region"  
System: "I remember doing this before, let me reuse what worked"
```

#### 3. Column Name Challenges

Your Excel schema has columns like "Feature Name" and "Policy Number" (with spaces), but sometimes systems expect "Feature_Name" (with underscores). Our system handles this automatically:

```python
# Automatic resolution:
"Feature_Name" ‚Üí "Feature Name" ‚Üí "Feature Name" (SQL: "Feature Name")
"feature name" ‚Üí "Feature Name" ‚Üí "Feature Name" (SQL: "Feature Name")  
"Policy Number" ‚Üí "Policy Number" ‚Üí "Policy Number" (SQL: "Policy Number")
```

---

## Testing Your System

### Running Tests

```bash
# Full integration test
python test_main.py

# Interactive testing mode
python test_main.py interactive

# Test specific components
python test_main.py schema    # Test schema loading
python test_main.py init      # Test initialization

# Production Snowflake agent
python snowflake_agent.py
```

### Test Coverage

The test suite covers:
- ‚úÖ **Schema Loading**: Excel file parsing and validation
- ‚úÖ **System Initialization**: All component setup
- ‚úÖ **Agent Pipeline**: Complete query processing
- ‚úÖ **Memory System**: Storage and retrieval
- ‚úÖ **Column Handling**: Space/underscore normalization
- ‚úÖ **SQL Execution**: Snowflake integration
- ‚úÖ **Error Handling**: Graceful failure management

### Example Test Output

```
üß™ SQL AGENT SYSTEM - COMPREHENSIVE TEST SUITE
============================================================
üìä System Information:
   schema_file: hackathon_final_schema_file_v1.xlsx
   total_tables: 15
   total_columns: 250
   column_mappings: 500
   gpt_available: True

‚úÖ Schema Loading PASSED
‚úÖ System Initialization PASSED  
‚úÖ Query Pipeline Tests: 4/5 PASSED (80%)
‚úÖ Memory System PASSED
‚úÖ Column Handling PASSED

üéâ INTEGRATION TESTS PASSED! System is ready for production.
```

---

This comprehensive documentation now reflects your clean, production-ready SQL Agent System without any prefixes or suffixes, properly integrated with your Snowflake infrastructure and column naming conventions.