# Day 1 - Part 3: Types of Search and Recommendation Systems

## Table of Contents
1. [Classification Framework](#classification-framework)
2. [Content-Based Filtering Systems](#content-based-filtering)
3. [Collaborative Filtering Systems](#collaborative-filtering)
4. [Hybrid Recommendation Systems](#hybrid-systems)
5. [Semantic Search Systems](#semantic-search)
6. [Knowledge-Based Systems](#knowledge-based-systems)
7. [Context-Aware Systems](#context-aware-systems)
8. [Deep Learning Approaches](#deep-learning-approaches)
9. [Comparative Analysis](#comparative-analysis)
10. [Study Questions](#study-questions)
11. [Code Examples](#code-examples)

---

## Classification Framework

Search and recommendation systems can be classified along multiple dimensions, each addressing different aspects of the information discovery problem:

### Primary Classification Dimensions

#### 1. **Input Signal Type**
- **Explicit Feedback**: Direct user ratings, likes/dislikes, bookmarks
- **Implicit Feedback**: Views, clicks, purchase history, dwell time
- **Hybrid Feedback**: Combination of explicit and implicit signals

#### 2. **Recommendation Strategy**
- **Content-Based**: Item features and user preferences
- **Collaborative Filtering**: User-item interaction patterns
- **Knowledge-Based**: Domain expertise and rules
- **Hybrid**: Multiple strategy combinations

#### 3. **Learning Paradigm**
- **Memory-Based**: Store and retrieve similar patterns
- **Model-Based**: Learn underlying patterns and structures
- **Deep Learning**: Neural network representations

#### 4. **Personalization Level**
- **Non-Personalized**: Popular items, trending content
- **Demographic**: Group-based recommendations
- **Personalized**: Individual user modeling
- **Contextual**: Situation-aware personalization

### Technical Architecture Patterns

#### **Pipeline Architecture**
```
Query/User → Candidate Generation → Ranking → Post-Processing → Results
```

#### **Multi-Stage Funnel**
1. **Recall**: Broad candidate retrieval (thousands of items)
2. **Ranking**: Detailed scoring (hundreds of items)
3. **Re-ranking**: Business logic application (tens of items)

---

## Content-Based Filtering Systems

### Core Principles

Content-based filtering recommends items similar to those a user has previously liked, based on **item features** rather than user behavior patterns.

**Mathematical Foundation**:
```
similarity(item_i, item_j) = cosine(features_i, features_j)
recommendation_score(user_u, item_i) = Σ(similarity(item_i, liked_item_k) × rating_u_k)
```

### Feature Representation Strategies

#### **Text-Based Features**
**TF-IDF Vectors**: Traditional bag-of-words representation
- Pros: Interpretable, established technique
- Cons: Ignores word order, semantic relationships

**Word Embeddings**: Dense vector representations (Word2Vec, GloVe)
- Pros: Captures semantic similarity
- Cons: Requires large training corpus

**Document Embeddings**: Entire document representations (Doc2Vec, BERT)
- Pros: Contextual understanding
- Cons: Computational complexity

#### **Categorical Features**
**One-Hot Encoding**: Binary feature vectors
- Example: Genre [Action: 1, Comedy: 0, Drama: 1]
- Challenge: High dimensionality with many categories

**Embedding Layers**: Dense categorical representations
- Learned representations reduce dimensionality
- Capture categorical relationships

#### **Numerical Features**
**Continuous Attributes**: Price, duration, ratings
**Discretization**: Convert continuous to categorical
**Normalization**: Ensure comparable scales across features

### Advanced Content-Based Techniques

#### **Aspect-Based Content Analysis**
Analyze different aspects of items separately:
- **Movies**: Plot, cast, cinematography, music
- **Products**: Quality, price, design, functionality
- **Articles**: Topic, writing style, complexity, bias

#### **Multi-Modal Content Understanding**
**Text + Images**: Product descriptions with visual features
**Audio Features**: Music recommendation using spectral analysis
**Video Analysis**: Scene detection, object recognition

### Advantages and Limitations

#### **Advantages**
1. **No Cold-Start for Items**: Can recommend new items immediately
2. **Transparency**: Clear reasoning for recommendations
3. **User Independence**: Doesn't require other users' data
4. **Domain Knowledge Integration**: Can incorporate expert features

#### **Limitations**
1. **Limited Novelty**: Recommends similar items (filter bubble)
2. **Feature Engineering**: Requires domain expertise
3. **Cold-Start for Users**: Needs user preference history
4. **Overspecialization**: May not discover new interests

---

## Collaborative Filtering Systems

### Memory-Based Collaborative Filtering

#### **User-Based Collaborative Filtering**
**Core Idea**: Users with similar preferences will like similar items

**Algorithm Steps**:
1. Compute user similarity matrix
2. Find k most similar users (neighbors)
3. Predict ratings using neighbors' ratings

**Similarity Metrics**:
```python
# Pearson Correlation
def pearson_similarity(user1, user2):
    common_items = set(user1.keys()) & set(user2.keys())
    if len(common_items) == 0: return 0
    
    sum1 = sum([user1[item] for item in common_items])
    sum2 = sum([user2[item] for item in common_items])
    
    sum1Sq = sum([pow(user1[item], 2) for item in common_items])
    sum2Sq = sum([pow(user2[item], 2) for item in common_items])
    
    pSum = sum([user1[item] * user2[item] for item in common_items])
    
    num = pSum - (sum1 * sum2 / len(common_items))
    den = sqrt((sum1Sq - pow(sum1, 2) / len(common_items)) * 
               (sum2Sq - pow(sum2, 2) / len(common_items)))
    
    if den == 0: return 0
    return num / den
```

#### **Item-Based Collaborative Filtering**
**Core Idea**: Items that are liked by the same users are similar

**Advantages over User-Based**:
- Item relationships more stable than user preferences
- Better for systems with more users than items
- Precomputable similarity matrices

### Model-Based Collaborative Filtering

#### **Matrix Factorization Techniques**

**Singular Value Decomposition (SVD)**:
```
R ≈ U × Σ × V^T
where:
R: user-item rating matrix (m×n)
U: user feature matrix (m×k)
Σ: singular values diagonal matrix (k×k)  
V: item feature matrix (n×k)
k: number of latent factors
```

**Non-Negative Matrix Factorization (NMF)**:
Constrains factors to be non-negative, improving interpretability

**Alternating Least Squares (ALS)**:
Optimizes user and item factors alternately:
```
minimize ||R - UV^T||²_F + λ(||U||²_F + ||V||²_F)
```

#### **Probabilistic Models**

**Probabilistic Matrix Factorization (PMF)**:
Models ratings as Gaussian distributions:
```
R_ij ~ N(U_i^T V_j, σ²)
U_i ~ N(0, σ_U² I)
V_j ~ N(0, σ_V² I)
```

**Bayesian Personalized Ranking (BPR)**:
Optimizes for ranking rather than rating prediction:
```
maximize Σ ln σ(r̂_uij)
where r̂_uij = r̂_ui - r̂_uj (preference difference)
```

### Deep Learning Collaborative Filtering

#### **Neural Collaborative Filtering (NCF)**
Replaces inner product with neural networks:
```
ŷ_ui = f(P_u, Q_i | Θ_f)
where f is a multi-layer perceptron
```

#### **Autoencoders for Collaborative Filtering**
**AutoRec**: Reconstruct user or item vectors
- Input: Partially observed user/item vector
- Output: Complete rating predictions
- Loss: Reconstruction error on observed ratings

---

## Hybrid Recommendation Systems

### Hybridization Strategies

#### **Weighted Hybrid**
Combine scores from multiple recommenders:
```
score_hybrid = α × score_content + β × score_collaborative + γ × score_knowledge
where α + β + γ = 1
```

#### **Switching Hybrid**
Choose recommender based on situation:
- New users → Content-based
- Established users → Collaborative filtering
- Cold items → Knowledge-based

#### **Mixed Hybrid**
Present recommendations from multiple systems simultaneously

#### **Feature Combination**
Merge features from different systems into single algorithm

#### **Cascade Hybrid**
Use hierarchical refinement:
1. Content-based: Generate candidate set
2. Collaborative filtering: Rank candidates
3. Knowledge-based: Apply business rules

#### **Meta-Level Hybrid**
Use output of one system as input to another:
- Collaborative filtering learns user preferences
- Content-based uses learned preferences as features

### Advanced Hybrid Architectures

#### **Multi-Criteria Recommendations**
Consider multiple rating dimensions:
- **Hotels**: Location, price, service, cleanliness
- **Movies**: Plot, acting, cinematography, music
- **Products**: Quality, value, design, usability

**Mathematical Model**:
```
overall_rating = w₁×criteria₁ + w₂×criteria₂ + ... + wₙ×criteriaₙ
where weights w_i learned from user feedback
```

#### **Cross-Domain Recommendations**
Transfer knowledge between domains:
- **Music → Movies**: User preference for emotional content
- **Books → Products**: Lifestyle and interest indicators
- **Social → Shopping**: Friend influence on purchases

---

## Semantic Search Systems

### From Lexical to Semantic Matching

#### **Traditional Keyword Search Limitations**
- **Vocabulary Mismatch**: "automobile" vs "car"
- **Polysemy**: "bank" (financial vs river)
- **Synonymy**: Multiple words, same meaning
- **Context Ignorance**: "apple" (fruit vs company)

#### **Semantic Search Solutions**

**Query Expansion**:
- Add synonyms and related terms
- Use thesaurus, WordNet, or learned associations
- Risk: Query drift and reduced precision

**Concept-Based Matching**:
- Map queries and documents to concept space
- Use ontologies or knowledge graphs
- Maintain semantic relationships

### Vector Space Models for Semantic Search

#### **Dense Retrieval**
Transform queries and documents into dense vectors:
```
similarity(query, document) = cosine(embed(query), embed(document))
```

**Advantages**:
- Captures semantic similarity
- Handles vocabulary mismatch
- Learns from training data

**Challenges**:
- Requires large training datasets
- Computationally intensive
- Less interpretable than lexical matching

#### **Learned Sparse Retrieval**
Combine benefits of sparse and dense approaches:
- Learn importance weights for terms
- Maintain interpretability
- Reduce computational overhead

### Transformer-Based Semantic Search

#### **BERT for Search**
**Bidirectional Context**: Consider both left and right context
**Fine-tuning Strategies**:
- **Classification**: Query-document relevance scoring
- **Ranking**: Pairwise relevance comparison
- **Generation**: Query reformulation and expansion

#### **Dense Passage Retrieval (DPR)**
**Two-Tower Architecture**:
```
query_embedding = BERT_q(query)
passage_embedding = BERT_p(passage)
similarity = query_embedding · passage_embedding
```

**Training**: Contrastive learning with positive and negative passages

---

## Knowledge-Based Systems

### Expert System Approach

#### **Rule-Based Recommendations**
Encode domain expertise as explicit rules:
```
IF user_age < 25 AND user_location = "urban" AND time = "weekend"
THEN recommend(trendy_restaurants, weight=0.8)
```

#### **Constraint-Based Systems**
Model recommendations as constraint satisfaction:
- **Hard Constraints**: Must be satisfied (budget, dietary restrictions)
- **Soft Constraints**: Preferences with weights (location, cuisine type)

### Ontology-Based Systems

#### **Knowledge Graph Integration**
**Entities and Relations**:
- Users, items, attributes as graph nodes
- Relationships as graph edges
- Reasoning through graph traversal

**Graph Embedding Methods**:
- **TransE**: Relations as translations in embedding space
- **ComplEx**: Complex-valued embeddings for asymmetric relations
- **Graph Neural Networks**: Learn from graph structure

#### **Semantic Reasoning**
**Logical Inference**: Derive new facts from existing knowledge
**Taxonomic Reasoning**: Use is-a relationships for generalization
**Temporal Reasoning**: Consider time-dependent relationships

---

## Context-Aware Systems

### Context Dimensions

#### **Temporal Context**
- **Time of Day**: Morning news vs evening entertainment
- **Day of Week**: Weekday vs weekend preferences
- **Seasonality**: Holiday shopping, summer activities
- **Long-term Trends**: Evolving user interests

#### **Spatial Context**
- **Location**: Nearby restaurants, local events
- **Mobility**: Stationary vs traveling recommendations
- **Geographic Culture**: Regional preferences and availability

#### **Social Context**
- **Companion**: Individual vs group recommendations
- **Social Network**: Friend influences and recommendations
- **Social Events**: Party planning, gift suggestions

#### **Device Context**
- **Screen Size**: Mobile vs desktop content optimization
- **Input Method**: Voice vs text vs touch
- **Connection**: Bandwidth-aware content delivery

### Context Modeling Approaches

#### **Pre-filtering**
Filter data based on context before recommendation:
```
relevant_data = filter(all_data, current_context)
recommendations = recommend(user, relevant_data)
```

#### **Post-filtering**
Generate recommendations then filter by context:
```
initial_recommendations = recommend(user, all_data)
final_recommendations = filter(initial_recommendations, current_context)
```

#### **Contextual Modeling**
Incorporate context directly into recommendation model:
```
score(user, item, context) = model(user_features, item_features, context_features)
```

---

## Deep Learning Approaches

### Neural Network Architectures

#### **Multi-Layer Perceptrons (MLPs)**
**Wide & Deep Model** (Google):
- **Wide Component**: Linear model for memorization
- **Deep Component**: Neural network for generalization
- **Joint Training**: Optimize both components together

#### **Convolutional Neural Networks (CNNs)**
**Applications**:
- **Image Recommendations**: Visual similarity
- **Text Processing**: Local feature extraction
- **Audio Analysis**: Spectral pattern recognition

#### **Recurrent Neural Networks (RNNs)**
**Sequential Modeling**:
- **User Behavior**: Model temporal patterns
- **Session-Based**: Next-item prediction
- **Long-term vs Short-term**: Balance historical and recent preferences

#### **Attention Mechanisms**
**Self-Attention**: Focus on relevant parts of user history
**Cross-Attention**: Align user preferences with item features
**Multi-Head Attention**: Capture different types of relationships

### Advanced Neural Architectures

#### **Graph Neural Networks (GNNs)**
**User-Item Graphs**: Model interactions as graph structure
**Message Passing**: Aggregate information from neighbors
**Applications**: Social recommendations, knowledge graph reasoning

#### **Variational Autoencoders (VAEs)**
**Generative Modeling**: Learn user/item distributions
**Uncertainty Quantification**: Model recommendation confidence
**Disentangled Representations**: Separate different factors of variation

---

## Comparative Analysis

### Performance Trade-offs

| System Type | Accuracy | Scalability | Cold-Start | Transparency | Novelty |
|-------------|----------|-------------|------------|--------------|---------|
| Content-Based | Medium | High | Good (items) | High | Low |
| Collaborative | High | Medium | Poor | Low | High |
| Knowledge-Based | Medium | Low | Excellent | High | Medium |
| Deep Learning | High | Medium | Medium | Low | Medium |
| Hybrid | High | Medium | Good | Medium | High |

### Selection Criteria

#### **Data Availability**
- **Rich Content**: Content-based systems
- **Abundant Interactions**: Collaborative filtering
- **Domain Expertise**: Knowledge-based systems
- **Large Scale**: Deep learning approaches

#### **Business Requirements**
- **Explainability**: Content-based or knowledge-based
- **Real-time**: Memory-based or cached model-based
- **Diversity**: Hybrid or knowledge-based
- **Accuracy**: Collaborative filtering or deep learning

---

## Study Questions

### Beginner Level
1. What is the fundamental difference between content-based and collaborative filtering?
2. Why do hybrid systems often outperform individual recommendation approaches?
3. What is the cold-start problem and how do different system types handle it?
4. How does semantic search differ from traditional keyword search?

### Intermediate Level
1. Compare user-based vs item-based collaborative filtering in terms of computational complexity and recommendation quality.
2. Explain how matrix factorization techniques like SVD work for recommendations.
3. What are the trade-offs between pre-filtering, post-filtering, and contextual modeling for context-aware systems?
4. How do knowledge graphs enhance recommendation systems?

### Advanced Level
1. Design a hybrid system that combines collaborative filtering, content-based, and knowledge-based approaches for a specific domain.
2. Analyze the computational and storage trade-offs between exact and approximate nearest neighbor search in vector-based systems.
3. How do transformer architectures like BERT change the paradigm of semantic search compared to traditional embedding methods?
4. Compare and contrast different graph neural network architectures for recommendation systems.

### Tricky Questions
1. **Paradox**: Why might a more accurate recommendation system lead to lower user satisfaction?
2. **Scalability**: At what point does the curse of dimensionality make high-dimensional embeddings counterproductive?
3. **Context Sensitivity**: How do you handle conflicting contextual signals (e.g., location suggests restaurants but time suggests sleep)?
4. **Evaluation Challenge**: Why might offline metrics not correlate with online performance for different system types?

---

## Code Examples

### Content-Based Filtering Implementation
```python
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class ContentBasedRecommender:
    def __init__(self):
        self.vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)
        self.item_features = None
        self.feature_matrix = None
        
    def fit(self, items_content):
        """Train the content-based model"""
        self.item_features = items_content
        self.feature_matrix = self.vectorizer.fit_transform(items_content)
        
    def get_recommendations(self, liked_items, top_k=10):
        """Get recommendations based on liked items"""
        if not isinstance(liked_items, list):
            liked_items = [liked_items]
            
        # Create user profile by averaging liked items
        liked_indices = [i for i, item in enumerate(self.item_features) 
                        if item in liked_items]
        user_profile = np.mean(self.feature_matrix[liked_indices], axis=0)
        
        # Compute similarities
        similarities = cosine_similarity(user_profile, self.feature_matrix).flatten()
        
        # Get top recommendations (excluding already liked items)
        item_scores = [(i, score) for i, score in enumerate(similarities) 
                      if i not in liked_indices]
        item_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [(self.item_features[i], score) for i, score in item_scores[:top_k]]
```

### Matrix Factorization for Collaborative Filtering
```python
import numpy as np
from scipy.sparse import csr_matrix

class MatrixFactorization:
    def __init__(self, n_factors=50, learning_rate=0.01, regularization=0.1):
        self.n_factors = n_factors
        self.learning_rate = learning_rate
        self.regularization = regularization
        
    def fit(self, ratings_matrix, n_epochs=100):
        """Train matrix factorization model"""
        n_users, n_items = ratings_matrix.shape
        
        # Initialize factor matrices
        self.user_factors = np.random.normal(0, 0.1, (n_users, self.n_factors))
        self.item_factors = np.random.normal(0, 0.1, (n_items, self.n_factors))
        self.user_bias = np.zeros(n_users)
        self.item_bias = np.zeros(n_items)
        self.global_bias = np.mean(ratings_matrix.data)
        
        # Convert to sparse matrix for efficiency
        if not isinstance(ratings_matrix, csr_matrix):
            ratings_matrix = csr_matrix(ratings_matrix)
            
        # Training loop
        for epoch in range(n_epochs):
            for user_id, item_id, rating in zip(*ratings_matrix.nonzero(), 
                                               ratings_matrix.data):
                # Predict rating
                prediction = (self.global_bias + 
                            self.user_bias[user_id] + 
                            self.item_bias[item_id] +
                            np.dot(self.user_factors[user_id], 
                                  self.item_factors[item_id]))
                
                # Compute error
                error = rating - prediction
                
                # Update factors using gradient descent
                user_factor = self.user_factors[user_id].copy()
                
                self.user_factors[user_id] += self.learning_rate * (
                    error * self.item_factors[item_id] - 
                    self.regularization * self.user_factors[user_id]
                )
                
                self.item_factors[item_id] += self.learning_rate * (
                    error * user_factor - 
                    self.regularization * self.item_factors[item_id]
                )
                
                self.user_bias[user_id] += self.learning_rate * (
                    error - self.regularization * self.user_bias[user_id]
                )
                
                self.item_bias[item_id] += self.learning_rate * (
                    error - self.regularization * self.item_bias[item_id]
                )
    
    def predict(self, user_id, item_id):
        """Predict rating for user-item pair"""
        return (self.global_bias + 
                self.user_bias[user_id] + 
                self.item_bias[item_id] +
                np.dot(self.user_factors[user_id], self.item_factors[item_id]))
```

### Hybrid System Framework
```python
class HybridRecommender:
    def __init__(self, content_weight=0.3, collab_weight=0.5, knowledge_weight=0.2):
        self.content_recommender = ContentBasedRecommender()
        self.collab_recommender = MatrixFactorization()
        self.weights = {
            'content': content_weight,
            'collaborative': collab_weight,
            'knowledge': knowledge_weight
        }
        
    def fit(self, user_item_matrix, item_content, knowledge_rules):
        """Train all component recommenders"""
        self.content_recommender.fit(item_content)
        self.collab_recommender.fit(user_item_matrix)
        self.knowledge_rules = knowledge_rules
        
    def get_recommendations(self, user_id, user_context, top_k=10):
        """Generate hybrid recommendations"""
        # Get recommendations from each component
        content_recs = self.content_recommender.get_recommendations(
            self.get_user_liked_items(user_id), top_k * 2
        )
        
        collab_scores = {}
        for item_id in range(self.n_items):
            collab_scores[item_id] = self.collab_recommender.predict(user_id, item_id)
        
        knowledge_scores = self.apply_knowledge_rules(user_context)
        
        # Combine scores
        final_scores = {}
        all_items = set(range(self.n_items))
        
        for item_id in all_items:
            score = 0
            
            # Content-based score
            content_score = dict(content_recs).get(item_id, 0)
            score += self.weights['content'] * content_score
            
            # Collaborative score
            collab_score = collab_scores.get(item_id, 0)
            score += self.weights['collaborative'] * collab_score
            
            # Knowledge-based score
            knowledge_score = knowledge_scores.get(item_id, 0)
            score += self.weights['knowledge'] * knowledge_score
            
            final_scores[item_id] = score
        
        # Return top-k recommendations
        sorted_items = sorted(final_scores.items(), key=lambda x: x[1], reverse=True)
        return sorted_items[:top_k]
```

---

## Key Takeaways
1. **System Diversity**: Different recommendation approaches excel in different scenarios
2. **Hybrid Advantages**: Combining multiple approaches often yields better performance
3. **Context Importance**: Modern systems must be context-aware and adaptive
4. **Deep Learning Impact**: Neural approaches are becoming dominant for complex scenarios
5. **Trade-off Awareness**: Understanding performance, scalability, and interpretability trade-offs is crucial

---

**Next**: In day1_004.md, we'll explore key business questions and success metrics for evaluating search and recommendation systems.