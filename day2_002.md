# Day 2 - Part 2: TF-IDF and BM25 Scoring Algorithms

## Table of Contents
1. [From Boolean to Weighted Retrieval](#from-boolean-to-weighted-retrieval)
2. [Term Frequency (TF) Fundamentals](#term-frequency-fundamentals)
3. [Inverse Document Frequency (IDF) Theory](#inverse-document-frequency-theory)
4. [TF-IDF Scoring Model](#tf-idf-scoring-model)
5. [BM25 Algorithm Deep Dive](#bm25-algorithm)
6. [Document Length Normalization](#document-length-normalization)
7. [Variants and Extensions](#variants-and-extensions)
8. [Performance Analysis and Comparison](#performance-analysis)
9. [Study Questions](#study-questions)
10. [Code Examples](#code-examples)

---

## From Boolean to Weighted Retrieval

The transition from Boolean search to weighted retrieval represents one of the most significant advances in information retrieval, addressing the fundamental limitation of **binary relevance** in Boolean systems.

### Limitations of Boolean Search Revisited

#### **The All-or-Nothing Problem**
Boolean search suffers from the **coordination level problem**:
- Documents either match completely or not at all
- No mechanism to prefer documents that match "better"
- Users receive either too many results or too few

**Example Scenario**:
```
Query: "machine learning algorithms"
Boolean AND: Only documents containing ALL three terms
Boolean OR: Documents containing ANY term (potentially thousands)
```

Neither approach provides the desired middle ground of documents that are "mostly relevant."

#### **Lack of Term Weighting**
In Boolean search, all terms are treated equally:
```
Query: "artificial intelligence" vs "the intelligence"
Boolean model: Both "artificial" and "the" have equal importance
Reality: "artificial" is much more discriminative than "the"
```

### The Vector Space Model Foundation

#### **Documents as Vectors**
Represent documents as vectors in high-dimensional term space:
```
Document d = (w₁,d, w₂,d, w₃,d, ..., wₙ,d)
where wᵢ,d = weight of term i in document d
```

#### **Queries as Vectors**
Queries become vectors in the same space:
```
Query q = (w₁,q, w₂,q, w₃,q, ..., wₙ,q)
```

#### **Similarity Calculation**
Document relevance = similarity between query and document vectors:
```
similarity(q,d) = cos(θ) = (q⃗ · d⃗) / (||q⃗|| × ||d⃗||)
```

This geometric interpretation provides **ranked retrieval** where documents are ordered by similarity score.

---

## Term Frequency (TF) Fundamentals

Term Frequency addresses the intuitive notion that **documents mentioning query terms more frequently are more likely to be relevant**.

### Raw Term Frequency

#### **Basic Definition**
```
tf(t,d) = frequency of term t in document d
```

**Example**:
```
Document: "machine learning is a subset of artificial intelligence. 
          machine learning algorithms learn from data."

tf("machine", d) = 2
tf("learning", d) = 2  
tf("algorithm", d) = 1
tf("data", d) = 1
```

#### **Problems with Raw TF**
1. **Scale Issues**: Long documents have inflated term frequencies
2. **Diminishing Returns**: 10th occurrence less significant than 2nd occurrence
3. **Normalization**: Need comparable scores across documents of different lengths

### TF Normalization Schemes

#### **Log Normalization**
Addresses diminishing returns of additional term occurrences:
```
tf_log(t,d) = 1 + log(tf(t,d))    if tf(t,d) > 0
             = 0                   if tf(t,d) = 0
```

**Rationale**: Each additional occurrence has decreasing marginal value
**Effect**: tf=1→1, tf=10→2.4, tf=100→3.0

#### **Sublinear TF Scaling**
```
tf_sublinear(t,d) = 1 + log(tf(t,d))    if tf(t,d) > 0
                   = 0                   otherwise
```

#### **Double Normalization**
Prevents bias toward longer documents:
```
tf_norm(t,d) = 0.5 + 0.5 × (tf(t,d) / max_tf(d))
where max_tf(d) = maximum term frequency in document d
```

**Properties**:
- Values range from 0.5 to 1.0
- Most frequent term in document gets weight 1.0
- Reduces but doesn't eliminate length bias

#### **L2 Normalization**
Normalize by document vector length:
```
tf_l2(t,d) = tf(t,d) / sqrt(Σᵢ tf(i,d)²)
```

### Maximum Term Frequency Normalization

#### **Augmented Term Frequency**
```
tf_aug(t,d) = 0.5 + 0.5 × (tf(t,d) / max{tf(w,d) : w ∈ d})
```

**Advantages**:
- Reduces impact of document length
- Maintains relative term importance within document
- Bounded between 0.5 and 1.0

**Example Calculation**:
```
Document: "machine learning machine intelligence artificial intelligence"
tf("machine") = 2, tf("learning") = 1, tf("intelligence") = 2, tf("artificial") = 1
max_tf = 2

tf_aug("machine") = 0.5 + 0.5 × (2/2) = 1.0
tf_aug("learning") = 0.5 + 0.5 × (1/2) = 0.75
tf_aug("intelligence") = 0.5 + 0.5 × (2/2) = 1.0
tf_aug("artificial") = 0.5 + 0.5 × (1/2) = 0.75
```

---

## Inverse Document Frequency (IDF) Theory

IDF addresses the intuitive notion that **rare terms are more informative than common terms**.

### Information Theory Foundation

#### **Information Content**
From information theory, the information content of an event is:
```
I(event) = -log₂(P(event))
```

For terms in documents:
```
I(term t) = -log₂(P(term t appears in a random document))
          = -log₂(df(t) / N)
          = log₂(N / df(t))
```

Where:
- `df(t)` = document frequency of term t (number of documents containing t)
- `N` = total number of documents in collection

#### **IDF Formula**
```
idf(t) = log(N / df(t))
```

**Interpretation**:
- **High IDF**: Term appears in few documents (rare, discriminative)
- **Low IDF**: Term appears in many documents (common, less informative)

### IDF Variations

#### **Smooth IDF**
Prevents division by zero and adds smoothing:
```
idf_smooth(t) = log(N / (1 + df(t)))
```

#### **Max IDF**
Caps IDF at maximum value:
```
idf_max(t) = log(max(1, N / df(t)))
```

#### **Probabilistic IDF**
Based on probability of relevance:
```
idf_prob(t) = log((N - df(t)) / df(t))
```

### IDF Examples and Intuition

#### **Practical Example**
Collection of 10,000 documents:

```
Term "the": appears in 9,500 documents
idf("the") = log(10,000 / 9,500) = log(1.05) ≈ 0.021

Term "algorithm": appears in 100 documents  
idf("algorithm") = log(10,000 / 100) = log(100) ≈ 2.0

Term "tensorflow": appears in 10 documents
idf("tensorflow") = log(10,000 / 10) = log(1,000) ≈ 3.0
```

**Observation**: Specific technical terms receive much higher IDF weights than common words.

#### **Language-Specific IDF Patterns**
**Stop Words**: Articles, prepositions, conjunctions
- High document frequency → Low IDF → Low overall weight
- Naturally filtered out without explicit stop word lists

**Domain Terms**: Technical jargon, proper names
- Low document frequency → High IDF → High overall weight
- Automatically emphasized in retrieval

---

## TF-IDF Scoring Model

TF-IDF combines term frequency and inverse document frequency to create a comprehensive term weighting scheme.

### Basic TF-IDF Formula

#### **Standard Definition**
```
tf-idf(t,d) = tf(t,d) × idf(t)
            = tf(t,d) × log(N / df(t))
```

**Interpretation**: 
- Terms frequent in document BUT rare in collection get highest weights
- Terms common in collection get low weights regardless of document frequency
- Terms rare in document get low weights regardless of collection rarity

#### **Alternative Formulations**
**Log-normalized TF-IDF**:
```
tf-idf_log(t,d) = (1 + log(tf(t,d))) × log(N / df(t))    if tf(t,d) > 0
                = 0                                       otherwise
```

**Augmented TF-IDF**:
```
tf-idf_aug(t,d) = (0.5 + 0.5 × tf(t,d)/max_tf(d)) × log(N / df(t))
```

### Document Scoring with TF-IDF

#### **Query-Document Similarity**
For query q and document d:
```
score(q,d) = Σ(t∈q∩d) tf-idf(t,q) × tf-idf(t,d)
```

**Simplified Version** (query terms have unit weight):
```
score(q,d) = Σ(t∈q∩d) tf-idf(t,d)
```

#### **Cosine Similarity**
Normalize by vector lengths to handle document length bias:
```
cosine_sim(q,d) = (q⃗ · d⃗) / (||q⃗|| × ||d⃗||)
                = Σ(t∈q∩d) tf-idf(t,q) × tf-idf(t,d) / (||q⃗|| × ||d⃗||)
```

Where:
```
||d⃗|| = sqrt(Σ(t∈d) tf-idf(t,d)²)
||q⃗|| = sqrt(Σ(t∈q) tf-idf(t,q)²)
```

### TF-IDF Computational Example

#### **Example Setup**
**Collection**: 4 documents
**Query**: "machine learning"

```
Doc 1: "machine learning algorithms"
Doc 2: "machine learning is popular"  
Doc 3: "deep learning and machine vision"
Doc 4: "computer vision and pattern recognition"
```

#### **Step 1: Calculate Document Frequencies**
```
df("machine") = 3    (appears in docs 1, 2, 3)
df("learning") = 3   (appears in docs 1, 2, 3)
df("algorithms") = 1 (appears in doc 1)
df("is") = 1         (appears in doc 2)
df("popular") = 1    (appears in doc 2)
df("deep") = 1       (appears in doc 3)
...
```

#### **Step 2: Calculate IDF Values**
```
N = 4 (total documents)

idf("machine") = log(4/3) ≈ 0.125
idf("learning") = log(4/3) ≈ 0.125  
idf("algorithms") = log(4/1) ≈ 0.602
idf("deep") = log(4/1) ≈ 0.602
```

#### **Step 3: Calculate TF-IDF for Each Document**
**Document 1**: "machine learning algorithms"
```
tf("machine", d1) = 1, idf("machine") = 0.125
tf-idf("machine", d1) = 1 × 0.125 = 0.125

tf("learning", d1) = 1, idf("learning") = 0.125  
tf-idf("learning", d1) = 1 × 0.125 = 0.125

Sum for query terms: 0.125 + 0.125 = 0.25
```

**Document 2**: "machine learning is popular"
```
tf-idf("machine", d2) = 1 × 0.125 = 0.125
tf-idf("learning", d2) = 1 × 0.125 = 0.125
Sum for query terms: 0.125 + 0.125 = 0.25
```

**Document 3**: "deep learning and machine vision"
```
tf-idf("machine", d3) = 1 × 0.125 = 0.125
tf-idf("learning", d3) = 1 × 0.125 = 0.125
Sum for query terms: 0.125 + 0.125 = 0.25
```

**Document 4**: No query terms → Score = 0

**Ranking**: Documents 1, 2, 3 tie with score 0.25, Document 4 has score 0.

---

## BM25 Algorithm Deep Dive

BM25 (Best Matching 25) addresses several limitations of TF-IDF through more sophisticated term frequency saturation and document length normalization.

### Motivation for BM25

#### **TF-IDF Limitations**
1. **Unbounded TF Growth**: Even log-normalization allows unlimited growth
2. **Linear IDF**: Doesn't model diminishing returns optimally  
3. **Simple Length Normalization**: Cosine normalization may be too aggressive
4. **Parameter Rigidity**: No tunable parameters for different collections

#### **BM25 Design Principles**
1. **Term Frequency Saturation**: Additional occurrences have diminishing returns
2. **Tunable Parameters**: Allow optimization for specific collections
3. **Document Length Normalization**: Sophisticated length penalty
4. **Theoretical Foundation**: Based on probabilistic ranking principle

### BM25 Formula

#### **Complete BM25 Score**
```
BM25(q,d) = Σ(t∈q) idf(t) × (tf(t,d) × (k₁ + 1)) / (tf(t,d) + k₁ × (1 - b + b × |d|/avgdl))
```

Where:
- `tf(t,d)` = term frequency of t in document d
- `idf(t)` = inverse document frequency of term t
- `|d|` = document length (number of terms)
- `avgdl` = average document length in collection
- `k₁` = term frequency saturation parameter (typically 1.2-2.0)
- `b` = length normalization parameter (typically 0.75)

#### **Component Analysis**

**IDF Component**: 
```
idf(t) = log((N - df(t) + 0.5) / (df(t) + 0.5))
```
This is the **Robertson-Sparck Jones weight**, slightly different from standard IDF.

**TF Component with Saturation**:
```
tf_component = (tf(t,d) × (k₁ + 1)) / (tf(t,d) + k₁ × normalization_factor)
```

**Length Normalization Factor**:
```
normalization_factor = (1 - b + b × |d|/avgdl)
```

### Parameter Analysis

#### **k₁ Parameter (Term Frequency Saturation)**
Controls how quickly term frequency effect saturates:

```
For k₁ = 1.2:
tf=1 → tf_component ≈ 0.69
tf=2 → tf_component ≈ 0.92  
tf=5 → tf_component ≈ 1.26
tf=10 → tf_component ≈ 1.47
tf→∞ → tf_component → (k₁ + 1) = 2.2
```

**Effect**:
- **Low k₁ (0.5-1.0)**: Aggressive saturation, diminishing returns kick in quickly
- **High k₁ (2.0-3.0)**: Gentle saturation, allows more TF growth
- **k₁=0**: Equivalent to Boolean matching
- **k₁→∞**: Approaches linear TF (like TF-IDF)

#### **b Parameter (Length Normalization)**
Controls document length penalty:

```
b = 0: No length normalization
b = 1: Full length normalization  
b = 0.75: Typical compromise value
```

**Document Length Effects**:
```
Short document (|d| = 0.5 × avgdl):
normalization_factor = 1 - 0.75 + 0.75 × 0.5 = 0.625 (boost score)

Average document (|d| = avgdl):  
normalization_factor = 1 - 0.75 + 0.75 × 1.0 = 1.0 (neutral)

Long document (|d| = 2 × avgdl):
normalization_factor = 1 - 0.75 + 0.75 × 2.0 = 1.75 (penalize score)
```

### BM25 vs TF-IDF Comparison

#### **Term Frequency Handling**
```python
import matplotlib.pyplot as plt
import numpy as np

tf_values = np.arange(1, 21)

# TF-IDF (log normalization)
tf_idf_scores = 1 + np.log(tf_values)

# BM25 (k1 = 1.2)
k1 = 1.2
bm25_scores = (tf_values * (k1 + 1)) / (tf_values + k1)

# Plot comparison
plt.plot(tf_values, tf_idf_scores, label='TF-IDF (log)')
plt.plot(tf_values, bm25_scores, label='BM25 (k1=1.2)')
plt.xlabel('Term Frequency')
plt.ylabel('Score Component')
plt.legend()
plt.title('TF-IDF vs BM25 Term Frequency Scaling')
```

**Key Differences**:
- **TF-IDF**: Unbounded logarithmic growth
- **BM25**: Bounded asymptotic growth with tunable saturation

#### **Document Length Normalization**
**TF-IDF**: Uses cosine normalization (divides by document vector length)
**BM25**: Uses adaptive normalization based on average document length

---

## Document Length Normalization

Document length normalization is crucial for fair comparison between documents of different sizes.

### The Document Length Problem

#### **Why Length Matters**
1. **Longer documents have more terms**: Higher chance of containing query terms
2. **Repetition advantage**: More opportunities for term repetition
3. **Collection bias**: Retrieval systems may favor verbose documents

#### **Length Bias Examples**
```
Query: "machine learning"

Short Doc: "Machine learning overview" (3 terms)
Long Doc: "This comprehensive guide covers machine learning algorithms, 
          machine learning applications, machine learning theory..." (100+ terms)
```

Without normalization, the long document gets unfairly high scores due to repeated query terms.

### BM25 Length Normalization

#### **Adaptive Normalization Formula**
```
length_norm = 1 - b + b × (|d| / avgdl)
```

**Behavior Analysis**:
- **b = 0**: No length normalization (length_norm = 1)
- **b = 1**: Full normalization relative to average length
- **0 < b < 1**: Partial normalization (typical: b = 0.75)

#### **Impact on Different Document Lengths**
```python
def bm25_length_effect(doc_length, avg_length, b=0.75):
    return 1 - b + b * (doc_length / avg_length)

# Example: avgdl = 100 terms
avg_length = 100
lengths = [25, 50, 100, 200, 400]

for length in lengths:
    norm_factor = bm25_length_effect(length, avg_length)
    print(f"Length {length}: normalization factor = {norm_factor:.3f}")
    
# Output:
# Length 25: normalization factor = 0.438 (boost short docs)
# Length 50: normalization factor = 0.625  
# Length 100: normalization factor = 1.000 (neutral)
# Length 200: normalization factor = 1.750 (penalize long docs)
# Length 400: normalization factor = 3.250 (heavily penalize)
```

### Collection-Specific Tuning

#### **Parameter Optimization**
**Grid Search**: Test different (k₁, b) combinations
**Cross-Validation**: Use held-out relevance judgments
**Domain Adaptation**: Adjust parameters for specific collections

**Typical Parameter Ranges**:
```
k₁: 1.0 - 2.0 (web search: 1.2, academic papers: 1.6)
b: 0.5 - 1.0 (homogeneous collections: 0.5, diverse collections: 0.9)
```

#### **Collection Characteristics Impact**
**Homogeneous Collections** (similar document lengths):
- Lower b values (0.5-0.7)
- Length differences less meaningful

**Heterogeneous Collections** (varied document lengths):
- Higher b values (0.8-1.0)
- Length normalization more critical

---

## Variants and Extensions

### BM25 Variants

#### **BM25F (Field-Based BM25)**
Extends BM25 for documents with multiple fields (title, body, anchor text):

```
BM25F(q,d) = Σ(t∈q) idf(t) × (tf'(t,d) × (k₁ + 1)) / (tf'(t,d) + k₁)
```

Where:
```
tf'(t,d) = Σ(f∈fields) wf × tf(t,f) / (1 - bf + bf × |f|/avgfl_f)
```

**Field-specific parameters**:
- `wf` = field weight (title > body > anchor)
- `bf` = field length normalization
- `avgfl_f` = average length of field f

#### **BM25+ (BM25 Plus)**
Addresses potential negative TF components in BM25:

```
BM25+(q,d) = Σ(t∈q) idf(t) × (δ + (tf(t,d) × (k₁ + 1)) / (tf(t,d) + k₁ × length_norm))
```

Where δ (typically 1.0) ensures non-negative term contributions.

#### **BM11 (Binary Independence Model)**
Simplified version focusing on presence/absence rather than frequency:

```
BM11(q,d) = Σ(t∈q∩d) idf(t)
```

**Use Cases**: 
- Collections where TF is less informative
- Boolean-like queries with ranking

### TF-IDF Variants

#### **Okapi TF-IDF**
Incorporates BM25-style saturation into TF-IDF:

```
Okapi_TF-IDF = ((k₁ + 1) × tf) / (k₁ + tf) × log(N/df)
```

#### **Pivoted Length Normalization**
Alternative length normalization approach:

```
tf_pivoted = tf / (1 - s + s × |d|/avgdl)
```

Where s is the slope parameter (similar to b in BM25).

### Query Expansion Integration

#### **Relevance Feedback with BM25**
```python
def expand_query_bm25(original_query, relevant_docs, alpha=1.0, beta=0.75):
    """
    Expand query using BM25 scores from relevant documents
    """
    expanded_terms = {}
    
    for doc in relevant_docs:
        for term in doc.terms:
            if term not in original_query:
                bm25_score = calculate_bm25_term_score(term, doc)
                expanded_terms[term] = expanded_terms.get(term, 0) + bm25_score
    
    # Weight original query terms vs expansion terms
    final_query = {}
    for term in original_query:
        final_query[term] = alpha
    
    for term, score in expanded_terms.items():
        final_query[term] = beta * score
    
    return final_query
```

---

## Performance Analysis and Comparison

### Empirical Effectiveness

#### **Standard Test Collections**
**TREC Collections**: Text REtrieval Conference benchmarks
- **TREC-8**: 528,155 documents, 50 queries
- **Robust04**: 528,036 documents, 249 queries  
- **ClueWeb**: 1+ billion web pages

**Typical Performance Ranking**:
1. **BM25**: Generally best overall performance
2. **TF-IDF (cosine normalized)**: Close second
3. **BM25+**: Slight improvement over BM25
4. **Raw TF-IDF**: Significantly worse due to length bias

#### **Domain-Specific Performance**
**Web Search**: BM25 with k₁=1.2, b=0.75
**Academic Papers**: BM25 with k₁=1.6, b=0.6  
**Short Text (Twitter)**: Lower b values (0.3-0.5)
**Legal Documents**: Higher k₁ values (2.0+) due to terminology repetition

### Computational Complexity

#### **Time Complexity**
**Scoring Phase**: O(|q| × |d|) per query-document pair
- |q| = query length
- |d| = document length

**Index Lookup**: O(|q| × log|V|) for term dictionary lookup
- |V| = vocabulary size

#### **Space Complexity**
**TF-IDF Storage**: O(|V| × |D|) for full term-document matrix
**Inverted Index**: O(Σ|d|) - sum of all document lengths
**BM25 Precomputation**: Additional O(|D|) for document length statistics

#### **Optimization Strategies**
**Early Termination**: Stop scoring when remaining terms can't improve ranking
**Score Caching**: Cache frequently accessed TF-IDF/BM25 scores
**Approximate Scoring**: Use quantized or compressed scores for speed

### Practical Implementation Considerations

#### **Numerical Stability**
**Logarithm Domain**: Use log-space arithmetic to prevent underflow
**Smoothing**: Add small constants to prevent division by zero
**Precision**: Use appropriate floating-point precision for score calculations

#### **Update Efficiency**
**Incremental Updates**: Update IDF values when adding/removing documents
**Batch Processing**: Recompute statistics periodically rather than per update
**Distributed Computing**: Parallelize score computation across machines

---

## Study Questions

### Beginner Level
1. What problem does TF-IDF solve that Boolean search cannot handle?
2. Why do rare terms get higher IDF weights than common terms?
3. What is the intuition behind multiplying TF and IDF components?
4. How does BM25 handle term frequency saturation differently from TF-IDF?

### Intermediate Level
1. Compare different TF normalization schemes (raw, log, augmented). When would you use each?
2. Explain how the BM25 parameters k₁ and b affect retrieval behavior. How would you tune them?
3. Why might cosine similarity normalization be too aggressive for document length differences?
4. How do TF-IDF and BM25 handle the vocabulary mismatch problem differently?

### Advanced Level
1. Derive the BM25 formula from the Robertson-Sparck Jones probabilistic model.
2. Design an experiment to determine optimal BM25 parameters for a new document collection.
3. Analyze the computational trade-offs between exact BM25 scoring and approximate methods.
4. How would you extend BM25 to handle multi-field documents with different importance weights?

### Tricky Questions
1. **Paradox**: When might a document with lower TF-IDF scores be more relevant than one with higher scores?
2. **Parameter Sensitivity**: Why do small changes in BM25 parameters sometimes cause large ranking changes?
3. **Collection Dependency**: How does adding new documents to a collection affect existing TF-IDF scores?
4. **Saturation Effect**: Under what conditions might BM25's term frequency saturation hurt retrieval performance?

---

## Code Examples

### Complete TF-IDF Implementation
```python
import math
import re
from collections import defaultdict, Counter
from typing import List, Dict, Tuple

class TFIDFRetrieval:
    def __init__(self, tf_scheme='log', idf_scheme='standard', normalize=True):
        self.tf_scheme = tf_scheme
        self.idf_scheme = idf_scheme  
        self.normalize = normalize
        
        self.documents = {}
        self.term_doc_freq = defaultdict(int)
        self.document_frequencies = {}
        self.avg_doc_length = 0
        self.total_docs = 0
        
    def add_document(self, doc_id: int, content: str):
        """Add document to collection and update statistics"""
        terms = self.tokenize(content)
        self.documents[doc_id] = terms
        
        # Update document frequency for each unique term
        unique_terms = set(terms)
        for term in unique_terms:
            self.term_doc_freq[term] += 1
            
        # Update document length statistics
        self.total_docs += 1
        total_length = sum(len(doc) for doc in self.documents.values())
        self.avg_doc_length = total_length / self.total_docs
        
    def tokenize(self, text: str) -> List[str]:
        """Simple tokenization"""
        text = text.lower()
        terms = re.findall(r'\b\w+\b', text)
        return terms
    
    def calculate_tf(self, term: str, document: List[str]) -> float:
        """Calculate term frequency with specified scheme"""
        tf_raw = document.count(term)
        
        if tf_raw == 0:
            return 0.0
            
        if self.tf_scheme == 'raw':
            return float(tf_raw)
        elif self.tf_scheme == 'log':
            return 1.0 + math.log(tf_raw)
        elif self.tf_scheme == 'augmented':
            max_tf = max(Counter(document).values())
            return 0.5 + 0.5 * (tf_raw / max_tf)
        elif self.tf_scheme == 'boolean':
            return 1.0 if tf_raw > 0 else 0.0
        else:
            return float(tf_raw)
    
    def calculate_idf(self, term: str) -> float:
        """Calculate inverse document frequency"""
        df = self.term_doc_freq[term]
        
        if df == 0:
            return 0.0
            
        if self.idf_scheme == 'standard':
            return math.log(self.total_docs / df)
        elif self.idf_scheme == 'smooth':
            return math.log(self.total_docs / (1 + df))
        elif self.idf_scheme == 'max':
            return math.log(max(1, self.total_docs / df))
        elif self.idf_scheme == 'probabilistic':
            return math.log((self.total_docs - df) / df)
        else:
            return math.log(self.total_docs / df)
    
    def calculate_tfidf_vector(self, doc_id: int) -> Dict[str, float]:
        """Calculate TF-IDF vector for a document"""
        if doc_id not in self.documents:
            return {}
            
        document = self.documents[doc_id]
        tfidf_vector = {}
        
        for term in set(document):
            tf = self.calculate_tf(term, document)
            idf = self.calculate_idf(term)
            tfidf_vector[term] = tf * idf
            
        if self.normalize:
            # L2 normalization
            norm = math.sqrt(sum(score ** 2 for score in tfidf_vector.values()))
            if norm > 0:
                tfidf_vector = {term: score / norm for term, score in tfidf_vector.items()}
                
        return tfidf_vector
    
    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """Search documents using TF-IDF similarity"""
        query_terms = self.tokenize(query)
        query_vector = {}
        
        # Calculate query TF-IDF vector
        for term in set(query_terms):
            tf = self.calculate_tf(term, query_terms)
            idf = self.calculate_idf(term)
            query_vector[term] = tf * idf
            
        if self.normalize:
            # Normalize query vector
            norm = math.sqrt(sum(score ** 2 for score in query_vector.values()))
            if norm > 0:
                query_vector = {term: score / norm for term, score in query_vector.items()}
        
        # Calculate similarity scores
        scores = []
        for doc_id in self.documents:
            doc_vector = self.calculate_tfidf_vector(doc_id)
            similarity = self.cosine_similarity(query_vector, doc_vector)
            if similarity > 0:
                scores.append((doc_id, similarity))
        
        # Sort by similarity score
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_k]
    
    def cosine_similarity(self, vec1: Dict[str, float], vec2: Dict[str, float]) -> float:
        """Calculate cosine similarity between two vectors"""
        common_terms = set(vec1.keys()) & set(vec2.keys())
        
        if not common_terms:
            return 0.0
            
        dot_product = sum(vec1[term] * vec2[term] for term in common_terms)
        
        if not self.normalize:
            # Calculate norms if vectors aren't normalized
            norm1 = math.sqrt(sum(score ** 2 for score in vec1.values()))
            norm2 = math.sqrt(sum(score ** 2 for score in vec2.values()))
            
            if norm1 == 0 or norm2 == 0:
                return 0.0
                
            return dot_product / (norm1 * norm2)
        else:
            # Vectors are already normalized
            return dot_product
```

### BM25 Implementation
```python
class BM25Retrieval:
    def __init__(self, k1=1.2, b=0.75):
        self.k1 = k1
        self.b = b
        
        self.documents = {}
        self.term_doc_freq = defaultdict(int)
        self.doc_lengths = {}
        self.avg_doc_length = 0
        self.total_docs = 0
        
    def add_document(self, doc_id: int, content: str):
        """Add document to collection"""
        terms = self.tokenize(content)
        self.documents[doc_id] = terms
        self.doc_lengths[doc_id] = len(terms)
        
        # Update document frequency
        unique_terms = set(terms)
        for term in unique_terms:
            self.term_doc_freq[term] += 1
            
        # Update average document length
        self.total_docs += 1
        total_length = sum(self.doc_lengths.values())
        self.avg_doc_length = total_length / self.total_docs
        
    def tokenize(self, text: str) -> List[str]:
        """Simple tokenization"""
        text = text.lower()
        terms = re.findall(r'\b\w+\b', text)
        return terms
    
    def calculate_idf(self, term: str) -> float:
        """Calculate Robertson-Sparck Jones IDF"""
        df = self.term_doc_freq[term]
        
        if df == 0:
            return 0.0
            
        # RSJ IDF formula
        return math.log((self.total_docs - df + 0.5) / (df + 0.5))
    
    def calculate_bm25_score(self, query_terms: List[str], doc_id: int) -> float:
        """Calculate BM25 score for a document"""
        if doc_id not in self.documents:
            return 0.0
            
        document = self.documents[doc_id]
        doc_length = self.doc_lengths[doc_id]
        score = 0.0
        
        # Calculate length normalization factor
        length_norm = 1 - self.b + self.b * (doc_length / self.avg_doc_length)
        
        for term in query_terms:
            if term in document:
                # Calculate term frequency in document
                tf = document.count(term)
                
                # Calculate IDF
                idf = self.calculate_idf(term)
                
                # Calculate BM25 component for this term
                tf_component = (tf * (self.k1 + 1)) / (tf + self.k1 * length_norm)
                
                score += idf * tf_component
                
        return score
    
    def search(self, query: str, top_k: int = 10) -> List[Tuple[int, float]]:
        """Search using BM25 scoring"""
        query_terms = self.tokenize(query)
        
        scores = []
        for doc_id in self.documents:
            score = self.calculate_bm25_score(query_terms, doc_id)
            if score > 0:
                scores.append((doc_id, score))
        
        # Sort by BM25 score
        scores.sort(key=lambda x: x[1], reverse=True)
        return scores[:top_k]
    
    def explain_score(self, query: str, doc_id: int) -> Dict:
        """Explain BM25 score calculation for debugging"""
        query_terms = self.tokenize(query)
        
        if doc_id not in self.documents:
            return {"error": "Document not found"}
            
        document = self.documents[doc_id]
        doc_length = self.doc_lengths[doc_id]
        length_norm = 1 - self.b + self.b * (doc_length / self.avg_doc_length)
        
        explanation = {
            "doc_id": doc_id,
            "doc_length": doc_length,
            "avg_doc_length": self.avg_doc_length,
            "length_normalization": length_norm,
            "term_scores": {},
            "total_score": 0.0
        }
        
        total_score = 0.0
        
        for term in query_terms:
            tf = document.count(term)
            idf = self.calculate_idf(term)
            df = self.term_doc_freq[term]
            
            if tf > 0:
                tf_component = (tf * (self.k1 + 1)) / (tf + self.k1 * length_norm)
                term_score = idf * tf_component
                total_score += term_score
                
                explanation["term_scores"][term] = {
                    "tf": tf,
                    "df": df,
                    "idf": idf,
                    "tf_component": tf_component,
                    "term_score": term_score
                }
            else:
                explanation["term_scores"][term] = {
                    "tf": 0,
                    "df": df,
                    "idf": idf,
                    "tf_component": 0,
                    "term_score": 0
                }
        
        explanation["total_score"] = total_score
        return explanation

# Example usage and comparison
def compare_retrieval_methods():
    # Create sample documents
    documents = {
        1: "machine learning algorithms for data analysis",
        2: "deep learning neural networks and artificial intelligence", 
        3: "machine learning is a subset of artificial intelligence",
        4: "data science uses machine learning and statistical methods",
        5: "artificial neural networks in deep learning applications"
    }
    
    # Initialize both systems
    tfidf_system = TFIDFRetrieval(tf_scheme='log', normalize=True)
    bm25_system = BM25Retrieval(k1=1.2, b=0.75)
    
    # Add documents to both systems
    for doc_id, content in documents.items():
        tfidf_system.add_document(doc_id, content)
        bm25_system.add_document(doc_id, content)
    
    # Compare results for a query
    query = "machine learning algorithms"
    
    print("TF-IDF Results:")
    tfidf_results = tfidf_system.search(query, top_k=5)
    for doc_id, score in tfidf_results:
        print(f"  Doc {doc_id}: {score:.4f} - {documents[doc_id]}")
    
    print("\nBM25 Results:")
    bm25_results = bm25_system.search(query, top_k=5)
    for doc_id, score in bm25_results:
        print(f"  Doc {doc_id}: {score:.4f} - {documents[doc_id]}")
    
    # Show detailed BM25 explanation for top result
    if bm25_results:
        top_doc_id = bm25_results[0][0]
        explanation = bm25_system.explain_score(query, top_doc_id)
        print(f"\nBM25 Score Explanation for Doc {top_doc_id}:")
        print(f"Document Length: {explanation['doc_length']}")
        print(f"Length Normalization: {explanation['length_normalization']:.4f}")
        for term, details in explanation['term_scores'].items():
            print(f"  {term}: TF={details['tf']}, IDF={details['idf']:.4f}, Score={details['term_score']:.4f}")
        print(f"Total Score: {explanation['total_score']:.4f}")

if __name__ == "__main__":
    compare_retrieval_methods()
```

---

## Key Takeaways
1. **Foundation**: TF-IDF introduced weighted retrieval, addressing Boolean search limitations
2. **Component Balance**: Successful term weighting balances local relevance (TF) with global discriminativeness (IDF)
3. **BM25 Superiority**: BM25's parameter tuning and saturation effects generally outperform TF-IDF
4. **Length Normalization**: Critical for fair comparison between documents of different lengths
5. **Parameter Sensitivity**: BM25 parameters (k₁, b) significantly impact retrieval effectiveness and should be tuned per collection

---

**Next**: In day2_003.md, we'll explore document indexing and inverted index structures that enable efficient implementation of TF-IDF and BM25 scoring at scale.