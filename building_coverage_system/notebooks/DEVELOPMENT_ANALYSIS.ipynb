{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Coverage System - Development Analysis\n",
    "\n",
    "This notebook provides development and analysis tools for the building coverage system.\n",
    "It includes data exploration, model testing, and performance analysis capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add project root to path\n",
    "project_root = os.path.abspath('..')\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('husl')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"Development environment setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration and Test Data Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration\n",
    "from config import get_config_manager, get_config\n",
    "\n",
    "# Set development environment\n",
    "os.environ['ENVIRONMENT'] = 'development'\n",
    "os.environ['DEV_OVERRIDE'] = 'local'\n",
    "\n",
    "config_manager = get_config_manager()\n",
    "config = get_config()\n",
    "\n",
    "print(\"Environment:\", config_manager.environment)\n",
    "print(\"Development mode:\", config_manager.is_development())\n",
    "print(\"Max workers:\", config['pipeline']['parallel_processing']['max_workers'])\n",
    "\n",
    "# Display key configuration sections\n",
    "print(\"\\nPipeline Configuration:\")\n",
    "for key, value in config['pipeline'].items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate synthetic test data for development\n",
    "def generate_test_claims(n_claims=100):\n",
    "    \"\"\"Generate synthetic claim data for testing.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Building-related damage descriptions\n",
    "    building_damages = [\n",
    "        \"Foundation damage due to water intrusion affecting structural integrity\",\n",
    "        \"Roof damage from storm with extensive building material deterioration\",\n",
    "        \"Wall damage and building structural problems from flooding event\",\n",
    "        \"Floor damage with structural building components affected by water\",\n",
    "        \"Ceiling damage requiring building structural repairs and reconstruction\",\n",
    "        \"Exterior wall damage affecting building envelope and structure\",\n",
    "        \"Interior structural damage to load-bearing walls and supports\",\n",
    "        \"Building foundation settling causing structural damage throughout\",\n",
    "        \"Roof collapse requiring complete building structural assessment\",\n",
    "        \"Fire damage to building structure requiring extensive reconstruction\"\n",
    "    ]\n",
    "    \n",
    "    # Non-building damage descriptions\n",
    "    non_building_damages = [\n",
    "        \"Vehicle damage in parking lot with no building involvement\",\n",
    "        \"Landscaping damage from storm with no structural building impact\",\n",
    "        \"Personal property damage inside building but no structural damage\",\n",
    "        \"Equipment damage with no building structural involvement\",\n",
    "        \"Contents damage due to water but building structure unaffected\",\n",
    "        \"Theft of personal property with no building damage\",\n",
    "        \"Business interruption claim with no physical building damage\",\n",
    "        \"Liability claim with no building or property damage\"\n",
    "    ]\n",
    "    \n",
    "    # Generate claims\n",
    "    claims_data = []\n",
    "    building_ratio = 0.6  # 60% building-related claims\n",
    "    \n",
    "    for i in range(n_claims):\n",
    "        claim_no = f\"DEV{i:06d}\"\n",
    "        \n",
    "        # Choose damage type\n",
    "        if np.random.random() < building_ratio:\n",
    "            damage_text = np.random.choice(building_damages)\n",
    "            lob_cd = np.random.choice(['15', '17'])  # Building LOBs\n",
    "            loss_desc = 'Building damage'\n",
    "            expected_coverage = 'BUILDING COVERAGE'\n",
    "        else:\n",
    "            damage_text = np.random.choice(non_building_damages)\n",
    "            lob_cd = np.random.choice(['15', '17', '18'])\n",
    "            loss_desc = 'Other damage'\n",
    "            expected_coverage = 'NO BUILDING COVERAGE'\n",
    "        \n",
    "        # Add some variation to the text\n",
    "        variation = f\" Additional details for claim {i} with specific circumstances and location information.\"\n",
    "        full_text = damage_text + variation\n",
    "        \n",
    "        # Generate dates\n",
    "        loss_date = datetime.now() - timedelta(days=np.random.randint(1, 365))\n",
    "        report_date = loss_date + timedelta(days=np.random.randint(0, 30))\n",
    "        \n",
    "        claims_data.append({\n",
    "            'CLAIMNO'; claim_no,\n",
    "            'CLAIMKEY'; f\"KEY{i:06d}\",\n",
    "            'clean_FN_TEXT'; full_text,\n",
    "            'LOBCD'; lob_cd,\n",
    "            'LOSSDESC'; loss_desc,\n",
    "            'LOSSDT'; loss_date,\n",
    "            'REPORTEDDT'; report_date,\n",
    "            'expected_coverage'; expected_coverage  # For validation\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(claims_data)\n",
    "\n",
    "# Generate test data\n",
    "test_claims = generate_test_claims(100)\n",
    "print(f\"Generated {len(test_claims)} test claims\")\n",
    "print(f\"Building coverage claims: {(test_claims['expected_coverage'] == 'BUILDING COVERAGE').sum()}\")\n",
    "print(f\"Non-building claims: {(test_claims['expected_coverage'] == 'NO BUILDING COVERAGE').sum()}\")\n",
    "\n",
    "# Display sample\n",
    "test_claims.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Exploration and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# Distribution of LOB codes\n",
    "test_claims['LOBCD'].value_counts().plot(kind='bar', ax=axes[0,0])\n",
    "axes[0,0].set_title('Distribution of LOB Codes')\n",
    "axes[0,0].set_xlabel('LOB Code')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "\n",
    "# Text length distribution\n",
    "text_lengths = test_claims['clean_FN_TEXT'].str.len()\n",
    "text_lengths.hist(bins=20, ax=axes[0,1])\n",
    "axes[0,1].set_title('Distribution of Text Lengths')\n",
    "axes[0,1].set_xlabel('Text Length (characters)')\n",
    "axes[0,1].set_ylabel('Frequency')\n",
    "\n",
    "# Expected coverage distribution\n",
    "test_claims['expected_coverage'].value_counts().plot(kind='pie', ax=axes[1,0], autopct='%1.1f%%')\n",
    "axes[1,0].set_title('Expected Coverage Distribution')\n",
    "\n",
    "# Reporting lag analysis\n",
    "test_claims['reporting_lag'] = (test_claims['REPORTEDDT'] - test_claims['LOSSDT']).dt.days\n",
    "test_claims['reporting_lag'].hist(bins=15, ax=axes[1,1])\n",
    "axes[1,1].set_title('Reporting Lag Distribution')\n",
    "axes[1,1].set_xlabel('Days')\n",
    "axes[1,1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Summary statistics\n",
    "print(\"\\nSummary Statistics:\")\n",
    "print(f\"Average text length: {text_lengths.mean():.1f} characters\")\n",
    "print(f\"Median reporting lag: {test_claims['reporting_lag'].median():.1f} days\")\n",
    "print(f\"Date range: {test_claims['LOSSDT'].min().date()} to {test_claims['LOSSDT'].max().date()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Pipeline Component Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test individual pipeline components\n",
    "from modules.core.pipeline import CoveragePipeline\n",
    "from modules.core.monitor import PerformanceMonitor\n",
    "from unittest.mock import Mock\n",
    "\n",
    "# Create performance monitor\n",
    "monitor = PerformanceMonitor()\n",
    "\n",
    "# Test performance monitoring\n",
    "print(\"Testing Performance Monitor:\")\n",
    "monitor.start_operation('test_analysis')\n",
    "\n",
    "# Simulate some processing\n",
    "import time\n",
    "time.sleep(0.1)\n",
    "\n",
    "duration = monitor.end_operation('test_analysis')\n",
    "monitor.add_custom_metric('test_claims_processed', len(test_claims), 'analysis')\n",
    "\n",
    "print(f\"Operation duration: {duration:.3f} seconds\")\n",
    "\n",
    "# Get performance summary\n",
    "summary = monitor.get_summary()\n",
    "print(\"\\nPerformance Summary:\")\n",
    "for key, value in summary.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test pre-processing hooks\n",
    "from custom_hooks.pre_processing import pre_process\n",
    "\n",
    "print(\"Testing Pre-processing Hook:\")\n",
    "\n",
    "# Test with sample data\n",
    "sample_claims = test_claims.head(10).copy()\n",
    "print(f\"Input claims: {len(sample_claims)}\")\n",
    "\n",
    "# Apply pre-processing\n",
    "processed_claims = pre_process(sample_claims)\n",
    "print(f\"Output claims: {len(processed_claims)}\")\n",
    "\n",
    "# Check added columns\n",
    "added_columns = set(processed_claims.columns) - set(sample_claims.columns)\n",
    "print(f\"Added columns: {list(added_columns)}\")\n",
    "\n",
    "# Display processing metrics\n",
    "if 'processing_confidence' in processed_claims.columns:\n",
    "    print(f\"Average processing confidence: {processed_claims['processing_confidence'].mean():.3f}\")\n",
    "    \n",
    "if 'complexity_score' in processed_claims.columns:\n",
    "    print(f\"Average complexity score: {processed_claims['complexity_score'].mean():.3f}\")\n",
    "\n",
    "# Show sample processed data\n",
    "print(\"\\nSample processed data:\")\n",
    "processed_claims[['CLAIMNO', 'processing_confidence', 'data_quality_flags']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Mock Pipeline Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock pipeline execution for development testing\n",
    "from unittest.mock import patch\n",
    "\n",
    "def mock_pipeline_execution(claims_df):\n",
    "    \"\"\"Mock pipeline execution for testing purposes.\"\"\"\n",
    "    \n",
    "    print(\"Starting mock pipeline execution...\")\n",
    "    \n",
    "    # Mock RAG processing results\n",
    "    rag_results = []\n",
    "    \n",
    "    for _, claim in claims_df.iterrows():\n",
    "        # Simple rule-based mock for testing\n",
    "        text = claim['clean_FN_TEXT'].lower()\n",
    "        \n",
    "        building_keywords = ['building', 'structure', 'foundation', 'roof', 'wall', 'floor', 'ceiling']\n",
    "        keyword_matches = sum(1 for keyword in building_keywords if keyword in text)\n",
    "        \n",
    "        if keyword_matches >= 2:\n",
    "            prediction = 'BUILDING COVERAGE'\n",
    "            confidence = min(0.95, 0.6 + (keyword_matches * 0.1))\n",
    "            summary = f\"Building coverage recommended due to {keyword_matches} structural keywords\"\n",
    "        elif keyword_matches >= 1:\n",
    "            prediction = 'BUILDING COVERAGE'\n",
    "            confidence = 0.5 + (keyword_matches * 0.05)\n",
    "            summary = f\"Possible building coverage with {keyword_matches} keyword(s)\"\n",
    "        else:\n",
    "            prediction = 'NO BUILDING COVERAGE'\n",
    "            confidence = 0.8\n",
    "            summary = \"No building-related keywords found\"\n",
    "        \n",
    "        rag_results.append({\n",
    "            'CLAIMNO': claim['CLAIMNO'],\n",
    "            'prediction': prediction,\n",
    "            'confidence': confidence,\n",
    "            'summary': summary,\n",
    "            'keyword_matches': keyword_matches\n",
    "        })\n",
    "    \n",
    "    rag_df = pd.DataFrame(rag_results)\n",
    "    \n",
    "    # Merge with original data\n",
    "    final_results = claims_df.merge(rag_df, on='CLAIMNO', how='left')\n",
    "    \n",
    "    print(f\"Processed {len(final_results)} claims\")\n",
    "    \n",
    "    return final_results\n",
    "\n",
    "# Execute mock pipeline\n",
    "test_sample = test_claims.head(20)\n",
    "mock_results = mock_pipeline_execution(test_sample)\n",
    "\n",
    "# Display results\n",
    "print(\"\\nMock Pipeline Results:\")\n",
    "print(f\"Total claims processed: {len(mock_results)}\")\n",
    "print(f\"Building coverage predictions: {(mock_results['prediction'] == 'BUILDING COVERAGE').sum()}\")\n",
    "print(f\"Average confidence: {mock_results['confidence'].mean():.3f}\")\n",
    "\n",
    "# Show sample results\n",
    "display_columns = ['CLAIMNO', 'expected_coverage', 'prediction', 'confidence', 'keyword_matches']\n",
    "mock_results[display_columns].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Accuracy Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze mock prediction accuracy\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Calculate accuracy metrics\n",
    "y_true = mock_results['expected_coverage']\n",
    "y_pred = mock_results['prediction']\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Mock Pipeline Accuracy: {accuracy:.3f}\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_true, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['No Coverage', 'Building Coverage'],\n",
    "            yticklabels=['No Coverage', 'Building Coverage'])\n",
    "plt.title('Mock Pipeline Confusion Matrix')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "# Confidence distribution analysis\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "mock_results['confidence'].hist(bins=20, alpha=0.7)\n",
    "plt.title('Confidence Score Distribution')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "building_conf = mock_results[mock_results['prediction'] == 'BUILDING COVERAGE']['confidence']\n",
    "no_building_conf = mock_results[mock_results['prediction'] == 'NO BUILDING COVERAGE']['confidence']\n",
    "\n",
    "plt.hist(building_conf, bins=15, alpha=0.7, label='Building Coverage', color='blue')\n",
    "plt.hist(no_building_conf, bins=15, alpha=0.7, label='No Building Coverage', color='red')\n",
    "plt.title('Confidence by Prediction Type')\n",
    "plt.xlabel('Confidence')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance benchmarking with different data sizes\n",
    "import time\n",
    "\n",
    "def benchmark_pipeline(sizes=[10, 50, 100, 500]):\n",
    "    \"\"\"Benchmark pipeline performance with different data sizes.\"\"\"\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for size in sizes:\n",
    "        print(f\"Benchmarking with {size} claims...\")\n",
    "        \n",
    "        # Generate data of specified size\n",
    "        test_data = generate_test_claims(size)\n",
    "        \n",
    "        # Measure processing time\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Run mock pipeline\n",
    "        processed_data = mock_pipeline_execution(test_data)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        processing_time = end_time - start_time\n",
    "        \n",
    "        # Calculate metrics\n",
    "        throughput = size / processing_time if processing_time > 0 else 0\n",
    "        \n",
    "        results.append({\n",
    "            'size': size,\n",
    "            'processing_time': processing_time,\n",
    "            'throughput': throughput,\n",
    "            'avg_time_per_claim': processing_time / size if size > 0 else 0\n",
    "        })\n",
    "        \n",
    "        print(f\"  Processing time: {processing_time:.3f} seconds\")\n",
    "        print(f\"  Throughput: {throughput:.1f} claims/second\")\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Run benchmarks\n",
    "benchmark_results = benchmark_pipeline([10, 50, 100, 200])\n",
    "\n",
    "# Display results\n",
    "print(\"\\nBenchmark Results:\")\n",
    "benchmark_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize performance results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Processing time vs data size\n",
    "axes[0].plot(benchmark_results['size'], benchmark_results['processing_time'], 'b-o')\n",
    "axes[0].set_xlabel('Number of Claims')\n",
    "axes[0].set_ylabel('Processing Time (seconds)')\n",
    "axes[0].set_title('Processing Time vs Data Size')\n",
    "axes[0].grid(True)\n",
    "\n",
    "# Throughput analysis\n",
    "axes[1].plot(benchmark_results['size'], benchmark_results['throughput'], 'g-o')\n",
    "axes[1].set_xlabel('Number of Claims')\n",
    "axes[1].set_ylabel('Throughput (claims/second)')\n",
    "axes[1].set_title('Throughput vs Data Size')\n",
    "axes[1].grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Performance summary\n",
    "print(\"Performance Summary:\")\n",
    "print(f\"Best throughput: {benchmark_results['throughput'].max():.1f} claims/second\")\n",
    "print(f\"Average time per claim: {benchmark_results['avg_time_per_claim'].mean():.4f} seconds\")\n",
    "print(f\"Scalability factor: {benchmark_results['processing_time'].iloc[-1] / benchmark_results['processing_time'].iloc[0]:.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Configuration Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test configuration management\n",
    "from config import ConfigManager\n",
    "\n",
    "print(\"Testing Configuration Management:\")\n",
    "\n",
    "# Test different environments\n",
    "dev_config = ConfigManager('development')\n",
    "prod_config = ConfigManager('production')\n",
    "\n",
    "print(\"Development vs Production Comparison:\")\n",
    "comparison_items = [\n",
    "    ('Max Workers', 'pipeline.parallel_processing.max_workers'),\n",
    "    ('Batch Size', 'pipeline.parallel_processing.batch_size'),\n",
    "    ('Log Level', 'logging.level'),\n",
    "    ('Debug Mode', 'debug')\n",
    "]\n",
    "\n",
    "for item_name, config_path in comparison_items:\n",
    "    dev_value = dev_config.get_config()\n",
    "    prod_value = prod_config.get_config()\n",
    "    \n",
    "    # Navigate config path\n",
    "    for key in config_path.split('.'):\n",
    "        dev_value = dev_value.get(key, 'N/A')\n",
    "        prod_value = prod_value.get(key, 'N/A')\n",
    "    \n",
    "    print(f\"  {item_name}:\")\n",
    "    print(f\"    Development: {dev_value}\")\n",
    "    print(f\"    Production:  {prod_value}\")\n",
    "    print()\n",
    "\n",
    "# Test SQL queries\n",
    "dev_queries = dev_config.get_sql_queries()\n",
    "prod_queries = prod_config.get_sql_queries()\n",
    "\n",
    "print(\"SQL Query Comparison:\")\n",
    "for query_name in dev_queries.keys():\n",
    "    dev_length = len(dev_queries[query_name])\n",
    "    prod_length = len(prod_queries.get(query_name, ''))\n",
    "    print(f\"  {query_name}: Dev({dev_length} chars) vs Prod({prod_length} chars)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Development Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate development recommendations based on analysis\n",
    "def generate_recommendations(mock_results, benchmark_results):\n",
    "    \"\"\"Generate development recommendations based on analysis results.\"\"\"\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Accuracy recommendations\n",
    "    accuracy = accuracy_score(mock_results['expected_coverage'], mock_results['prediction'])\n",
    "    if accuracy < 0.8:\n",
    "        recommendations.append(\n",
    "            f\"🔴 Low accuracy ({accuracy:.3f}): Consider improving the RAG model or rule-based logic\"\n",
    "        )\n",
    "    elif accuracy < 0.9:\n",
    "        recommendations.append(\n",
    "            f\"🟡 Moderate accuracy ({accuracy:.3f}): Fine-tune the model for better performance\"\n",
    "        )\n",
    "    else:\n",
    "        recommendations.append(\n",
    "            f\"🟢 Good accuracy ({accuracy:.3f}): Current model performs well\"\n",
    "        )\n",
    "    \n",
    "    # Performance recommendations\n",
    "    best_throughput = benchmark_results['throughput'].max()\n",
    "    if best_throughput < 10:\n",
    "        recommendations.append(\n",
    "            f\"🔴 Low throughput ({best_throughput:.1f} claims/sec): Consider parallel processing optimization\"\n",
    "        )\n",
    "    elif best_throughput < 50:\n",
    "        recommendations.append(\n",
    "            f\"🟡 Moderate throughput ({best_throughput:.1f} claims/sec): Room for performance improvement\"\n",
    "        )\n",
    "    else:\n",
    "        recommendations.append(\n",
    "            f\"🟢 Good throughput ({best_throughput:.1f} claims/sec): Performance is adequate\"\n",
    "        )\n",
    "    \n",
    "    # Confidence recommendations\n",
    "    avg_confidence = mock_results['confidence'].mean()\n",
    "    low_confidence_count = (mock_results['confidence'] < 0.7).sum()\n",
    "    \n",
    "    if avg_confidence < 0.7:\n",
    "        recommendations.append(\n",
    "            f\"🔴 Low average confidence ({avg_confidence:.3f}): Review model confidence calibration\"\n",
    "        )\n",
    "    \n",
    "    if low_confidence_count > len(mock_results) * 0.2:\n",
    "        recommendations.append(\n",
    "            f\"🟡 {low_confidence_count} claims with low confidence: Consider manual review process\"\n",
    "        )\n",
    "    \n",
    "    # Data quality recommendations\n",
    "    text_lengths = mock_results['clean_FN_TEXT'].str.len()\n",
    "    short_texts = (text_lengths < 100).sum()\n",
    "    \n",
    "    if short_texts > 0:\n",
    "        recommendations.append(\n",
    "            f\"🟡 {short_texts} claims with short text: May need additional context for better predictions\"\n",
    "        )\n",
    "    \n",
    "    # Development process recommendations\n",
    "    recommendations.extend([\n",
    "        \"📋 Implement comprehensive unit tests for all pipeline components\",\n",
    "        \"📋 Set up continuous integration with automated testing\",\n",
    "        \"📋 Create integration tests with real data samples\",\n",
    "        \"📋 Implement logging and monitoring for production deployment\",\n",
    "        \"📋 Consider A/B testing framework for model improvements\"\n",
    "    ])\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Generate and display recommendations\n",
    "recommendations = generate_recommendations(mock_results, benchmark_results)\n",
    "\n",
    "print(\"🚀 Development Recommendations:\")\n",
    "print(\"=\" * 50)\n",
    "for i, rec in enumerate(recommendations, 1):\n",
    "    print(f\"{i:2d}. {rec}\")\n",
    "\n",
    "print(\"\\n📊 Summary Statistics:\")\n",
    "print(f\"  Total test claims: {len(test_claims)}\")\n",
    "print(f\"  Mock accuracy: {accuracy_score(mock_results['expected_coverage'], mock_results['prediction']):.3f}\")\n",
    "print(f\"  Best throughput: {benchmark_results['throughput'].max():.1f} claims/second\")\n",
    "print(f\"  Average confidence: {mock_results['confidence'].mean():.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export analysis results for further use\n",
    "import os\n",
    "\n",
    "# Create output directory\n",
    "output_dir = '../output/development_analysis'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Export datasets\n",
    "test_claims.to_csv(f'{output_dir}/test_claims.csv', index=False)\n",
    "mock_results.to_csv(f'{output_dir}/mock_results.csv', index=False)\n",
    "benchmark_results.to_csv(f'{output_dir}/benchmark_results.csv', index=False)\n",
    "\n",
    "# Export analysis summary\n",
    "analysis_summary = {\n",
    "    'timestamp': datetime.now().isoformat(),\n",
    "    'test_claims_count': len(test_claims),\n",
    "    'mock_accuracy': accuracy_score(mock_results['expected_coverage'], mock_results['prediction']),\n",
    "    'average_confidence': mock_results['confidence'].mean(),\n",
    "    'best_throughput': benchmark_results['throughput'].max(),\n",
    "    'recommendations_count': len(recommendations)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(f'{output_dir}/analysis_summary.json', 'w') as f:\n",
    "    json.dump(analysis_summary, f, indent=2, default=str)\n",
    "\n",
    "# Export recommendations\n",
    "with open(f'{output_dir}/recommendations.txt', 'w') as f:\n",
    "    f.write('Development Analysis Recommendations\\n')\n",
    "    f.write('=' * 50 + '\\n')\n",
    "    f.write(f'Generated: {datetime.now()}\\n\\n')\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        f.write(f'{i:2d}. {rec}\\n')\n",
    "\n",
    "print(f\"Analysis results exported to {output_dir}/\")\n",
    "print(\"Files created:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    print(f\"  - {file}\")\n",
    "\n",
    "print(\"\\n✅ Development Analysis Complete!\")\n",
    "print(\"Use these results to guide further development and testing.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}