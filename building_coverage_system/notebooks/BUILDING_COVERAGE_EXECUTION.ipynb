{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Coverage Match System - Main Execution Notebook\n",
    "\n",
    "This notebook provides the main execution interface for the building coverage match system,\n",
    "combining the original functionality with advanced parallel processing and custom hooks.\n",
    "\n",
    "## Features\n",
    "- **Multi-source Data Loading**: Parallel loading from AIP, Atlas, and Snowflake\n",
    "- **Advanced RAG Processing**: Multi-threaded claim analysis\n",
    "- **Custom Hooks**: Pre and post-processing customization\n",
    "- **Performance Monitoring**: Built-in performance tracking\n",
    "- **Comprehensive Validation**: Data quality checks throughout the pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Original system imports (backward compatibility)\n",
    "from coverage_configs.src.environment import DatabricksEnv\n",
    "from coverage_configs.src.credentials import get_credentials\n",
    "from coverage_configs.src.sql import get_sql_query\n",
    "from coverage_configs.src.rag_params import get_rag_params\n",
    "from coverage_configs.src.prompts import get_prompt\n",
    "\n",
    "# New modular system imports\n",
    "from modules.core.pipeline import CoveragePipeline\n",
    "from modules.core.loader import ConfigLoader\n",
    "from modules.core.monitor import PipelinePerformanceMonitor\n",
    "from modules.core.validator import PipelineValidator\n",
    "\n",
    "print(\"Building Coverage Match System - Enhanced with Parallel Processing\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Execution started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Configuration Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original databricks configuration (modify with your actual values)\n",
    "databricks_dict = {\n",
    "    'environment': 'production',\n",
    "    'region': 'us-east-1',\n",
    "    'workspace_url': 'your-workspace-url',\n",
    "    'resource_group': 'your-resource-group',\n",
    "    'subscription_id': 'your-subscription-id'\n",
    "    # Add your actual databricks configuration here\n",
    "}\n",
    "\n",
    "# Load original environment configuration\n",
    "print(\"Loading original environment configuration...\")\n",
    "env = DatabricksEnv(databricks_dict)\n",
    "\n",
    "# Enhanced configuration with parallel processing and hooks\n",
    "config_overrides = {\n",
    "    'pipeline': {\n",
    "        'parallel_processing': {\n",
    "            'enabled': True,\n",
    "            'max_workers': 6,  # Adjust based on your system capacity\n",
    "            'batch_size': 50\n",
    "        },\n",
    "        'source_loading': {\n",
    "            'enabled_sources': ['aip', 'atlas', 'snowflake'],\n",
    "            'parallel_loading': True,\n",
    "            'timeout_seconds': 300\n",
    "        },\n",
    "        'hooks': {\n",
    "            'pre_processing_enabled': True,\n",
    "            'post_processing_enabled': True,\n",
    "            'pre_hook_path': 'custom_hooks/pre_processing.py',\n",
    "            'post_hook_path': 'custom_hooks/post_processing.py'\n",
    "        },\n",
    "        'monitoring': {\n",
    "            'performance_tracking': True,\n",
    "            'detailed_reporting': True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Load complete configuration with enhancements\n",
    "print(\"Loading enhanced configuration...\")\n",
    "config = ConfigLoader.load_config(env.__dict__, config_overrides)\n",
    "\n",
    "# Display configuration summary\n",
    "print(\"\\nüìã Configuration Summary:\")\n",
    "print(f\"  Parallel Processing: {'‚úÖ Enabled' if config['pipeline']['parallel_processing']['enabled'] else '‚ùå Disabled'}\")\n",
    "print(f\"  Max Workers: {config['pipeline']['parallel_processing']['max_workers']}\")\n",
    "print(f\"  Data Sources: {', '.join(config['pipeline']['source_loading']['enabled_sources'])}\")\n",
    "print(f\"  Pre-processing Hook: {'‚úÖ Enabled' if config['pipeline']['hooks']['pre_processing_enabled'] else '‚ùå Disabled'}\")\n",
    "print(f\"  Post-processing Hook: {'‚úÖ Enabled' if config['pipeline']['hooks']['post_processing_enabled'] else '‚ùå Disabled'}\")\n",
    "print(f\"  Performance Monitoring: {'‚úÖ Enabled' if config['pipeline']['monitoring']['performance_tracking'] else '‚ùå Disabled'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Initialize Pipeline Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize performance monitor\n",
    "print(\"\\nüîß Initializing pipeline components...\")\n",
    "monitor = PipelinePerformanceMonitor()\n",
    "monitor.start_operation('pipeline_initialization')\n",
    "\n",
    "# Initialize data validator\n",
    "validator = PipelineValidator()\n",
    "\n",
    "# Initialize the enhanced coverage pipeline\n",
    "try:\n",
    "    pipeline = CoveragePipeline(\n",
    "        credentials_dict=env.credentials_dict,\n",
    "        sql_queries=env.sql_queries,\n",
    "        rag_params=env.rag_params,\n",
    "        crypto_spark=env.crypto_spark,\n",
    "        logger=env.logger,\n",
    "        SQL_QUERY_CONFIGS=env.SQL_QUERY_CONFIGS,\n",
    "        pre_hook_path=config['pipeline']['hooks']['pre_hook_path'] if config['pipeline']['hooks']['pre_processing_enabled'] else None,\n",
    "        post_hook_path=config['pipeline']['hooks']['post_hook_path'] if config['pipeline']['hooks']['post_processing_enabled'] else None,\n",
    "        max_workers=config['pipeline']['parallel_processing']['max_workers']\n",
    "    )\n",
    "    \n",
    "    monitor.end_operation('pipeline_initialization')\n",
    "    print(\"‚úÖ Pipeline initialized successfully!\")\n",
    "    \n",
    "    # Display pipeline statistics\n",
    "    pipeline_stats = pipeline.get_pipeline_stats()\n",
    "    print(\"\\nüìä Pipeline Statistics:\")\n",
    "    for key, value in pipeline_stats.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    monitor.end_operation('pipeline_initialization')\n",
    "    print(f\"‚ùå Pipeline initialization failed: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define Building Coverage Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building coverage conditions (same as original system)\n",
    "bldg_conditions = [\n",
    "    \"BLDG in LOSSDESC\",\n",
    "    \"BUILDING in LOSSDESC\", \n",
    "    \"STRUCTURE in LOSSDESC\",\n",
    "    \"ROOF in LOSSDESC\",\n",
    "    \"WALL in LOSSDESC\",\n",
    "    \"FOUNDATION in LOSSDESC\",\n",
    "    \"FLOOR in LOSSDESC\",\n",
    "    \"CEILING in LOSSDESC\"\n",
    "]\n",
    "\n",
    "print(f\"\\nüèóÔ∏è Building Coverage Conditions ({len(bldg_conditions)} rules):\")\n",
    "for i, condition in enumerate(bldg_conditions, 1):\n",
    "    print(f\"  {i}. {condition}\")\n",
    "\n",
    "# Additional processing parameters\n",
    "processing_params = {\n",
    "    'confidence_threshold': 0.7,\n",
    "    'batch_processing': True,\n",
    "    'quality_validation': True,\n",
    "    'performance_optimization': True\n",
    "}\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è Processing Parameters:\")\n",
    "for param, value in processing_params.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Execute Enhanced Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüöÄ Starting enhanced building coverage match processing...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Start full pipeline timing\n",
    "monitor.start_operation('full_pipeline_execution')\n",
    "overall_start_time = time.time()\n",
    "\n",
    "try:\n",
    "    # Execute the enhanced pipeline\n",
    "    final_df = pipeline.run_pipeline(bldg_conditions)\n",
    "    \n",
    "    # End timing\n",
    "    monitor.end_operation('full_pipeline_execution')\n",
    "    overall_end_time = time.time()\n",
    "    total_execution_time = overall_end_time - overall_start_time\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"üéâ PIPELINE EXECUTION COMPLETED SUCCESSFULLY!\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Display results summary\n",
    "    if not final_df.empty:\n",
    "        print(f\"\\nüìà Results Summary:\")\n",
    "        print(f\"  Total Claims Processed: {len(final_df):,}\")\n",
    "        print(f\"  Dataset Shape: {final_df.shape}\")\n",
    "        print(f\"  Execution Time: {total_execution_time:.2f} seconds\")\n",
    "        print(f\"  Processing Rate: {len(final_df) / total_execution_time:.1f} claims/second\")\n",
    "        \n",
    "        # Display column information\n",
    "        print(f\"\\nüìã Output Columns ({len(final_df.columns)}):\")\n",
    "        for i, col in enumerate(final_df.columns, 1):\n",
    "            print(f\"  {i:2d}. {col}\")\n",
    "        \n",
    "        # Display sample results\n",
    "        print(f\"\\nüîç Sample Results (first 5 rows):\")\n",
    "        display_columns = ['CLAIMNO', 'prediction', 'confidence'] if all(col in final_df.columns for col in ['CLAIMNO', 'prediction', 'confidence']) else final_df.columns[:5]\n",
    "        print(final_df[display_columns].head().to_string(index=False))\n",
    "        \n",
    "        # Show prediction distribution if available\n",
    "        if 'prediction' in final_df.columns:\n",
    "            print(f\"\\nüìä Prediction Distribution:\")\n",
    "            pred_counts = final_df['prediction'].value_counts()\n",
    "            for pred, count in pred_counts.items():\n",
    "                percentage = (count / len(final_df)) * 100\n",
    "                print(f\"  {pred}: {count:,} ({percentage:.1f}%)\")\n",
    "        \n",
    "        # Show quality flags if available\n",
    "        if 'final_quality_flag' in final_df.columns:\n",
    "            print(f\"\\nüè∑Ô∏è Quality Distribution:\")\n",
    "            quality_counts = final_df['final_quality_flag'].value_counts()\n",
    "            for quality, count in quality_counts.items():\n",
    "                percentage = (count / len(final_df)) * 100\n",
    "                print(f\"  {quality}: {count:,} ({percentage:.1f}%)\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è No results generated - check data sources and configuration\")\n",
    "        \n",
    "except Exception as e:\n",
    "    monitor.end_operation('full_pipeline_execution')\n",
    "    print(f\"\\n‚ùå Pipeline execution failed: {e}\")\n",
    "    print(\"\\nüîç Error Details:\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "    \n",
    "    # Set empty dataframe for error handling\n",
    "    final_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Analysis and Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nüìä PERFORMANCE ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display detailed performance report\n",
    "monitor.print_pipeline_report()\n",
    "\n",
    "# Get comprehensive performance statistics\n",
    "performance_summary = monitor.get_pipeline_summary()\n",
    "\n",
    "print(f\"\\nüéØ Key Performance Metrics:\")\n",
    "if performance_summary['total_time'] > 0:\n",
    "    print(f\"  Total Execution Time: {performance_summary['total_time']:.2f} seconds\")\n",
    "    print(f\"  Claims Processing Rate: {performance_summary['processing_rate_claims_per_second']:.2f} claims/second\")\n",
    "    print(f\"  Average Time per Claim: {performance_summary['average_time_per_claim']:.3f} seconds\")\n",
    "    \n",
    "    # Calculate efficiency metrics\n",
    "    if len(final_df) > 0:\n",
    "        efficiency_score = min(100, (performance_summary['processing_rate_claims_per_second'] / 2.0) * 100)\n",
    "        print(f\"  Processing Efficiency: {efficiency_score:.1f}%\")\n",
    "        \n",
    "        # Memory efficiency (if available)\n",
    "        try:\n",
    "            import psutil\n",
    "            memory_info = psutil.virtual_memory()\n",
    "            print(f\"  Memory Usage: {memory_info.percent:.1f}%\")\n",
    "            print(f\"  Available Memory: {memory_info.available / (1024**3):.1f} GB\")\n",
    "        except ImportError:\n",
    "            print(f\"  Memory Monitoring: Not available (psutil not installed)\")\n",
    "\n",
    "# Performance comparison with theoretical sequential processing\n",
    "if len(final_df) > 0 and performance_summary['total_time'] > 0:\n",
    "    estimated_sequential_time = len(final_df) * 0.5  # Assume 0.5 seconds per claim for sequential\n",
    "    time_saved = estimated_sequential_time - performance_summary['total_time']\n",
    "    improvement_percent = (time_saved / estimated_sequential_time) * 100\n",
    "    \n",
    "    print(f\"\\n‚ö° Parallel Processing Benefits:\")\n",
    "    print(f\"  Estimated Sequential Time: {estimated_sequential_time:.2f} seconds\")\n",
    "    print(f\"  Actual Parallel Time: {performance_summary['total_time']:.2f} seconds\")\n",
    "    print(f\"  Time Saved: {time_saved:.2f} seconds ({improvement_percent:.1f}% improvement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Quality Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not final_df.empty:\n",
    "    print(\"\\nüîç DATA QUALITY VALIDATION\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Validate output data quality\n",
    "    validation_results = validator.validate_output_data(final_df)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Validation Status: {'PASSED' if validation_results['is_valid'] else 'FAILED'}\")\n",
    "    \n",
    "    if validation_results['errors']:\n",
    "        print(f\"\\n‚ùå Validation Errors:\")\n",
    "        for error in validation_results['errors']:\n",
    "            print(f\"  ‚Ä¢ {error}\")\n",
    "    \n",
    "    if validation_results['warnings']:\n",
    "        print(f\"\\n‚ö†Ô∏è Validation Warnings:\")\n",
    "        for warning in validation_results['warnings']:\n",
    "            print(f\"  ‚Ä¢ {warning}\")\n",
    "    \n",
    "    # Display validation statistics\n",
    "    if validation_results['statistics']:\n",
    "        print(f\"\\nüìà Validation Statistics:\")\n",
    "        for stat_name, stat_value in validation_results['statistics'].items():\n",
    "            print(f\"  {stat_name}: {stat_value}\")\n",
    "    \n",
    "    # Additional data quality checks\n",
    "    print(f\"\\nüî¨ Additional Quality Checks:\")\n",
    "    \n",
    "    # Check for missing values in critical columns\n",
    "    critical_columns = ['CLAIMNO', 'prediction', 'confidence']\n",
    "    for col in critical_columns:\n",
    "        if col in final_df.columns:\n",
    "            missing_count = final_df[col].isnull().sum()\n",
    "            missing_percent = (missing_count / len(final_df)) * 100\n",
    "            status = \"‚úÖ\" if missing_count == 0 else \"‚ö†Ô∏è\" if missing_percent < 5 else \"‚ùå\"\n",
    "            print(f\"  {status} {col}: {missing_count} missing ({missing_percent:.1f}%)\")\n",
    "    \n",
    "    # Check confidence score distribution\n",
    "    if 'confidence' in final_df.columns:\n",
    "        high_conf = (final_df['confidence'] >= 0.8).sum()\n",
    "        medium_conf = ((final_df['confidence'] >= 0.6) & (final_df['confidence'] < 0.8)).sum()\n",
    "        low_conf = (final_df['confidence'] < 0.6).sum()\n",
    "        \n",
    "        print(f\"\\nüéØ Confidence Distribution:\")\n",
    "        print(f\"  High Confidence (‚â•0.8): {high_conf:,} ({(high_conf/len(final_df)*100):.1f}%)\")\n",
    "        print(f\"  Medium Confidence (0.6-0.8): {medium_conf:,} ({(medium_conf/len(final_df)*100):.1f}%)\")\n",
    "        print(f\"  Low Confidence (<0.6): {low_conf:,} ({(low_conf/len(final_df)*100):.1f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No data available for quality validation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not final_df.empty:\n",
    "    print(\"\\nüíæ SAVING RESULTS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    monitor.start_operation('data_saving')\n",
    "    \n",
    "    try:\n",
    "        # Save to multiple destinations using the multi-writer\n",
    "        save_config = {\n",
    "            'table': 'building_coverage_predictions_enhanced',\n",
    "            'schema': 'dbo',\n",
    "            'write_mode': 'append',\n",
    "            'output_dir': 'output',\n",
    "            'filename': f'building_coverage_predictions_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "        }\n",
    "        \n",
    "        # Choose destinations based on configuration\n",
    "        destinations = ['local']  # Always save locally\n",
    "        \n",
    "        # Add database destinations if configured\n",
    "        if env.credentials_dict.get('sql_warehouse'):\n",
    "            destinations.append('sql_warehouse')\n",
    "        \n",
    "        # Save results\n",
    "        save_results = pipeline.storage_writer.save_data_parallel(\n",
    "            final_df,\n",
    "            destinations=destinations,\n",
    "            max_workers=2,\n",
    "            write_config=save_config\n",
    "        )\n",
    "        \n",
    "        monitor.end_operation('data_saving')\n",
    "        \n",
    "        print(f\"\\nüìÅ Save Results:\")\n",
    "        for destination, result in save_results.items():\n",
    "            if destination != '_timing_info':\n",
    "                status = \"‚úÖ\" if \"Success\" in str(result) else \"‚ùå\"\n",
    "                print(f\"  {status} {destination}: {result}\")\n",
    "        \n",
    "        # Display timing information\n",
    "        if '_timing_info' in save_results:\n",
    "            timing_info = save_results['_timing_info']\n",
    "            print(f\"\\n‚è±Ô∏è Save Performance:\")\n",
    "            print(f\"  Total Save Time: {timing_info['total_time']:.2f} seconds\")\n",
    "            print(f\"  Records Saved: {timing_info['records_written']:,}\")\n",
    "            print(f\"  Save Rate: {timing_info['records_written'] / timing_info['total_time']:.0f} records/second\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        monitor.end_operation('data_saving')\n",
    "        print(f\"‚ùå Error saving results: {e}\")\n",
    "        \n",
    "        # Fallback: save to local CSV\n",
    "        try:\n",
    "            fallback_filename = f\"output/building_coverage_fallback_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv\"\n",
    "            final_df.to_csv(fallback_filename, index=False)\n",
    "            print(f\"üíæ Fallback save successful: {fallback_filename}\")\n",
    "        except Exception as fallback_error:\n",
    "            print(f\"‚ùå Fallback save also failed: {fallback_error}\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è No results to save\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Execution Summary and Final Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üìã FINAL EXECUTION SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Overall execution status\n",
    "execution_successful = not final_df.empty\n",
    "print(f\"\\nüéØ Execution Status: {'‚úÖ SUCCESSFUL' if execution_successful else '‚ùå FAILED'}\")\n",
    "print(f\"‚è∞ Completion Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "if execution_successful:\n",
    "    # Summary statistics\n",
    "    print(f\"\\nüìä Summary Statistics:\")\n",
    "    print(f\"  ‚úÖ Total Claims Processed: {len(final_df):,}\")\n",
    "    print(f\"  ‚ö° Processing Mode: {'Parallel' if config['pipeline']['parallel_processing']['enabled'] else 'Sequential'}\")\n",
    "    print(f\"  üîß Workers Used: {config['pipeline']['parallel_processing']['max_workers']}\")\n",
    "    print(f\"  üìÅ Data Sources: {len(config['pipeline']['source_loading']['enabled_sources'])}\")\n",
    "    print(f\"  ü™ù Custom Hooks: {'Pre & Post' if config['pipeline']['hooks']['pre_processing_enabled'] and config['pipeline']['hooks']['post_processing_enabled'] else 'None'}\")\n",
    "    \n",
    "    # Performance highlights\n",
    "    final_summary = monitor.get_pipeline_summary()\n",
    "    if final_summary['total_time'] > 0:\n",
    "        print(f\"\\n‚ö° Performance Highlights:\")\n",
    "        print(f\"  üöÄ Total Execution Time: {final_summary['total_time']:.2f} seconds\")\n",
    "        print(f\"  üìà Processing Rate: {final_summary['processing_rate_claims_per_second']:.2f} claims/second\")\n",
    "        print(f\"  üéØ Average per Claim: {final_summary['average_time_per_claim']:.3f} seconds\")\n",
    "    \n",
    "    # Quality assessment\n",
    "    if 'final_quality_flag' in final_df.columns:\n",
    "        good_quality = (final_df['final_quality_flag'] == 'GOOD').sum()\n",
    "        quality_percentage = (good_quality / len(final_df)) * 100\n",
    "        print(f\"\\nüè∑Ô∏è Quality Assessment:\")\n",
    "        print(f\"  ‚úÖ Good Quality: {good_quality:,} ({quality_percentage:.1f}%)\")\n",
    "        print(f\"  ‚ö†Ô∏è  Needs Review: {len(final_df) - good_quality:,} ({100 - quality_percentage:.1f}%)\")\n",
    "    \n",
    "    # Confidence assessment\n",
    "    if 'confidence' in final_df.columns:\n",
    "        avg_confidence = final_df['confidence'].mean()\n",
    "        high_confidence = (final_df['confidence'] >= 0.8).sum()\n",
    "        print(f\"\\nüéØ Confidence Assessment:\")\n",
    "        print(f\"  üìä Average Confidence: {avg_confidence:.1%}\")\n",
    "        print(f\"  üî• High Confidence (‚â•80%): {high_confidence:,} ({(high_confidence/len(final_df)*100):.1f}%)\")\n",
    "\n",
    "# System recommendations\n",
    "print(f\"\\nüí° System Recommendations:\")\n",
    "if execution_successful:\n",
    "    if final_summary.get('processing_rate_claims_per_second', 0) < 1.0:\n",
    "        print(f\"  üîß Consider increasing max_workers for better performance\")\n",
    "    if 'confidence' in final_df.columns and final_df['confidence'].mean() < 0.7:\n",
    "        print(f\"  üìö Consider tuning RAG parameters for higher confidence\")\n",
    "    if len(final_df) > 10000:\n",
    "        print(f\"  üíæ Consider implementing batch processing for large datasets\")\n",
    "    print(f\"  ‚úÖ System is performing well - continue with current configuration\")\n",
    "else:\n",
    "    print(f\"  üîç Check data source connectivity and configuration\")\n",
    "    print(f\"  üìã Review error logs above for specific issues\")\n",
    "    print(f\"  üõ†Ô∏è Consider running in debug mode for detailed troubleshooting\")\n",
    "\n",
    "# Final performance report (detailed)\n",
    "print(f\"\\n\" + \"=\" * 50)\n",
    "print(f\"üìà DETAILED PERFORMANCE REPORT\")\n",
    "print(f\"=\" * 50)\n",
    "monitor.print_pipeline_report()\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(f\"üéâ BUILDING COVERAGE MATCH SYSTEM EXECUTION COMPLETE\")\n",
    "print(f\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}