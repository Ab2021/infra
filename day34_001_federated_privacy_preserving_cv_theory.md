# Day 34 - Part 1: Federated & Privacy-Preserving Computer Vision Theory

## üìö Learning Objectives
By the end of this section, you will understand:
- Mathematical foundations of federated learning for computer vision systems
- Theoretical analysis of differential privacy and privacy-preserving training
- Mathematical principles of secure multi-party computation for vision tasks
- Information-theoretic perspectives on privacy-utility trade-offs
- Theoretical frameworks for communication-efficient federated optimization
- Mathematical modeling of adversarial attacks on privacy and defense mechanisms

---

## üîê Federated Learning Theory

### Mathematical Foundation of Federated Optimization

#### Statistical Heterogeneity and Non-IID Data
**Mathematical Formulation of Federated Learning**:
```
Global Objective:
min F(w) = Œ£·µ¢‚Çå‚ÇÅ·µê p·µ¢F·µ¢(w)
where F·µ¢(w) = E[‚Ñì(w; Œæ·µ¢)] is local objective for client i

Data Heterogeneity:
P·µ¢(x,y) ‚â† P‚±º(x,y) for i ‚â† j
Non-identical data distributions across clients

Statistical Challenges:
- Covariate shift: P(x) differs across clients
- Label shift: P(y) differs across clients  
- Concept shift: P(y|x) differs across clients
- Sample size heterogeneity: |D·µ¢| varies significantly

Mathematical Impact:
Local optima may differ: w*·µ¢ ‚â† w*‚±º
Global optimum may not exist
Convergence guarantees require assumptions
Communication complexity increases with heterogeneity
```

**FedAvg Algorithm Analysis**:
```
FedAvg Update Rule:
Local update: w·µ¢‚ÅΩ·µó‚Å∫¬π‚Åæ = w·µ¢‚ÅΩ·µó‚Åæ - Œ∑‚àáF·µ¢(w·µ¢‚ÅΩ·µó‚Åæ)
Global aggregation: w‚ÅΩ·µó‚Å∫¬π‚Åæ = Œ£·µ¢ (n·µ¢/n) w·µ¢‚ÅΩ·µó‚Å∫¬π‚Åæ

Convergence Analysis:
E[F(wÃÑ·µÄ)] - F* ‚â§ O(1/T) + O(Œ∑¬≤G¬≤) + O(Œ∑¬≤œÉ¬≤)
where:
- T: communication rounds
- G: gradient bound
- œÉ¬≤: variance due to heterogeneity

Mathematical Terms:
- O(1/T): optimization error (decreases with rounds)
- O(Œ∑¬≤G¬≤): gradient noise (learning rate dependent)
- O(Œ∑¬≤œÉ¬≤): heterogeneity penalty (fundamental limit)

Key Insight:
Perfect convergence impossible with heterogeneous data
Trade-off between communication and accuracy
```

#### Communication-Efficient Optimization
**Gradient Compression Theory**:
```
Compression Function:
C(g) = quantized or sparsified version of gradient g
Unbiased: E[C(g)] = g
Bounded variance: E[||C(g) - g||¬≤] ‚â§ œÅ||g||¬≤

Top-K Sparsification:
Keep largest K components, zero others
Compression ratio: K/d
Variance bound: ||C(g) - g||¬≤ ‚â§ (1 - K/d)||g||¬≤

Quantization:
Reduce precision of gradient components
k-bit quantization: 2·µè possible values
Error bound depends on quantization scheme

Convergence with Compression:
E[F(wÃÑ·µÄ)] - F* ‚â§ O(1/T) + O(Œ∑œÅ)
Additional term œÅ due to compression error
Compression-communication trade-off
```

**Error Feedback Mechanisms**:
```
Error Accumulation:
e·µ¢‚ÅΩ·µó‚Åæ = e·µ¢‚ÅΩ·µó‚Åª¬π‚Åæ + g·µ¢‚ÅΩ·µó‚Åæ - C(e·µ¢‚ÅΩ·µó‚Åª¬π‚Åæ + g·µ¢‚ÅΩ·µó‚Åæ)
Accumulate compression errors over time

Mathematical Benefits:
- Unbiased updates: E[compression + error] = original
- Better convergence: removes O(Œ∑œÅ) term
- Memory efficient: store only error vector
- Works with any compression scheme

Theoretical Guarantee:
With error feedback: E[F(wÃÑ·µÄ)] - F* ‚â§ O(1/T)
Same rate as uncompressed case
Communication reduction without convergence penalty
```

### Personalized Federated Learning

#### Mathematical Framework for Personalization
**Multi-Task Learning Perspective**:
```
Personalized Objective:
min Œ£·µ¢ [F·µ¢(w·µ¢) + Œª/2 ||w·µ¢ - wÃÑ||¬≤]
Local model w·µ¢ + global regularization toward wÃÑ

Mathematical Interpretation:
- F·µ¢(w·µ¢): client-specific performance
- Œª||w·µ¢ - wÃÑ||¬≤: similarity to global model
- Œª controls personalization-generalization trade-off

Optimal Solution:
w·µ¢* = argmin F·µ¢(w·µ¢) + Œª/2 ||w·µ¢ - wÃÑ||¬≤
Balances local adaptation with global knowledge

Theoretical Analysis:
Bias-variance decomposition:
Error = bias¬≤ + variance + irreducible
Personalization reduces bias, may increase variance
```

**Meta-Learning for Personalization**:
```
MAML-like Approach:
Find global initialization w‚ÇÄ
Fast adaptation: w·µ¢ = w‚ÇÄ - Œ±‚àáF·µ¢(w‚ÇÄ)

Mathematical Objective:
min E_i[F·µ¢(w‚ÇÄ - Œ±‚àáF·µ¢(w‚ÇÄ))]
Second-order optimization problem

Per-FedAvg Algorithm:
Alternating optimization:
1. Local adaptation: w·µ¢ ‚Üê w‚ÇÄ - Œ±‚àáF·µ¢(w‚ÇÄ)
2. Meta update: w‚ÇÄ ‚Üê w‚ÇÄ - Œ≤‚àá_{w‚ÇÄ} Œ£·µ¢ F·µ¢(w·µ¢)

Convergence Guarantees:
Under similarity assumptions between clients
Faster adaptation than standard federated learning
Better generalization to new clients
```

#### Clustered Federated Learning
**Mathematical Clustering Framework**:
```
Client Similarity Measure:
d(i,j) = ||‚àáF·µ¢(w) - ‚àáF‚±º(w)||‚ÇÇ
Gradient-based distance measure

Clustering Objective:
min Œ£‚Çñ Œ£·µ¢‚ààC‚Çñ ||w·µ¢ - w‚Çñ||¬≤ + Œª Œ£‚Çñ |C‚Çñ|
Within-cluster similarity + cluster size penalty

Online Clustering:
Update clusters based on gradient similarity
Split/merge clusters based on performance
Mathematical: adaptive cluster assignment

Theoretical Benefits:
- Reduces heterogeneity within clusters
- Better convergence than global approach
- Handles diverse client populations
- Automatic discovery of client groups
```

---

## üõ°Ô∏è Differential Privacy Theory

### Mathematical Foundation of Privacy

#### Differential Privacy Definition and Properties
**Formal Definition**:
```
(Œµ,Œ¥)-Differential Privacy:
For neighboring datasets D, D' differing in one record:
P(M(D) ‚àà S) ‚â§ exp(Œµ) P(M(D') ‚àà S) + Œ¥

Mathematical Parameters:
- Œµ: privacy budget (smaller = more private)
- Œ¥: failure probability (typically Œ¥ << 1/n)
- M: randomized mechanism
- S: any subset of outputs

Pure Differential Privacy:
Œ¥ = 0, only Œµ matters
Stronger guarantee, harder to achieve
Approximate DP: Œ¥ > 0, more practical
```

**Composition Theorems**:
```
Sequential Composition:
k mechanisms with (Œµ·µ¢, Œ¥·µ¢) give (Œ£·µ¢Œµ·µ¢, Œ£·µ¢Œ¥·µ¢)
Privacy budget depletes linearly
Total privacy cost accumulates

Advanced Composition:
For k identical (Œµ,Œ¥)-DP mechanisms:
Overall privacy: (Œµ', kŒ¥ + Œ¥')
where Œµ' ‚âà Œµ‚àö(2k log(1/Œ¥'))

Moments Accountant:
Tighter bounds for specific algorithms
Track privacy loss distribution
Essential for practical deep learning
Mathematical: moment generating functions
```

#### Privacy Mechanisms for Machine Learning
**Gaussian Mechanism**:
```
Noisy Gradient:
ƒù = g + N(0, œÉ¬≤I)
where œÉ¬≤ = 2 log(1.25/Œ¥) S¬≤/Œµ¬≤

Sensitivity Analysis:
S = max ||g(D) - g(D')||‚ÇÇ
Maximum change in gradient for neighboring datasets
Bounded by gradient clipping: S ‚â§ C

Privacy Analysis:
Gaussian noise calibrated to sensitivity
Higher sensitivity ‚Üí more noise ‚Üí less utility
Gradient clipping essential for bounded sensitivity

Practical Implementation:
Clip individual gradients: g·µ¢ ‚Üê g·µ¢/max(1, ||g·µ¢||/C)
Add noise to sum: ƒù = Œ£·µ¢g·µ¢ + N(0, œÉ¬≤I)
Mathematical: preserve privacy while enabling learning
```

**Exponential Mechanism**:
```
Discrete Selection:
P(output = r) ‚àù exp(Œµf(D,r)/(2Œîf))
where f is utility function, Œîf is sensitivity

Applications:
- Model selection: choose best hyperparameters
- Feature selection: select important features
- Architecture search: private neural architecture
- Synthetic data generation: select representative samples

Mathematical Properties:
- Works for discrete outputs
- Utility-based selection with privacy
- No noise addition, probabilistic selection
- Composition rules apply
```

### Private Training Algorithms

#### DP-SGD (Differentially Private SGD)
**Algorithm Description**:
```
DP-SGD Steps:
1. Sample batch B uniformly at random
2. Compute per-example gradients: g·µ¢ = ‚àá‚Ñì(Œ∏, x·µ¢, y·µ¢)
3. Clip gradients: ·∏°·µ¢ = g·µ¢/max(1, ||g·µ¢||‚ÇÇ/C)
4. Add noise: gÃÉ = (1/|B|)[Œ£·µ¢ ·∏°·µ¢ + N(0, œÉ¬≤C¬≤I)]
5. Update: Œ∏ ‚Üê Œ∏ - Œ∑ gÃÉ

Privacy Analysis:
Per-step privacy: (Œµ‚ÇÄ, Œ¥‚ÇÄ) with Œµ‚ÇÄ ‚âà 2qœÉ‚Åª¬π
where q = batch_size/dataset_size
Overall privacy via composition theorems

Mathematical Challenges:
- Gradient clipping changes optimization landscape
- Noise injection reduces learning rate effectiveness
- Privacy-utility trade-off fundamental
- Hyperparameter tuning affects privacy
```

**Privacy Amplification by Sampling**:
```
Subsampling Theorem:
If base mechanism has (Œµ,Œ¥)-DP,
subsampling with probability q gives (q¬∑Œµ, q¬∑Œ¥)-DP

Mathematical Intuition:
Each sample affects fewer training examples
Reduces sensitivity of the mechanism
Crucial for practical DP deep learning

Poisson Sampling:
Include each example independently with probability q
Tighter privacy analysis than fixed-size sampling
Mathematical: Poisson random variables
```

#### Communication-Efficient Private Federated Learning
**Private Aggregation**:
```
Secure Aggregation:
Œ£·µ¢ w·µ¢ computed without revealing individual w·µ¢
Based on secret sharing or homomorphic encryption
Mathematical: cryptographic protocols

Differential Privacy + Secure Aggregation:
Client-level DP: each client adds noise locally
Central DP: server adds noise to aggregate
Mathematical: stronger privacy guarantees

Privacy Amplification:
Federated setting provides natural subsampling
Each round uses subset of clients
Mathematical: composition with subsampling
Better privacy-utility trade-offs
```

**Compression with Privacy**:
```
Private Compression:
Combine gradient compression with noise injection
Sparsification before adding noise
Mathematical: analyze privacy of compressed mechanisms

Error Feedback with Privacy:
Accumulate errors privately
Add noise to compressed gradients + errors
Mathematical: privacy analysis of stateful algorithms

Theoretical Challenges:
- Compression changes sensitivity
- Adaptive compression affects privacy
- Error accumulation complicates analysis
- Communication-privacy-utility three-way trade-off
```

---

## üîí Secure Multi-Party Computation

### Cryptographic Protocols for Privacy

#### Homomorphic Encryption for ML
**Mathematical Foundation**:
```
Homomorphic Property:
Enc(a) ‚äï Enc(b) = Enc(a + b)
Enc(a) ‚äó Enc(b) = Enc(a √ó b)
where ‚äï, ‚äó are ciphertext operations

Leveled Homomorphism:
Support limited depth of operations
Each operation increases noise level
Bootstrapping: refresh ciphertext noise

CKKS Scheme:
Supports approximate computations
Efficient for floating-point operations
Suitable for neural network inference
Mathematical: ring learning with errors (RLWE)
```

**Private Neural Network Inference**:
```
Protocol Steps:
1. Client encrypts input: c = Enc(x)
2. Server computes: ƒâ = NN(c) homomorphically
3. Client decrypts: ≈∑ = Dec(ƒâ)

Mathematical Operations:
- Linear layers: matrix-vector multiplication
- Activation functions: polynomial approximation
- Pooling: selecting maximum (challenging)
- Batch normalization: requires statistics

Performance Considerations:
Computational overhead: 1000-10000√ó slowdown
Communication overhead: larger ciphertext sizes
Accuracy loss: approximation errors
Mathematical: error propagation through network
```

#### Secret Sharing Schemes
**Shamir's Secret Sharing**:
```
Mathematical Construction:
Secret s shared as polynomial p(x) = s + a‚ÇÅx + ... + a‚Çúx·µó
Share i: (i, p(i))
Reconstruction: Lagrange interpolation from t+1 shares

Privacy Property:
Any t shares reveal no information about s
Information-theoretic security
Mathematical: polynomial evaluation in finite field

Application to ML:
Share model weights and data
Compute linear operations exactly
Non-linear operations require protocols
Communication complexity: O(number of shares)
```

**Arithmetic Secret Sharing**:
```
Additive Sharing:
[x] = (x‚ÇÅ, x‚ÇÇ, x‚ÇÉ) where x = x‚ÇÅ + x‚ÇÇ + x‚ÇÉ
Addition: [x] + [y] = (x‚ÇÅ+y‚ÇÅ, x‚ÇÇ+y‚ÇÇ, x‚ÇÉ+y‚ÇÉ)
Scalar multiplication: c[x] = (cx‚ÇÅ, cx‚ÇÇ, cx‚ÇÉ)

Multiplication Protocol:
[x] √ó [y] requires communication
Beaver triples: pre-shared random values
Mathematical: convert multiplication to addition
Rounds of communication for each operation

BGW Protocol:
Secure computation for arithmetic circuits
Privacy threshold: t < n/2 honest parties
Mathematical: polynomial-based computation
Suitable for neural network computation
```

### Federated Learning with Cryptographic Privacy

#### Secure Aggregation Protocols
**Mathematical Framework**:
```
Aggregation Goal:
Compute Œ£·µ¢ x·µ¢ without revealing individual x·µ¢
Requirements: correctness, privacy, robustness

Masking-Based Approach:
Each client i generates random masks r·µ¢‚±º
Send x·µ¢ + Œ£‚±º r·µ¢‚±º to server
Masks cancel: Œ£·µ¢(x·µ¢ + Œ£‚±º r·µ¢‚±º) = Œ£·µ¢ x·µ¢
Mathematical: additive blinding with cancellation

Dropout Resilience:
Use threshold secret sharing for masks
Reconstruct aggregate even if some clients drop
Mathematical: error-correcting codes
Privacy preserved under honest majority
```

**Practical Secure Aggregation**:
```
Two-Server Model:
Split computation between two non-colluding servers
Each client shares data with both servers
Mathematical: replicated secret sharing

Performance Optimization:
Batch multiple aggregation rounds
Amortize cryptographic costs
Quantization before aggregation
Mathematical: communication-computation trade-offs

Security Analysis:
Honest-but-curious adversary model
Semi-honest server assumptions
Mathematical: computational security proofs
Practical efficiency considerations
```

#### Privacy-Preserving Model Updates
**Differential Privacy in Secure Aggregation**:
```
Combined Approach:
Local DP: clients add noise before sharing
Secure aggregation: hide individual contributions
Mathematical: composition of privacy mechanisms

Amplification Benefits:
Secure aggregation provides additional privacy
Lower noise requirements for same Œµ
Mathematical: privacy amplification theorems
Better utility-privacy trade-offs

Implementation Challenges:
Coordinate noise parameters across clients
Handle client dropout with privacy guarantees
Mathematical: robust protocol design
Practical deployment considerations
```

---

## üéØ Advanced Understanding Questions

### Federated Learning Theory:
1. **Q**: Analyze the mathematical relationship between data heterogeneity and convergence rates in federated learning, developing adaptive optimization strategies.
   **A**: Mathematical relationship: heterogeneity introduces variance term O(œÉ¬≤) in convergence bound where œÉ¬≤ measures client gradient diversity. Analysis: homogeneous data ‚Üí fast convergence, heterogeneous data ‚Üí slower convergence + potential divergence. Adaptive strategies: (1) client selection based on gradient similarity, (2) adaptive learning rates per client, (3) proximal terms for regularization. Mathematical framework: E[F(wÃÑ·µÄ)] - F* ‚â§ O(1/T) + O(œÉ¬≤/T). Key insight: heterogeneity fundamentally limits federated learning, requiring algorithmic adaptation.

2. **Q**: Develop a theoretical framework for communication-efficient federated learning that optimally balances compression, privacy, and convergence guarantees.
   **A**: Framework components: (1) gradient compression with error feedback, (2) differential privacy via noise injection, (3) convergence analysis under both constraints. Mathematical formulation: minimize communication subject to privacy budget Œµ and convergence rate constraints. Analysis: compression reduces communication by factor K/d, privacy adds O(‚àö(log(1/Œ¥))/Œµ) noise, both affect convergence. Optimal strategy: adaptive compression based on privacy budget and convergence requirements. Theoretical insight: three-way trade-off requires coordinated optimization across all dimensions.

3. **Q**: Compare personalized vs global federated learning approaches mathematically and derive conditions for when personalization provides benefits.
   **A**: Mathematical comparison: global FL minimizes Œ£·µ¢ p·µ¢F·µ¢(w), personalized FL minimizes F·µ¢(w·µ¢) + Œª||w·µ¢ - wÃÑ||¬≤. Benefits conditions: (1) high task diversity between clients, (2) sufficient local data for adaptation, (3) tasks share some common structure. Analysis: personalization reduces bias at cost of increased variance. Mathematical bound: personalized error ‚â§ global error when client diversity exceeds threshold. Key insight: personalization beneficial when clients have distinct but related tasks.

### Privacy-Preserving Learning:
4. **Q**: Analyze the fundamental privacy-utility trade-offs in differential privacy for computer vision and derive optimal noise injection strategies.
   **A**: Mathematical trade-off: utility decreases as O(1/Œµ) with privacy budget Œµ. Analysis: vision tasks require high-dimensional gradients, leading to large sensitivity bounds. Optimal noise: calibrate to actual vs worst-case sensitivity using gradient clipping. Strategies: (1) adaptive clipping based on gradient norms, (2) layer-wise privacy budgets, (3) public data for better initialization. Mathematical framework: minimize E[loss] subject to (Œµ,Œ¥)-DP constraints. Key insight: gradient structure in vision enables better privacy-utility trade-offs than worst-case analysis suggests.

5. **Q**: Develop a mathematical analysis of privacy amplification in federated learning through subsampling and secure aggregation.
   **A**: Analysis components: (1) subsampling amplification: q-fold reduction in privacy loss, (2) secure aggregation: computational privacy without noise, (3) composition: combined effect of both mechanisms. Mathematical framework: federated subsampling provides (qŒµ, qŒ¥)-DP per round, secure aggregation adds cryptographic privacy. Combined benefit: lower noise requirements for same privacy level. Theoretical bound: amplification factor ‚âà ‚àö(number_of_clients). Key insight: federated setting naturally provides privacy amplification through client subsampling.

6. **Q**: Compare the mathematical security guarantees of homomorphic encryption vs secret sharing for private neural network computation.
   **A**: Mathematical comparison: HE provides computational security based on cryptographic assumptions, secret sharing provides information-theoretic security. HE advantages: single-party computation, handles arbitrary circuits. SS advantages: perfect security, efficient for specific operations. Security analysis: HE secure against bounded adversaries, SS secure against unlimited adversaries. Performance: HE has high computational overhead, SS has high communication overhead. Key insight: choice depends on threat model and performance requirements.

### Advanced Privacy Mechanisms:
7. **Q**: Design a unified mathematical framework for federated learning that combines differential privacy, secure aggregation, and communication compression.
   **A**: Framework components: (1) local DP with gradient compression, (2) secure aggregation of compressed gradients, (3) privacy amplification through federation. Mathematical formulation: (qŒµ, qŒ¥)-DP per round with compression ratio œÅ and cryptographic security. Challenges: compression affects DP sensitivity, secure aggregation must handle variable-size messages. Solution: error feedback for compression, threshold secret sharing for robustness. Theoretical guarantee: maintains privacy and security while reducing communication by factor œÅ.

8. **Q**: Analyze the mathematical foundations of membership inference attacks against federated learning systems and develop provable defense mechanisms.
   **A**: Attack analysis: adversary tries to determine if specific data point was in training set. Mathematical formulation: distinguish between models trained with/without target data point. Defense mechanisms: (1) differential privacy provides formal defense, (2) secure aggregation prevents model inspection, (3) model compression reduces information leakage. Theoretical defense: (Œµ,Œ¥)-DP limits membership inference advantage to exp(Œµ). Key insight: privacy mechanisms provide provable defenses against membership inference, with quantifiable privacy loss bounds.

---

## üîë Key Federated & Privacy-Preserving CV Principles

1. **Federated Optimization Theory**: Statistical heterogeneity in federated learning fundamentally limits convergence rates, requiring adaptive algorithms and personalization strategies for optimal performance.

2. **Differential Privacy Mathematics**: Privacy-utility trade-offs in computer vision are governed by gradient sensitivity bounds, with careful algorithm design enabling practical private learning.

3. **Secure Computation Overhead**: Cryptographic privacy mechanisms (homomorphic encryption, secret sharing) provide strong security guarantees but with significant computational and communication overheads.

4. **Privacy Amplification**: Federated learning naturally provides privacy amplification through client subsampling and secure aggregation, enabling better privacy-utility trade-offs.

5. **Communication-Privacy-Utility Trilemma**: Optimizing federated learning requires balancing three competing objectives: communication efficiency, privacy preservation, and model utility, with no single solution optimal for all scenarios.

---

**Next**: Continue with Day 35 - Mobile & Edge-Optimized CV Theory